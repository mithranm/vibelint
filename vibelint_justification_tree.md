# Snapshot

## Filesystem Tree

```
vibelint/
├── src/
│   └── vibelint/
│       ├── cli/
│       │   ├── __init__.py
│       │   ├── ai.py
│       │   ├── analysis.py
│       │   ├── core.py
│       │   ├── maintenance.py
│       │   └── validation.py
│       ├── context/
│       │   ├── __init__.py
│       │   ├── analyzer.py
│       │   ├── probing.py
│       │   └── prompts.py
│       ├── llm/
│       │   ├── __init__.py
│       │   ├── llm_config.py
│       │   ├── llm_context_engineer.py
│       │   ├── llm_orchestrator.py
│       │   ├── llm_retry.py
│       │   └── manager.py
│       ├── validators/
│       │   ├── architecture/
│       │   │   ├── __init__.py
│       │   │   ├── basic_patterns.py
│       │   │   ├── fallback_patterns.py
│       │   │   ├── intelligent_llm_analysis.py
│       │   │   ├── llm_analysis.py
│       │   │   └── semantic_similarity.py
│       │   ├── project_wide/
│       │   │   ├── __init__.py
│       │   │   ├── api_consistency.py
│       │   │   ├── code_smells.py
│       │   │   ├── dead_code.py
│       │   │   ├── module_cohesion.py
│       │   │   ├── namespace_collisions.py
│       │   │   └── namespace_report.py
│       │   ├── single_file/
│       │   │   ├── __init__.py
│       │   │   ├── absolute_imports.py
│       │   │   ├── docstring.py
│       │   │   ├── emoji.py
│       │   │   ├── exports.py
│       │   │   ├── line_count.py
│       │   │   ├── logger_names.py
│       │   │   ├── print_statements.py
│       │   │   ├── self_validation.py
│       │   │   ├── strict_config.py
│       │   │   └── typing_quality.py
│       │   ├── __init__.py
│       │   └── registry.py
│       ├── workflows/
│       │   ├── core/
│       │   │   ├── __init__.py
│       │   │   └── base.py
│       │   ├── implementations/
│       │   │   ├── __init__.py
│       │   │   ├── coverage_analysis.py
│       │   │   ├── justification_analysis.py
│       │   │   ├── redundancy_detection.py
│       │   │   └── single_file_validation.py
│       │   ├── __init__.py
│       │   ├── cleanup.py
│       │   ├── evaluation.py
│       │   ├── manager.py
│       │   ├── orchestrator.py
│       │   ├── registry.py
│       │   └── single_file_validation.py
│       ├── __init__.py
│       ├── __main__.py
│       ├── auto_discovery.py
│       ├── cli.py
│       ├── config.py
│       ├── core.py
│       ├── dependency_graph_manager.py
│       ├── diagnostics.py
│       ├── discovery.py
│       ├── distributed_coordinator.py
│       ├── embedding_client.py
│       ├── fix.py
│       ├── justification.py
│       ├── justification_v2.py
│       ├── multi_representation_analyzer.py
│       ├── multitool_safeguards.py
│       ├── plugin_system.py
│       ├── project_map.py
│       ├── reporting.py
│       ├── results.py
│       ├── rules.py
│       ├── runtime_tracer.py
│       ├── self_improvement.py
│       ├── snapshot.py
│       ├── utils.py
│       ├── validation_engine.py
│       └── vector_store.py
├── tests/
│   ├── fixtures/
│   │   ├── check_success/
│   │   │   └── myproject/
│   │   │       └── src/
│   │   │           └── mypkg/
│   │   │               ├── __init__.py
│   │   │               └── module.py
│   │   └── fix_missing_all/
│   │       └── fixproj/
│   │           ├── another.py
│   │           └── needs_fix.py
│   ├── test_cli.py
│   ├── test_emoji_removal_safety.py
│   ├── test_fix_functionality.py
│   ├── test_plugin_system.py
│   ├── test_print_suppression.py
│   └── test_validators.py
└── fix_imports.py
```

## File Contents

Files are ordered alphabetically by path.

### File: fix_imports.py

```python
#!/usr/bin/env python3
"""
Convert all relative imports to absolute imports in vibelint.

This script systematically finds and replaces relative imports with absolute ones.
"""

import re
import sys
from pathlib import Path


def convert_relative_to_absolute(file_path: Path, content: str) -> str:
    """Convert relative imports to absolute imports."""

    # Get the module path relative to src/vibelint
    relative_path = file_path.relative_to(file_path.parents[2] / "src")
    module_parts = list(relative_path.parts[:-1])  # Remove the .py file part

    lines = content.split('\n')
    fixed_lines = []

    for line in lines:
        # Match relative imports like "from .module import something"
        rel_import_match = re.match(r'^(\s*)from (\.+)([^.\s]*) import (.+)$', line)
        if rel_import_match:
            indent, dots, module_name, imports = rel_import_match.groups()

            # Calculate the absolute module path
            levels_up = len(dots) - 1  # Number of parent directories to go up

            if levels_up == 0:
                # Same directory: from .module import something
                if module_name:
                    abs_module = ".".join(module_parts + [module_name])
                else:
                    # from . import something (importing from __init__.py)
                    abs_module = ".".join(module_parts)
            else:
                # Parent directories: from ..parent.module import something
                if len(module_parts) >= levels_up:
                    base_parts = module_parts[:-levels_up] if levels_up > 0 else module_parts
                    if module_name:
                        abs_module = ".".join(base_parts + [module_name])
                    else:
                        abs_module = ".".join(base_parts)
                else:
                    # Too many levels up, keep as is
                    fixed_lines.append(line)
                    continue

            # Create the absolute import
            fixed_line = f"{indent}from {abs_module} import {imports}"
            fixed_lines.append(fixed_line)
            print(f"  {file_path.name}: {line.strip()} -> {fixed_line.strip()}")

        else:
            # Not a relative import, keep as is
            fixed_lines.append(line)

    return '\n'.join(fixed_lines)


def main():
    """Convert all relative imports in the vibelint source."""

    vibelint_src = Path("src/vibelint")

    if not vibelint_src.exists():
        print("Error: src/vibelint directory not found")
        sys.exit(1)

    print("Converting relative imports to absolute imports...")

    # Find all Python files
    python_files = list(vibelint_src.rglob("*.py"))

    for py_file in python_files:
        try:
            content = py_file.read_text(encoding='utf-8')

            # Check if file has relative imports
            if 'from .' in content:
                print(f"\nProcessing {py_file}")
                fixed_content = convert_relative_to_absolute(py_file, content)
                py_file.write_text(fixed_content, encoding='utf-8')

        except Exception as e:
            print(f"Error processing {py_file}: {e}")

    print("\nDone! All relative imports converted to absolute imports.")


if __name__ == "__main__":
    main()
```

---
### File: src/vibelint/__init__.py

```python
"""
Vibelint: Self-Improving Code Analysis Engine

A sophisticated code analysis system that combines filesystem, vector, and graph
representations to enable foundation models to understand and improve codebases
autonomously. Follows MCP-style capability assignment patterns.

Key Components:
- Multi-representation analysis (filesystem, vector, graph, runtime)
- Hybrid dependency graphs with NetworkX + Qdrant embeddings
- Context engineering for foundation model consumption
- Self-improvement and autonomous evolution
- Zero-config auto-discovery scaling

Usage:
    import vibelint

    # Create engine with auto-discovery
    engine = vibelint.VibelintEngine()

    # Assign capabilities
    engine.assign_capability("embedding", "vanguard_ensemble")
    engine.assign_capability("reasoning", "claude")
    engine.assign_capability("memory", "qdrant_enhanced")
    engine.assign_capability("tracing", "runtime_mock")

    # Analyze and improve
    results = await engine.analyze_and_improve("path/to/project")
"""

import asyncio
import logging
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

from vibelint.auto_discovery import AutoDiscovery
from vibelint.config import Config as VibelintConfig
from vibelint.dependency_graph_manager import DependencyGraphManager
from vibelint.justification import JustificationEngine
from vibelint.llm.llm_context_engineer import VibelintContextEngineer as LLMContextEngineer
from vibelint.llm import get_llm_config
from vibelint.multi_representation_analyzer import MultiRepresentationAnalyzer
from vibelint.runtime_tracer import RuntimeTracer
from vibelint.self_improvement import VibelintSelfImprover as SelfImprovementSystem

# Setup logging
logger = logging.getLogger(__name__)


@dataclass
class AnalysisResults:
    """Results from vibelint analysis and improvement."""

    filesystem_analysis: Dict[str, Any]
    vector_analysis: Dict[str, Any]
    graph_analysis: Dict[str, Any]
    runtime_analysis: Dict[str, Any]
    improvement_suggestions: List[Dict[str, Any]]
    context_engineered: Dict[str, Any]
    self_improvements: Optional[Dict[str, Any]] = None


class CapabilityRegistry:
    """Registry for managing assigned capabilities."""

    def __init__(self):
        self.capabilities = {
            "embedding": "vanguard_ensemble",  # Default
            "reasoning": "claude",  # Default
            "memory": "qdrant_enhanced",  # Default
            "tracing": "runtime_mock",  # Default
            "discovery": "auto_zero_config",  # Default
        }

        # Available capability implementations
        self.available_capabilities = {
            "embedding": [
                "vanguard_ensemble",  # VanguardOne + VanguardTwo
                "vanguard_one",  # VanguardOne only
                "vanguard_two",  # VanguardTwo only
                "openai_ada",  # OpenAI embeddings
                "sentence_transformers",  # Local embeddings
            ],
            "reasoning": [
                "claude",  # Claude API
                "gpt_oss_120b",  # GPT-OSS 120B
                "claudia",  # Local Claudia model
                "chip",  # Local Chip model
                "ollama",  # Local Ollama models
            ],
            "memory": [
                "qdrant_enhanced",  # Qdrant with EBR
                "qdrant_basic",  # Basic Qdrant
                "chromadb",  # ChromaDB
                "in_memory",  # RAM-only
            ],
            "tracing": [
                "runtime_mock",  # Mock value injection
                "static_analysis",  # AST-only analysis
                "hybrid",  # Mock + static
                "disabled",  # No tracing
            ],
            "discovery": [
                "auto_zero_config",  # Automatic discovery
                "config_file",  # Explicit config
                "manual",  # Manual specification
            ],
        }

    def assign(self, capability_type: str, implementation: str) -> bool:
        """Assign a specific implementation to a capability type."""
        if capability_type not in self.available_capabilities:
            logger.error(f"Unknown capability type: {capability_type}")
            return False

        if implementation not in self.available_capabilities[capability_type]:
            logger.error(f"Unknown implementation '{implementation}' for '{capability_type}'")
            logger.info(f"Available: {self.available_capabilities[capability_type]}")
            return False

        self.capabilities[capability_type] = implementation
        logger.info(f"Assigned {capability_type} -> {implementation}")
        return True

    def get(self, capability_type: str) -> str:
        """Get the assigned implementation for a capability type."""
        return self.capabilities.get(capability_type, "unknown")

    def list_available(self, capability_type: Optional[str] = None) -> Dict[str, List[str]]:
        """List available implementations for capabilities."""
        if capability_type:
            return {capability_type: self.available_capabilities.get(capability_type, [])}
        return self.available_capabilities.copy()


class VibelintEngine:
    """
    Main orchestrating engine for vibelint analysis and improvement.

    Provides MCP-style capability assignment and orchestrates all analysis
    components to deliver comprehensive code understanding and improvement.
    """

    def __init__(self, project_path: Optional[Union[str, Path]] = None):
        """Initialize the vibelint engine."""
        self.project_path = Path(project_path) if project_path else Path.cwd()
        self.capability_registry = CapabilityRegistry()

        # Initialize core components (lazy-loaded)
        self._config: Optional[VibelintConfig] = None
        self._auto_discovery: Optional[AutoDiscovery] = None
        self._analyzer: Optional[MultiRepresentationAnalyzer] = None
        self._graph_manager: Optional[DependencyGraphManager] = None
        self._context_engineer: Optional[LLMContextEngineer] = None
        self._runtime_tracer: Optional[RuntimeTracer] = None
        self._self_improvement: Optional[SelfImprovementSystem] = None

        logger.info(f"VibelintEngine initialized for project: {self.project_path}")

    def assign_capability(self, capability_type: str, implementation: str) -> bool:
        """
        Assign a specific implementation to a capability type.

        Args:
            capability_type: Type of capability (embedding, reasoning, memory, tracing, discovery)
            implementation: Specific implementation to use

        Returns:
            True if assignment successful, False otherwise
        """
        return self.capability_registry.assign(capability_type, implementation)

    def list_capabilities(self, capability_type: Optional[str] = None) -> Dict[str, List[str]]:
        """List available capability implementations."""
        return self.capability_registry.list_available(capability_type)

    def get_assigned_capabilities(self) -> Dict[str, str]:
        """Get currently assigned capabilities."""
        return self.capability_registry.capabilities.copy()

    async def _ensure_components_initialized(self):
        """Lazy-load and initialize all components based on assigned capabilities."""
        if self._config is None:
            discovery_type = self.capability_registry.get("discovery")
            if discovery_type == "auto_zero_config":
                self._auto_discovery = AutoDiscovery(self.project_path)
                discovered_config = await self._auto_discovery.discover_configuration()
                self._config = VibelintConfig(**discovered_config)
            else:
                self._config = VibelintConfig.from_project_path(self.project_path)

        if self._analyzer is None:
            embedding_type = self.capability_registry.get("embedding")
            self._analyzer = MultiRepresentationAnalyzer(
                config=self._config, embedding_strategy=embedding_type
            )

        if self._graph_manager is None:
            self._graph_manager = DependencyGraphManager(
                config=self._config, embedding_strategy=self.capability_registry.get("embedding")
            )

        if self._context_engineer is None:
            reasoning_type = self.capability_registry.get("reasoning")
            self._context_engineer = LLMContextEngineer(
                config=self._config,
                reasoning_strategy=reasoning_type,
                embedding_strategy=self.capability_registry.get("embedding"),
            )

        tracing_type = self.capability_registry.get("tracing")
        if self._runtime_tracer is None and tracing_type != "disabled":
            self._runtime_tracer = RuntimeTracer(config=self._config, tracing_strategy=tracing_type)

        if self._self_improvement is None:
            self._self_improvement = SelfImprovementSystem(
                config=self._config, reasoning_strategy=self.capability_registry.get("reasoning")
            )

    async def analyze(
        self,
        target_path: Optional[Union[str, Path]] = None,
        include_runtime: bool = True,
        include_improvements: bool = True,
    ) -> AnalysisResults:
        """
        Perform comprehensive analysis of the codebase.

        Args:
            target_path: Specific path to analyze (defaults to project_path)
            include_runtime: Whether to include runtime tracing analysis
            include_improvements: Whether to generate improvement suggestions

        Returns:
            AnalysisResults containing all analysis data
        """
        await self._ensure_components_initialized()

        analysis_path = Path(target_path) if target_path else self.project_path
        logger.info(f"Starting comprehensive analysis of: {analysis_path}")

        # Perform multi-representation analysis
        analysis_result = await self._analyzer.analyze_comprehensive(analysis_path)

        # Build and analyze dependency graph
        await self._graph_manager.build_project_graph(analysis_path)
        graph_analysis = await self._graph_manager.analyze_dependencies()

        # Runtime analysis (if enabled)
        runtime_analysis = {}
        if include_runtime and self._runtime_tracer:
            runtime_analysis = await self._runtime_tracer.trace_project_execution(analysis_path)

        # Generate improvement suggestions
        improvement_suggestions = []
        if include_improvements:
            improvement_suggestions = await self._analyzer.generate_improvements(
                analysis_result, graph_analysis, runtime_analysis
            )

        # Context engineering for LLM consumption
        context_engineered = await self._context_engineer.engineer_context(
            filesystem_analysis=analysis_result.get("filesystem", {}),
            vector_analysis=analysis_result.get("vector", {}),
            graph_analysis=graph_analysis,
            runtime_analysis=runtime_analysis,
            improvements=improvement_suggestions,
        )

        return AnalysisResults(
            filesystem_analysis=analysis_result.get("filesystem", {}),
            vector_analysis=analysis_result.get("vector", {}),
            graph_analysis=graph_analysis,
            runtime_analysis=runtime_analysis,
            improvement_suggestions=improvement_suggestions,
            context_engineered=context_engineered,
        )

    async def improve(self, analysis_results: AnalysisResults) -> Dict[str, Any]:
        """
        Apply improvements based on analysis results.

        Args:
            analysis_results: Results from analyze() call

        Returns:
            Dictionary containing improvement outcomes
        """
        await self._ensure_components_initialized()

        logger.info("Applying improvements based on analysis")

        # Use context engineering to create LLM-ready improvement requests
        improvement_context = analysis_results.context_engineered

        # Apply improvements through the reasoning capability with kaia-guardrails protection
        try:
            # Import kaia-guardrails for safety verification
            from kaia_guardrails.behavior_guardian import get_behavior_capture
            from kaia_guardrails.safety_rails import SafetyRails

            # Create safety rails for this improvement session
            safety_rails = SafetyRails(self.project_path, get_behavior_capture())

            # Use safety rails to verify improvements
            async with safety_rails.protection_mode():
                improvement_results = await self._context_engineer.execute_improvements(
                    improvement_context=improvement_context, target_path=self.project_path
                )

            return improvement_results

        except ImportError:
            logger.warning("kaia-guardrails not available - proceeding without safety verification")
            # Fallback to direct execution without safety rails
            improvement_results = await self._context_engineer.execute_improvements(
                improvement_context=improvement_context, target_path=self.project_path
            )
            return improvement_results

    async def analyze_and_improve(
        self,
        target_path: Optional[Union[str, Path]] = None,
        include_runtime: bool = True,
        auto_apply: bool = False,
    ) -> AnalysisResults:
        """
        Perform comprehensive analysis and optionally apply improvements.

        Args:
            target_path: Specific path to analyze (defaults to project_path)
            include_runtime: Whether to include runtime tracing analysis
            auto_apply: Whether to automatically apply improvements

        Returns:
            AnalysisResults with improvement outcomes included
        """
        # Perform analysis
        results = await self.analyze(
            target_path=target_path, include_runtime=include_runtime, include_improvements=True
        )

        # Apply improvements if requested
        if auto_apply and results.improvement_suggestions:
            improvement_outcomes = await self.improve(results)
            results.context_engineered["improvement_outcomes"] = improvement_outcomes

        return results

    async def self_improve(self) -> Dict[str, Any]:
        """
        Run self-improvement analysis on vibelint itself.

        Returns:
            Dictionary containing self-improvement results
        """
        await self._ensure_components_initialized()

        logger.info("Starting vibelint self-improvement analysis")

        # Analyze vibelint itself
        vibelint_path = Path(__file__).parent
        self_analysis = await self.analyze(
            target_path=vibelint_path, include_runtime=True, include_improvements=True
        )

        # Run self-improvement system
        self_improvements = await self._self_improvement.analyze_and_improve(vibelint_path)

        # Combine results
        self_analysis.self_improvements = self_improvements

        return {
            "analysis": self_analysis,
            "improvements": self_improvements,
            "meta": {
                "analyzed_path": str(vibelint_path),
                "capabilities": self.get_assigned_capabilities(),
                "timestamp": self._context_engineer.get_natural_timestamp(),
            },
        }

    async def get_project_context(
        self, query: Optional[str] = None, max_context_size: int = 32000
    ) -> Dict[str, Any]:
        """
        Get engineered context for the project suitable for LLM consumption.

        Args:
            query: Optional query to focus context generation
            max_context_size: Maximum context size in tokens

        Returns:
            Dictionary containing engineered context
        """
        await self._ensure_components_initialized()

        # Perform lightweight analysis for context
        analysis = await self.analyze(include_runtime=False, include_improvements=False)

        # Engineer context with optional query focus
        context = await self._context_engineer.engineer_focused_context(
            analysis_results=analysis, query=query, max_tokens=max_context_size
        )

        return context


# Convenience functions for common use cases
async def quick_analyze(
    project_path: Union[str, Path], embedding: str = "vanguard_ensemble", reasoning: str = "claude"
) -> AnalysisResults:
    """Quick analysis with common defaults."""
    engine = VibelintEngine(project_path)
    engine.assign_capability("embedding", embedding)
    engine.assign_capability("reasoning", reasoning)
    return await engine.analyze()


async def self_improve_vibelint() -> Dict[str, Any]:
    """Run self-improvement on vibelint itself."""
    engine = VibelintEngine()
    return await engine.self_improve()


def create_engine(**capabilities) -> VibelintEngine:
    """Create engine with specified capabilities."""
    engine = VibelintEngine()
    for cap_type, implementation in capabilities.items():
        engine.assign_capability(cap_type, implementation)
    return engine


# Export main classes and functions
__all__ = [
    "VibelintEngine",
    "AnalysisResults",
    "CapabilityRegistry",
    "get_llm_config",  # LLM configuration (top-level export)
    "JustificationEngine",  # Justification workflow (top-level export)
    "quick_analyze",
    "self_improve_vibelint",
    "create_engine",
]

__version__ = "0.1.0"
```

---
### File: src/vibelint/__main__.py

```python
"""
Main entry point for vibelint when run as a module.

Allows execution via: python -m vibelint

vibelint/src/vibelint/__main__.py
"""

from vibelint.cli import main

if __name__ == "__main__":
    main()
```

---
### File: src/vibelint/auto_discovery.py

```python
"""
Zero-Config Auto-Discovery for Multi-Project Vibelint

Automatically discovers:
- Project structure (single vs multi-project)
- Available services and their capabilities
- Shared resources (vector stores, LLM endpoints)
- Configuration inheritance without complex config files

Philosophy: Convention over configuration.
"""

from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional

import tomli


@dataclass
class ProjectInfo:
    """Discovered project information."""

    path: Path
    name: str
    type: str  # "library", "service", "orchestrator", "monorepo"
    python_package: Optional[str] = None
    has_pyproject: bool = False
    has_vibelint_config: bool = False
    dependencies: List[str] = None
    entry_points: Dict[str, str] = None


@dataclass
class ServiceCapabilities:
    """Auto-discovered service capabilities."""

    llm_endpoints: List[str] = None
    embedding_endpoints: List[str] = None
    vector_stores: List[str] = None
    validation_types: List[str] = None
    can_orchestrate: bool = False
    has_memory_system: bool = False


class AutoDiscovery:
    """
    Zero-configuration auto-discovery for vibelint multi-project scaling.

    Discovers project structure and capabilities without requiring configuration.
    Works for single projects, monorepos, and distributed microservices.
    """

    def __init__(self, starting_path: Path = None):
        self.starting_path = starting_path or Path.cwd()
        self.discovered_projects: Dict[str, ProjectInfo] = {}
        self.service_capabilities: Dict[str, ServiceCapabilities] = {}
        self.project_topology = {}

    def discover_project_structure(self) -> Dict[str, Any]:
        """
        Discover entire project structure starting from current directory.
        Returns complete topology for vibelint to operate on.
        """
        # Find project root(s)
        root_candidates = self._find_project_roots()

        # Determine if this is single project or multi-project setup
        if len(root_candidates) == 1:
            return self._discover_single_project(root_candidates[0])
        else:
            return self._discover_multi_project(root_candidates)

    def _find_project_roots(self) -> List[Path]:
        """Find all potential project roots."""
        roots = []
        current = self.starting_path

        # Walk up to find project markers
        for parent in [current] + list(current.parents):
            if self._is_project_root(parent):
                roots.append(parent)

            # Stop at git root or filesystem boundary
            if (parent / ".git").exists() or parent.parent == parent:
                break

        # If we found a git root, also search for sub-projects
        if roots:
            git_root = roots[-1] if (roots[-1] / ".git").exists() else roots[0]
            sub_projects = self._find_sub_projects(git_root)
            roots.extend(sub_projects)

        return list(set(roots))  # Deduplicate

    def _is_project_root(self, path: Path) -> bool:
        """Check if a path is a project root."""
        markers = ["pyproject.toml", "setup.py", "package.json", "Cargo.toml", ".git"]
        return any((path / marker).exists() for marker in markers)

    def _find_sub_projects(self, git_root: Path) -> List[Path]:
        """Find sub-projects within a git repository."""
        sub_projects = []

        # Common patterns for sub-projects
        search_patterns = [
            "tools/*/pyproject.toml",
            "services/*/pyproject.toml",
            "packages/*/pyproject.toml",
            "apps/*/pyproject.toml",
            "*/pyproject.toml",  # Direct subdirectories
        ]

        for pattern in search_patterns:
            for config_file in git_root.glob(pattern):
                project_dir = config_file.parent
                if project_dir != git_root:  # Don't include the root again
                    sub_projects.append(project_dir)

        return sub_projects

    def _discover_single_project(self, root: Path) -> Dict[str, Any]:
        """Discover single project structure."""
        project_info = self._analyze_project(root)
        self.discovered_projects[project_info.name] = project_info

        capabilities = self._discover_service_capabilities(project_info)
        self.service_capabilities[project_info.name] = capabilities

        return {
            "topology_type": "single_project",
            "root_project": project_info.name,
            "projects": {project_info.name: project_info},
            "capabilities": {project_info.name: capabilities},
            "routing": self._generate_single_project_routing(project_info, capabilities),
        }

    def _discover_multi_project(self, roots: List[Path]) -> Dict[str, Any]:
        """Discover multi-project structure."""
        projects = {}
        capabilities = {}

        for root in roots:
            project_info = self._analyze_project(root)
            projects[project_info.name] = project_info
            self.discovered_projects[project_info.name] = project_info

            project_capabilities = self._discover_service_capabilities(project_info)
            capabilities[project_info.name] = project_capabilities
            self.service_capabilities[project_info.name] = project_capabilities

        # Determine orchestrator and routing
        orchestrator = self._identify_orchestrator(projects, capabilities)
        routing = self._generate_multi_project_routing(projects, capabilities, orchestrator)

        return {
            "topology_type": "multi_project",
            "orchestrator": orchestrator,
            "projects": projects,
            "capabilities": capabilities,
            "routing": routing,
            "shared_resources": self._discover_shared_resources(projects),
        }

    def _analyze_project(self, path: Path) -> ProjectInfo:
        """Analyze a single project directory."""
        name = path.name

        # Check for Python package
        python_package = None
        if (path / "src").exists():
            # src layout
            src_dirs = [
                d for d in (path / "src").iterdir() if d.is_dir() and not d.name.startswith(".")
            ]
            if src_dirs:
                python_package = src_dirs[0].name
        elif any(
            (path / pkg).exists() and (path / pkg).is_dir()
            for pkg in [name, name.replace("-", "_")]
        ):
            # Direct package layout
            python_package = name.replace("-", "_")

        # Load pyproject.toml if exists
        pyproject_path = path / "pyproject.toml"
        dependencies = []
        entry_points = {}

        if pyproject_path.exists():
            try:
                with open(pyproject_path, "rb") as f:
                    pyproject = tomli.load(f)

                dependencies = pyproject.get("project", {}).get("dependencies", [])
                entry_points = pyproject.get("project", {}).get("entry-points", {})

                # Override name if specified in pyproject
                project_name = pyproject.get("project", {}).get("name")
                if project_name:
                    name = project_name

            except Exception:
                pass  # Continue with defaults

        # Determine project type
        project_type = self._infer_project_type(path, dependencies, entry_points)

        return ProjectInfo(
            path=path,
            name=name,
            type=project_type,
            python_package=python_package,
            has_pyproject=pyproject_path.exists(),
            has_vibelint_config=self._has_vibelint_config(path),
            dependencies=dependencies,
            entry_points=entry_points,
        )

    def _infer_project_type(
        self, path: Path, dependencies: List[str], entry_points: Dict[str, str]
    ) -> str:
        """Infer project type from structure and dependencies."""
        # Check for orchestrator patterns
        orchestrator_indicators = ["orchestrat", "guardrail", "coordinator", "manager", "control"]
        if any(indicator in path.name.lower() for indicator in orchestrator_indicators):
            return "orchestrator"

        # Check for service patterns
        service_indicators = ["service", "api", "worker", "daemon"]
        if any(indicator in path.name.lower() for indicator in service_indicators):
            return "service"

        # Check dependencies
        dep_strings = " ".join(dependencies).lower()
        if any(dep in dep_strings for dep in ["fastapi", "flask", "django"]):
            return "service"
        elif any(dep in dep_strings for dep in ["orchestrat", "celery", "prefect"]):
            return "orchestrator"

        # Check for tools directory pattern
        if "tools" in str(path):
            return "library"

        # Default based on structure
        if (path / "tests").exists() and (path / "src").exists():
            return "library"
        else:
            return "service"

    def _has_vibelint_config(self, path: Path) -> bool:
        """Check if project has vibelint configuration."""
        config_locations = [
            path / "pyproject.toml",
            path / "vibelint.toml",
            path / ".vibelint.toml",
        ]

        for config_path in config_locations:
            if config_path.exists():
                try:
                    with open(config_path, "rb") as f:
                        config = tomli.load(f)
                        if "tool" in config and "vibelint" in config["tool"]:
                            return True
                except Exception:
                    continue

        return False

    def _discover_service_capabilities(self, project: ProjectInfo) -> ServiceCapabilities:
        """Discover what capabilities a project/service has."""
        capabilities = ServiceCapabilities()

        # Check for LLM endpoint configuration
        if project.has_vibelint_config:
            config = self._load_project_config(project)
            llm_config = config.get("tool", {}).get("vibelint", {}).get("llm", {})

            endpoints = []
            if llm_config.get("fast_api_url"):
                endpoints.append("fast_llm")
            if llm_config.get("orchestrator_api_url"):
                endpoints.append("orchestrator_llm")

            capabilities.llm_endpoints = endpoints

            # Check embedding endpoints
            embedding_config = config.get("tool", {}).get("vibelint", {}).get("embeddings", {})
            embedding_endpoints = []
            if embedding_config.get("code_api_url"):
                embedding_endpoints.append("code_embeddings")
            if embedding_config.get("natural_api_url"):
                embedding_endpoints.append("natural_embeddings")

            capabilities.embedding_endpoints = embedding_endpoints

            # Check vector store
            vector_config = config.get("tool", {}).get("vibelint", {}).get("vector_store", {})
            if vector_config.get("backend"):
                capabilities.vector_stores = [vector_config["backend"]]

        # Check for orchestration capabilities
        if project.type == "orchestrator":
            capabilities.can_orchestrate = True

        # Check for memory system (look for specific imports/files)
        if project.python_package:
            memory_indicators = [
                "memory_system",
                "memory_manager",
                "conflict_resolver",
                "guardrails",
            ]

            for indicator in memory_indicators:
                potential_files = [
                    project.path / "src" / project.python_package / f"{indicator}.py",
                    project.path / project.python_package / f"{indicator}.py",
                ]

                if any(f.exists() for f in potential_files):
                    capabilities.has_memory_system = True
                    break

        # Determine validation types this project can handle
        validation_types = ["single_file"]  # All projects can do basic validation

        if project.type in ["orchestrator", "service"]:
            validation_types.extend(["project_wide", "architecture"])

        if capabilities.has_memory_system:
            validation_types.extend(["memory_conflicts", "security"])

        capabilities.validation_types = validation_types

        return capabilities

    def _load_project_config(self, project: ProjectInfo) -> Dict[str, Any]:
        """Load project configuration."""
        config_path = project.path / "pyproject.toml"
        if config_path.exists():
            try:
                with open(config_path, "rb") as f:
                    return tomli.load(f)
            except Exception:
                pass
        return {}

    def _identify_orchestrator(
        self, projects: Dict[str, ProjectInfo], capabilities: Dict[str, ServiceCapabilities]
    ) -> Optional[str]:
        """Identify which project should act as orchestrator."""
        # Explicit orchestrator type
        for name, project in projects.items():
            if project.type == "orchestrator":
                return name

        # Project with orchestration capabilities
        for name, caps in capabilities.items():
            if caps.can_orchestrate:
                return name

        # Project with memory system (good orchestrator candidate)
        for name, caps in capabilities.items():
            if caps.has_memory_system:
                return name

        # Fallback: project with most capabilities
        if capabilities:
            best_project = max(capabilities.items(), key=lambda x: len(x[1].validation_types or []))
            return best_project[0]

        return None

    def _generate_single_project_routing(
        self, project: ProjectInfo, capabilities: ServiceCapabilities
    ) -> Dict[str, Any]:
        """Generate routing for single project setup."""
        return {
            "validation_routing": {
                vtype: [project.name]
                for vtype in (capabilities.validation_types or ["single_file"])
            },
            "llm_routing": {"primary": project.name if capabilities.llm_endpoints else None},
            "embedding_routing": {
                "primary": project.name if capabilities.embedding_endpoints else None
            },
        }

    def _generate_multi_project_routing(
        self,
        projects: Dict[str, ProjectInfo],
        capabilities: Dict[str, ServiceCapabilities],
        orchestrator: Optional[str],
    ) -> Dict[str, Any]:
        """Generate routing for multi-project setup."""
        routing = {"validation_routing": {}, "llm_routing": {}, "embedding_routing": {}}

        # Validation routing - distribute based on capabilities
        all_validation_types = set()
        for caps in capabilities.values():
            if caps.validation_types:
                all_validation_types.update(caps.validation_types)

        for vtype in all_validation_types:
            capable_services = [
                name
                for name, caps in capabilities.items()
                if caps.validation_types and vtype in caps.validation_types
            ]
            routing["validation_routing"][vtype] = capable_services

        # LLM routing - prefer orchestrator, fallback to any with LLM
        llm_services = [name for name, caps in capabilities.items() if caps.llm_endpoints]

        if orchestrator and orchestrator in llm_services:
            routing["llm_routing"]["primary"] = orchestrator
            routing["llm_routing"]["fallback"] = [s for s in llm_services if s != orchestrator]
        elif llm_services:
            routing["llm_routing"]["primary"] = llm_services[0]
            routing["llm_routing"]["fallback"] = llm_services[1:]

        # Embedding routing - distribute by type if possible
        embedding_services = [
            name for name, caps in capabilities.items() if caps.embedding_endpoints
        ]

        if embedding_services:
            routing["embedding_routing"]["primary"] = embedding_services[0]
            routing["embedding_routing"]["all"] = embedding_services

        return routing

    def _discover_shared_resources(self, projects: Dict[str, ProjectInfo]) -> Dict[str, Any]:
        """Discover shared resources across projects."""
        shared = {"vector_stores": [], "common_dependencies": [], "shared_configs": []}

        # Find common vector store configurations
        vector_stores = set()
        for project in projects.values():
            config = self._load_project_config(project)
            vector_config = config.get("tool", {}).get("vibelint", {}).get("vector_store", {})
            if vector_config.get("qdrant_collection"):
                vector_stores.add(vector_config["qdrant_collection"])

        shared["vector_stores"] = list(vector_stores)

        # Find common dependencies
        all_deps = []
        for project in projects.values():
            if project.dependencies:
                all_deps.extend(project.dependencies)

        # Count dependency frequency
        dep_counts = {}
        for dep in all_deps:
            dep_counts[dep] = dep_counts.get(dep, 0) + 1

        # Dependencies used by multiple projects
        common_deps = [dep for dep, count in dep_counts.items() if count > 1]
        shared["common_dependencies"] = common_deps

        return shared

    def get_vibelint_config(self) -> Dict[str, Any]:
        """
        Generate vibelint configuration based on auto-discovery.
        This replaces complex manual configuration.
        """
        structure = self.discover_project_structure()

        # Generate simple, unified config
        config = {
            "discovered_topology": structure["topology_type"],
            "auto_routing": structure["routing"],
            "services": {
                name: {
                    "path": str(info.path),
                    "type": info.type,
                    "capabilities": structure["capabilities"].get(name, {}),
                }
                for name, info in structure["projects"].items()
            },
        }

        # Add shared resources if multi-project
        if structure["topology_type"] == "multi_project":
            config["shared_resources"] = structure.get("shared_resources", {})
            config["orchestrator"] = structure.get("orchestrator")

        return config


# Convenience function for immediate discovery
def discover_and_configure(starting_path: Path = None) -> Dict[str, Any]:
    """
    One-function auto-discovery and configuration for vibelint.
    This is the main entry point for zero-config scaling.
    """
    discovery = AutoDiscovery(starting_path)
    return discovery.get_vibelint_config()


# Integration with existing vibelint
def integrate_with_vibelint_core():
    """
    Modify vibelint core to use auto-discovery by default.
    This makes scaling seamless without config changes.
    """
    # This would be integrated into vibelint's main config loading
    discovered_config = discover_and_configure()

    return {
        "auto_discovered": True,
        "manual_config_override": False,  # Set to True to use manual config instead
        "discovered_config": discovered_config,
    }
```

---
### File: src/vibelint/cli/__init__.py

```python
"""
Modular CLI architecture for vibelint.

This package breaks down the monolithic CLI into logical, maintainable modules:

- core.py: Main CLI group and shared utilities
- validation.py: check, validate commands
- analysis.py: namespace, snapshot, diagnostics commands
- maintenance.py: regen-docstrings, rollback, setup commands
- ai.py: thinking-tokens, justify commands
- presentation.py: Result formatting and display utilities

Each module is focused on a specific domain, making the codebase more maintainable
and easier to extend.

vibelint/src/vibelint/cli/__init__.py
"""

import logging
import sys

from .core import VibelintContext, cli

__all__ = ["cli", "main"]


def main() -> None:
    """
    Main entry point for the vibelint CLI application.

    This function provides the entry point specified in pyproject.toml.
    """
    try:
        cli(obj=VibelintContext(), prog_name="vibelint")
    except SystemExit as e:
        sys.exit(e.code)
    except (RuntimeError, ValueError, OSError, ImportError) as e:
        from rich.console import Console

        console = Console()
        console.print(f"[bold red]An unexpected error occurred: {e}[/bold red]")

        logger = logging.getLogger(__name__)
        # Check if logger was configured before logging error
        if logger.hasHandlers():
            logger.error("Unhandled exception in CLI execution.", exc_info=True)
        else:
            # Fallback if error happened before logging setup
            import traceback

            traceback.print_exc()
        sys.exit(1)
```

---
### File: src/vibelint/cli/ai.py

```python
"""
AI-powered commands: justify, thinking-tokens.

These commands use LLM capabilities for code analysis and explanation.

vibelint/src/vibelint/cli/ai.py
"""

import logging
from pathlib import Path

import click
from rich.console import Console

from .core import VibelintContext, cli

console = Console()
logger = logging.getLogger(__name__)


@cli.command("justify")
@click.argument("file_path", type=click.Path(exists=True, path_type=Path))
@click.option("--rule-id", help="Specific rule to justify")
@click.pass_context
def justify(ctx: click.Context, file_path: Path, rule_id: str | None) -> None:
    """Justify code decisions using static analysis and minimal LLM calls."""
    vibelint_ctx: VibelintContext = ctx.obj
    project_root = vibelint_ctx.project_root
    assert project_root is not None, "Project root missing"

    console.print("[bold purple]🤖 Analyzing Code Justification...[/bold purple]\n")

    try:
        from ..justification import JustificationEngine
        from ..config import load_config

        # Load config
        config = load_config(project_root)

        # Create justification engine
        engine = JustificationEngine(config)

        # Read file content
        content = file_path.read_text(encoding="utf-8")

        # Perform justification analysis
        result = engine.justify_file(file_path, content)

        # Save session logs
        detailed_log, summary_log = engine.save_session_logs(str(file_path), result)

        # Display results
        console.print(f"[green]✅ Analyzed {file_path}[/green]")
        console.print(f"Quality Score: {result.quality_score:.1%}")
        console.print(f"[dim]Logs saved to: {summary_log}[/dim]")

        if rule_id:
            # Filter to specific rule if requested
            relevant_justifications = [
                j for j in result.justifications
                if rule_id.lower() in j.justification.lower()
            ]
            console.print(f"\nJustifications for rule '{rule_id}':")
            for just in relevant_justifications:
                console.print(f"  • {just.element_name}: {just.justification}")
        else:
            # Show all justifications
            console.print(f"\nFound {len(result.justifications)} code elements:")
            for just in result.justifications:
                confidence_color = "green" if just.confidence > 0.7 else "yellow" if just.confidence > 0.4 else "red"
                console.print(f"  [{confidence_color}]•[/] {just.element_name} ({just.element_type}): {just.justification}")

        if result.redundancies_found:
            console.print(f"\n[yellow]⚠️ Potential redundancies:[/yellow]")
            for redundancy in result.redundancies_found:
                console.print(f"  • {redundancy}")

        if result.recommendations:
            console.print(f"\n[blue]💡 Recommendations:[/blue]")
            for rec in result.recommendations:
                console.print(f"  • {rec}")

    except Exception as e:
        console.print(f"[red]❌ Justification failed: {e}[/red]")
        logger.error(f"Justification error: {e}")


@cli.command("compare")
@click.argument("method1", type=str)
@click.argument("method2", type=str)
@click.pass_context
def compare_methods(ctx: click.Context, method1: str, method2: str) -> None:
    """Compare two methods for similarity using fast LLM."""
    vibelint_ctx: VibelintContext = ctx.obj
    project_root = vibelint_ctx.project_root
    assert project_root is not None, "Project root missing"

    console.print("[bold cyan]🔍 Comparing Methods...[/bold cyan]\n")

    try:
        from ..justification import JustificationEngine
        from ..config import load_config

        # Parse method specifications (file:method format)
        try:
            path1, name1 = method1.rsplit(":", 1)
            path2, name2 = method2.rsplit(":", 1)
        except ValueError:
            console.print("[red]❌ Use format: file_path:method_name[/red]")
            return

        # Load config and create engine
        config = load_config(project_root)
        engine = JustificationEngine(config)

        # Compare methods
        result = engine.justify_method_comparison(path1, name1, path2, name2)

        # Display results
        if result["similar"]:
            console.print(f"[yellow]⚠️ Methods appear similar (confidence: {result['confidence']:.1%})[/yellow]")
        else:
            console.print(f"[green]✅ Methods are distinct (confidence: {result['confidence']:.1%})[/green]")

        console.print(f"Reasoning: {result.get('reason', result.get('reasoning', 'No explanation'))}")

    except Exception as e:
        console.print(f"[red]❌ Comparison failed: {e}[/red]")
        logger.error(f"Method comparison error: {e}")


@cli.command("status")
@click.pass_context
def llm_status(ctx: click.Context) -> None:
    """Show LLM configuration status."""
    vibelint_ctx: VibelintContext = ctx.obj
    project_root = vibelint_ctx.project_root

    console.print("[bold cyan]🔧 LLM Configuration Status[/bold cyan]\n")

    try:
        from ..config import load_config
        from ..llm import create_llm_manager

        config = load_config(project_root)
        llm_manager = create_llm_manager(config)

        if llm_manager:
            status = llm_manager.get_status()
            console.print(f"Fast LLM: {'✅ Available' if status['fast_configured'] else '❌ Not configured'}")
            console.print(f"Orchestrator LLM: {'✅ Available' if status['orchestrator_configured'] else '❌ Not configured'}")
        else:
            console.print("[red]❌ No LLM configuration found[/red]")

    except Exception as e:
        console.print(f"[red]❌ Status check failed: {e}[/red]")
```

---
### File: src/vibelint/cli/analysis.py

```python
"""
Analysis commands: namespace, snapshot, diagnostics.

These commands handle project analysis and introspection.

vibelint/src/vibelint/cli/analysis.py
"""

import logging
from pathlib import Path

import click
from rich.console import Console

from .core import VibelintContext, cli

console = Console()
logger = logging.getLogger(__name__)


@cli.command("namespace")
@click.option(
    "--output",
    type=click.Path(path_type=Path),
    help="Write detailed namespace report to file",
)
@click.pass_context
def namespace(ctx: click.Context, output: Path | None) -> None:
    """
    Analyze namespace collisions and import patterns.

    Detects:
    - Conflicting module names across packages
    - Import shadowing issues
    - Circular import risks
    - Package structure problems

    Examples:
      vibelint namespace                    # Show namespace analysis
      vibelint namespace --output report.json  # Save detailed report
    """
    vibelint_ctx: VibelintContext = ctx.obj
    project_root = vibelint_ctx.project_root
    assert project_root is not None, "Project root missing"

    console.print("[bold blue]🔍 Analyzing Namespace Collisions...[/bold blue]\n")

    # TODO: Move implementation from monolithic cli.py
    console.print("[yellow]⚠️  Namespace command moved to modular structure[/yellow]")
    console.print(f"   Project: {project_root}")
    console.print(f"   Output: {output or 'console only'}")


@cli.command("snapshot")
@click.option(
    "--output",
    type=click.Path(path_type=Path),
    required=True,
    help="Output file for project snapshot",
)
@click.pass_context
def snapshot(ctx: click.Context, output: Path) -> None:
    """
    Create a comprehensive snapshot of the current project state.

    Captures:
    - Current codebase structure and metrics
    - Active validation rules and their status
    - Configuration settings and environment
    - Dependencies and package information

    Examples:
      vibelint snapshot --output project_state.json
    """
    vibelint_ctx: VibelintContext = ctx.obj
    project_root = vibelint_ctx.project_root
    assert project_root is not None, "Project root missing"

    console.print("[bold green]📸 Creating Project Snapshot...[/bold green]\n")

    # Import and use the actual snapshot implementation
    from ..config import load_config
    from ..snapshot import create_snapshot

    try:
        config = load_config(project_root)
        create_snapshot(output, [project_root], config)
        console.print(f"[green]✅ Snapshot created: {output}[/green]")
    except Exception as e:
        console.print(f"[red]❌ Snapshot failed: {e}[/red]")
        raise


@cli.command("diagnostics")
@click.pass_context
def diagnostics_cmd(ctx: click.Context) -> None:
    """
    Run system diagnostics and configuration validation.

    Checks:
    - vibelint installation and configuration
    - LLM endpoint connectivity and authentication
    - Vector store configuration and connectivity
    - Python environment and dependencies
    - Project structure and configuration validity

    Examples:
      vibelint diagnostics              # Run all diagnostic checks
    """
    vibelint_ctx: VibelintContext = ctx.obj
    project_root = vibelint_ctx.project_root

    console.print("[bold cyan]🔧 Running System Diagnostics...[/bold cyan]\n")

    # TODO: Move implementation from monolithic cli.py
    console.print("[yellow]⚠️  Diagnostics command moved to modular structure[/yellow]")
    console.print(f"   Project: {project_root or 'Not found'}")

    # This would be a good place to test the new config loading
    if project_root:
        from ..config import load_config

        config = load_config(project_root)
        console.print(f"   Config loaded: {config.is_present()}")
        console.print(f"   Config source: {config.project_root}")
    else:
        console.print("   [red]No project configuration found[/red]")
```

---
### File: src/vibelint/cli/core.py

```python
"""
Core CLI group and shared context for vibelint.

This module provides the base CLI group and shared context object.
Individual command modules register their commands with this group.

Responsibility: CLI structure and shared state only.
Command logic belongs in validators/, workflows/, and other subsystems.

vibelint/src/vibelint/cli/core.py
"""

import logging
from dataclasses import dataclass
from pathlib import Path

import click
from rich.console import Console

console = Console()
logger = logging.getLogger(__name__)


@dataclass
class VibelintContext:
    """Shared context for CLI commands."""

    project_root: Path | None = None
    config_path: Path | None = None
    verbose: bool = False


@click.group()
@click.option(
    "--project-root",
    type=click.Path(exists=True, file_okay=False, dir_okay=True, path_type=Path),
    help="Project root directory (auto-detected if not specified)",
)
@click.option(
    "--config",
    type=click.Path(exists=True, dir_okay=False, path_type=Path),
    help="Configuration file path",
)
@click.option("--verbose", "-v", is_flag=True, help="Enable verbose output")
@click.pass_context
def cli(
    ctx: click.Context,
    project_root: Path | None,
    config: Path | None,
    verbose: bool,
) -> None:
    """
    vibelint: Intelligent code quality and style validator.

    Advanced linting with LLM-powered analysis, namespace collision detection,
    and project-wide validation rules.
    """
    # Auto-detect project root if not specified
    if not project_root:
        current = Path.cwd()
        # Walk up looking for pyproject.toml or .git
        for parent in [current] + list(current.parents):
            if (parent / "pyproject.toml").exists() or (parent / ".git").exists():
                project_root = parent
                break

    # Store context for subcommands
    ctx.obj = VibelintContext(
        project_root=project_root,
        config_path=config,
        verbose=verbose,
    )

    # Configure logging
    if verbose:
        logging.basicConfig(level=logging.DEBUG)
    else:
        logging.basicConfig(level=logging.INFO)


# Import command modules to register them with the CLI group
# Each module registers its commands using @cli.command() decorators
def register_commands():
    """Import all command modules to register their commands."""
    try:
        from . import ai  # justify, thinking-tokens
        from . import analysis  # namespace, snapshot, diagnostics
        from . import maintenance  # setup, rollback, regen-docstrings
        from . import validation  # check, validate

        logger.debug("All command modules registered successfully")
    except ImportError as e:
        logger.error(f"Failed to register command modules: {e}")
        # Don't fail completely - some modules might work


# Register commands on import
register_commands()
```

---
### File: src/vibelint/cli/maintenance.py

```python
"""
Maintenance commands: setup, rollback, regen-docstrings.

These commands handle project setup, restoration, and code generation.

vibelint/src/vibelint/cli/maintenance.py
"""

import logging
from pathlib import Path

import click
from rich.console import Console

from .core import VibelintContext, cli

console = Console()
logger = logging.getLogger(__name__)


@cli.command("setup")
@click.option(
    "--project-root",
    type=click.Path(exists=True, file_okay=False, dir_okay=True, path_type=Path),
    help="Project root directory (defaults to current directory)",
)
@click.option(
    "--config-only",
    is_flag=True,
    help="Only create configuration files, skip other setup steps",
)
@click.pass_context
def setup(ctx: click.Context, project_root: Path | None, config_only: bool) -> None:
    """
    Initialize vibelint in the current project.

    Creates:
    - Project configuration in pyproject.toml
    - Git hooks (if git repository detected)
    - Sample validation rules
    - IDE integration files

    Examples:
      vibelint setup                           # Setup in current directory
      vibelint setup --project-root ./myproj   # Setup in specific directory
      vibelint setup --config-only             # Only create config files
    """
    vibelint_ctx: VibelintContext = ctx.obj
    target_root = project_root or vibelint_ctx.project_root or Path.cwd()

    console.print("[bold green]🔧 Setting up vibelint...[/bold green]\n")

    # TODO: Move implementation from monolithic cli.py
    console.print("[yellow]⚠️  Setup command moved to modular structure[/yellow]")
    console.print(f"   Target directory: {target_root}")
    console.print(f"   Config only: {config_only}")


@cli.command("rollback")
@click.option(
    "--commit",
    help="Git commit hash to rollback to",
)
@click.option(
    "--snapshot-file",
    type=click.Path(exists=True, dir_okay=False, path_type=Path),
    help="Snapshot file to restore from",
)
@click.option(
    "--dry-run",
    is_flag=True,
    help="Show what would be rolled back without making changes",
)
@click.pass_context
def rollback(
    ctx: click.Context,
    commit: str | None,
    snapshot_file: Path | None,
    dry_run: bool,
) -> None:
    """
    Rollback project to a previous state.

    Can restore from:
    - Git commit hash
    - vibelint snapshot file
    - Automatically detected safe point

    Examples:
      vibelint rollback --commit abc123       # Rollback to specific commit
      vibelint rollback --snapshot-file state.json  # Restore from snapshot
      vibelint rollback --dry-run             # Preview rollback actions
    """
    vibelint_ctx: VibelintContext = ctx.obj
    project_root = vibelint_ctx.project_root
    assert project_root is not None, "Project root missing"

    if not commit and not snapshot_file:
        console.print("[red]❌ Either --commit or --snapshot-file must be specified[/red]")
        raise click.Abort()

    console.print("[bold yellow]⏪ Rolling back project state...[/bold yellow]\n")

    # TODO: Move implementation from monolithic cli.py
    console.print("[yellow]⚠️  Rollback command moved to modular structure[/yellow]")
    console.print(f"   Project: {project_root}")
    console.print(f"   Commit: {commit or 'Not specified'}")
    console.print(f"   Snapshot: {snapshot_file or 'Not specified'}")
    console.print(f"   Dry run: {dry_run}")


@cli.command("regen-docstrings")
@click.option(
    "--files",
    multiple=True,
    type=click.Path(exists=True, dir_okay=False, path_type=Path),
    help="Specific files to regenerate docstrings for",
)
@click.option(
    "--force",
    is_flag=True,
    help="Overwrite existing docstrings",
)
@click.pass_context
def regen_docstrings(
    ctx: click.Context,
    files: tuple[Path, ...],
    force: bool,
) -> None:
    """
    Regenerate docstrings for Python functions and classes.

    Uses LLM to generate comprehensive docstrings that follow:
    - Google/NumPy/Sphinx docstring conventions
    - Type hint compatibility
    - Project-specific patterns and terminology

    Examples:
      vibelint regen-docstrings                    # All Python files
      vibelint regen-docstrings --files module.py # Specific files
      vibelint regen-docstrings --force           # Overwrite existing
    """
    vibelint_ctx: VibelintContext = ctx.obj
    project_root = vibelint_ctx.project_root
    assert project_root is not None, "Project root missing"

    console.print("[bold blue]📝 Regenerating Docstrings...[/bold blue]\n")

    # TODO: Move implementation from monolithic cli.py
    console.print("[yellow]⚠️  Regen-docstrings command moved to modular structure[/yellow]")
    console.print(f"   Project: {project_root}")
    console.print(f"   Files: {list(files) if files else 'All Python files'}")
    console.print(f"   Force overwrite: {force}")
```

---
### File: src/vibelint/cli/validation.py

```python
"""
Validation commands: check and validate.

These commands handle the core linting and validation functionality.

vibelint/src/vibelint/cli/validation.py
"""

import logging
from pathlib import Path
from typing import Optional

import click
from rich.console import Console

from ..config import Config, load_config
from .core import VibelintContext, cli

console = Console()
logger = logging.getLogger(__name__)


@cli.command("check")
@click.argument(
    "targets",
    nargs=-1,
    type=click.Path(exists=True, path_type=Path),
    required=False,
)
@click.option("--yes", is_flag=True, help="Skip confirmation prompt for large directories.")
@click.option(
    "--output-format",
    type=click.Choice(["human", "json", "natural", "sarif", "llm"]),
    default="human",
    help="Output format for results.",
)
@click.option(
    "--categories",
    help="Comma-separated list of rule categories to run: core, static, ai, or 'all' for everything. Default: 'all'.",
)
@click.option(
    "--exclude-ai",
    is_flag=True,
    help="Exclude AI-powered validators (faster, no API calls).",
)
@click.option(
    "--rules",
    help="Comma-separated list of specific rules to run (overrides categories).",
)
@click.option(
    "--report",
    is_flag=True,
    help="Generate detailed analysis reports (saved to .vibelint-reports/).",
)
@click.pass_context
def check(
    ctx: click.Context,
    targets: tuple[Path, ...],
    yes: bool,
    output_format: str,
    categories: Optional[str],
    exclude_ai: bool,
    rules: Optional[str],
    report: bool,
) -> None:
    """
    Run vibelint validation on the specified targets.

    By default, runs ALL rules configured in pyproject.toml including AI validators.
    Use --exclude-ai for faster runs without API calls.

    TARGETS can be files or directories. If none specified, checks entire project.

    Examples:
      vibelint check                    # Check entire project
      vibelint check src/               # Check src directory
      vibelint check file.py            # Check single file
      vibelint check --exclude-ai       # Skip AI validators
      vibelint check --categories=core  # Only core rules
    """
    vibelint_ctx: VibelintContext = ctx.obj
    project_root = vibelint_ctx.project_root
    assert project_root is not None, "Project root missing in check command"

    # Don't show UI messages for machine-readable formats
    if output_format in ["human", "natural"]:
        console.print("\n[bold magenta]Initiating Vibe Check...[/bold magenta]\n")

    logger.debug(f"Running 'check' command (yes={yes}, report={report})")

    # Load configuration using the file-based approach
    config: Config = load_config(project_root)
    if config.project_root is None:
        logger.error("Project root lost after config load. Aborting Vibe Check.")
        ctx.exit(1)

    # TODO: Implement the actual validation logic
    # This would move from the monolithic cli.py
    console.print("[yellow]⚠️  Check command implementation moved to modular structure[/yellow]")
    console.print(f"   Project root: {project_root}")
    console.print(f"   Config loaded: {config.is_present()}")
    console.print(f"   Targets: {targets if targets else 'entire project'}")
    console.print(f"   Format: {output_format}")
    console.print(f"   Categories: {categories or 'all'}")
    console.print(f"   Exclude AI: {exclude_ai}")


@cli.command("validate")
@click.argument(
    "path",
    type=click.Path(exists=True, path_type=Path),
    required=True,
)
@click.option(
    "--output-format",
    type=click.Choice(["human", "json", "natural"]),
    default="json",
    help="Output format for results (default: json for programmatic use).",
)
@click.option(
    "--recursive",
    is_flag=True,
    help="Recursively validate all Python files in directory.",
)
@click.pass_context
def validate_cmd(ctx: click.Context, path: Path, output_format: str, recursive: bool) -> None:
    """
    Validate a specific file or directory (optimized for single-file use).

    This command is designed for:
    - IDE integrations and editor plugins
    - CI/CD pipeline file-by-file validation
    - Pre-commit hooks and git hooks
    - Quick validation of individual files

    Unlike 'check', this command:
    - Focuses on single file or directory validation
    - Uses file-based config discovery (starts from file location)
    - Optimized for speed and minimal output
    - JSON output by default for programmatic use

    Examples:
      vibelint validate src/module.py           # Validate single file
      vibelint validate src/ --recursive        # Validate directory recursively
      vibelint validate file.py --output-format human  # Human-readable output
    """
    vibelint_ctx: VibelintContext = ctx.obj
    project_root = vibelint_ctx.project_root

    # For validate command, we use file-based config discovery
    # This matches how other linters work (ESLint, Black, etc.)
    from ..config import load_config

    # Start config search from the file/directory being validated
    start_path = path if path.is_dir() else path.parent
    config: Config = load_config(start_path)

    logger.debug(f"Validating: {path}")
    logger.debug(f"Config source: {config.project_root}")
    logger.debug(f"Recursive: {recursive}")

    # Import and use the single file validation workflow
    try:
        from ..workflows.implementations.single_file_validation import \
            SingleFileValidationWorkflow

        # Create workflow instance
        workflow = SingleFileValidationWorkflow(config)

        # Determine files to validate
        files_to_validate = []

        if path.is_file():
            if path.suffix == ".py":
                files_to_validate = [path]
            else:
                console.print(f"[bold red]Error:[/bold red] {path} is not a Python file")
                ctx.exit(1)
        else:
            # Directory validation
            if recursive:
                files_to_validate = list(path.rglob("*.py"))
            else:
                files_to_validate = list(path.glob("*.py"))

            if not files_to_validate:
                console.print("[bold red]Error:[/bold red] No Python files found to validate")
                ctx.exit(1)

        # Validate all files
        results = []
        total_violations = 0
        failed_files = 0

        for file_path in files_to_validate:
            try:
                import time

                start_time = time.time()
                result = workflow.validate_file(file_path)
                execution_time = (time.time() - start_time) * 1000

                file_result = {
                    "file_path": str(file_path),
                    "file_name": file_path.name,
                    "health_score": result.health_score,
                    "violations": [v.to_dict() for v in result.violations],
                    "execution_time_ms": execution_time,
                    "success": True,
                    "error": None,
                }
                results.append(file_result)
                total_violations += len(result.violations)

                if result.violations:
                    failed_files += 1

            except Exception as e:
                failed_files += 1
                file_result = {
                    "file_path": str(file_path),
                    "file_name": file_path.name,
                    "health_score": 0.0,
                    "violations": [],
                    "execution_time_ms": 0.0,
                    "success": False,
                    "error": str(e),
                }
                results.append(file_result)
                console.print(f"[bold red]Error validating {file_path}:[/bold red] {e}")

        # Display results
        if output_format == "json":
            import json

            output = {
                "summary": {
                    "total_files": len(results),
                    "failed_files": failed_files,
                    "total_violations": total_violations,
                },
                "files": results,
            }
            print(json.dumps(output, indent=2))
        else:
            # Natural format display
            total_files = len(results)

            if total_files == 1:
                # Single file display
                result = results[0]
                console.print(f"\nFile: {result['file_name']}")
                console.print(f"Health Score: {result['health_score']}/100")
                console.print(f"Execution Time: {result['execution_time_ms']:.1f}ms")

                if result["success"] and not result["violations"]:
                    console.print("\n[green]No violations found![/green]")
                elif result["violations"]:
                    console.print(f"\n[red]Found {len(result['violations'])} violation(s):[/red]")
                    for violation in result["violations"]:
                        console.print(f"  - {violation}")

                if not result["success"]:
                    console.print(f"[bold red]Error:[/bold red] {result['error']}")
            else:
                # Multi-file summary
                console.print(f"\nRunning validation on {total_files} Python files...")
                console.print("\nValidation Summary")
                console.print(f"Files processed: {total_files}")
                console.print(f"Failed files: {failed_files}")
                console.print(f"Total violations: {total_violations}")

                if failed_files == 0 and total_violations == 0:
                    console.print("\n[green]All files passed validation![/green]")
                else:
                    console.print(
                        f"\n[red]{failed_files} files failed, {total_violations} violations found[/red]"
                    )

        # Determine exit code
        exit_code = 1 if failed_files > 0 or total_violations > 0 else 0
        ctx.exit(exit_code)

    except ImportError as e:
        console.print(f"[bold red]Error:[/bold red] Missing module for validation: {e}")
        ctx.exit(1)
    except Exception as e:
        console.print(f"[bold red]Error:[/bold red] Unexpected error during validation: {e}")
        if logger.level == logging.DEBUG:
            import traceback

            console.print(traceback.format_exc())
        ctx.exit(1)
```

---
### File: src/vibelint/cli.py

```python

```

---
### File: src/vibelint/config.py

```python
"""
Configuration loading for vibelint.

Reads settings *only* from pyproject.toml under the [tool.vibelint] section.
No default values are assumed by this module. Callers must handle missing
configuration keys.

vibelint/src/vibelint/config.py
"""

import logging
import sys
from collections.abc import Mapping
from pathlib import Path
from typing import Any, Union

from vibelint.utils import find_package_root

logger = logging.getLogger(__name__)

if sys.version_info >= (3, 11):

    import tomllib
else:

    try:

        import tomli as tomllib
    except ImportError as e:

        raise ImportError(
            "vibelint requires Python 3.11+ or the 'tomli' package "
            "to parse pyproject.toml on Python 3.10. "
            "Hint: Try running: pip install tomli"
        ) from e


class Config:
    """
    Holds the vibelint configuration loaded *exclusively* from pyproject.toml.

    Provides access to the project root and the raw configuration dictionary.
    It does *not* provide default values for missing keys. Callers must
    check for the existence of required settings.

    Attributes:
    project_root: The detected root of the project containing pyproject.toml.
    Can be None if pyproject.toml is not found.
    settings: A read-only view of the dictionary loaded from the
    [tool.vibelint] section of pyproject.toml. Empty if the
    file or section is missing or invalid.

    vibelint/src/vibelint/config.py
    """

    def __init__(self, project_root: Path | None, config_dict: dict[str, Any]):
        """
        Initializes Config.

        vibelint/src/vibelint/config.py
        """
        self._project_root = project_root
        self._config_dict = config_dict.copy()

    @property
    def project_root(self) -> Path | None:
        """
        The detected project root directory, or None if not found.

        vibelint/src/vibelint/config.py
        """
        return self._project_root

    @property
    def settings(self) -> Mapping[str, Union[str, bool, int, list, dict]]:
        """
        Read-only view of the settings loaded from [tool.vibelint].

        vibelint/src/vibelint/config.py
        """
        return self._config_dict

    @property
    def ignore_codes(self) -> list[str]:
        """
        Returns the list of error codes to ignore, from config or empty list.

        vibelint/src/vibelint/config.py
        """
        ignored = self.get("ignore", [])
        if isinstance(ignored, list) and all(isinstance(item, str) for item in ignored):
            return ignored

        # Handle invalid configuration
        if ignored:
            logger.warning(
                "Configuration key 'ignore' in [tool.vibelint] is not a list of strings. Ignoring it."
            )

        return []

    def get(
        self, key: str, default: Union[str, bool, int, list, dict, None] = None
    ) -> Union[str, bool, int, list, dict, None]:
        """
        Gets a value from the loaded settings, returning default if not found.

        vibelint/src/vibelint/config.py
        """
        return self._config_dict.get(key, default)

    def __getitem__(self, key: str) -> Union[str, bool, int, list, dict]:
        """
        Gets a value, raising KeyError if the key is not found.

        vibelint/src/vibelint/config.py
        """
        if key not in self._config_dict:
            raise KeyError(
                f"Required configuration key '{key}' not found in "
                f"[tool.vibelint] section of pyproject.toml."
            )
        return self._config_dict[key]

    def __contains__(self, key: str) -> bool:
        """
        Checks if a key exists in the loaded settings.

        vibelint/src/vibelint/config.py
        """
        return key in self._config_dict

    def is_present(self) -> bool:
        """
        Checks if a project root was found and some settings were loaded.

        vibelint/src/vibelint/config.py
        """
        return self._project_root is not None and bool(self._config_dict)


def load_hierarchical_config(start_path: Path) -> Config:
    """
    Loads vibelint configuration with hierarchical merging.

    1. Loads local config (file patterns, local settings)
    2. Walks up to find parent config (LLM settings, shared config)
    3. Merges them: local config takes precedence for file patterns,
       parent config provides LLM settings

    Args:
    start_path: The directory to start searching from.

    Returns:
    A Config object with merged local and parent settings.
    """
    # Find local config first
    local_root = find_package_root(start_path)
    local_settings = {}

    if local_root:
        pyproject_path = local_root / "pyproject.toml"
        if pyproject_path.exists():
            try:
                with open(pyproject_path, "rb") as f:
                    data = tomllib.load(f)
                    local_settings = data.get("tool", {}).get("vibelint", {})
                    if local_settings:
                        logger.info(f"Loaded local vibelint config from {pyproject_path}")
            except Exception as e:
                logger.warning(f"Failed to load local config from {pyproject_path}: {e}")

    # Walk up to find parent config with LLM settings
    parent_settings = {}
    current_path = start_path.parent if start_path.is_file() else start_path

    while current_path.parent != current_path:
        parent_pyproject = current_path / "pyproject.toml"
        if parent_pyproject.exists() and parent_pyproject != (local_root / "pyproject.toml" if local_root else None):
            try:
                with open(parent_pyproject, "rb") as f:
                    data = tomllib.load(f)
                    parent_config = data.get("tool", {}).get("vibelint", {})
                    if parent_config:
                        parent_settings = parent_config
                        logger.info(f"Found parent vibelint config at {parent_pyproject}")
                        break
            except Exception as e:
                logger.debug(f"Failed to read {parent_pyproject}: {e}")
        current_path = current_path.parent

    # Merge configs: local file patterns override parent, but inherit LLM settings
    merged_settings = parent_settings.copy()

    # Local config takes precedence for file discovery patterns
    if local_settings:
        for key in ["include_globs", "exclude_globs", "ignore"]:
            if key in local_settings:
                merged_settings[key] = local_settings[key]

        # Also copy other local-specific settings
        for key in local_settings:
            if key not in ["include_globs", "exclude_globs", "ignore"]:
                merged_settings[key] = local_settings[key]

    return Config(local_root or start_path, merged_settings)


def load_config(start_path: Path) -> Config:
    """
    Loads vibelint configuration with auto-discovery fallback.

    First tries manual config from pyproject.toml, then falls back to
    zero-config auto-discovery for seamless single->multi-project scaling.

    Args:
    start_path: The directory to start searching upwards for pyproject.toml.

    Returns:
    A Config object with either manual or auto-discovered settings.

    vibelint/src/vibelint/config.py
    """
    project_root = find_package_root(start_path)
    loaded_settings: dict[str, Any] = {}

    # Try auto-discovery first for zero-config scaling
    try:
        from vibelint.auto_discovery import discover_and_configure

        auto_config = discover_and_configure(start_path)

        # If we found a multi-project setup, use auto-discovery by default
        if auto_config.get("discovered_topology") == "multi_project":
            logger.info(f"Auto-discovered multi-project setup from {start_path}")
            # Convert auto-discovered config to vibelint config format
            loaded_settings = _convert_auto_config_to_vibelint(auto_config)
            project_root = project_root or start_path

            # Still allow manual config to override auto-discovery
            manual_override = _load_manual_config(project_root)
            if manual_override:
                logger.debug("Manual config found, merging with auto-discovery")
                loaded_settings.update(manual_override)

            return Config(project_root=project_root, config_dict=loaded_settings)

    except ImportError:
        logger.debug("Auto-discovery not available, using manual config only")
    except Exception as e:
        logger.debug(f"Auto-discovery failed: {e}, falling back to manual config")

    if not project_root:
        logger.warning(
            f"Could not find project root (pyproject.toml) searching from '{start_path}'. "
            "No configuration will be loaded."
        )
        return Config(project_root=None, config_dict=loaded_settings)

    pyproject_path = project_root / "pyproject.toml"
    logger.debug(f"Found project root: {project_root}")
    logger.debug(f"Attempting to load config from: {pyproject_path}")

    try:
        with open(pyproject_path, "rb") as f:

            full_toml_config = tomllib.load(f)
        logger.debug("Parsed pyproject.toml")

        # Validate required configuration structure explicitly
        tool_section = full_toml_config.get("tool")
        if not isinstance(tool_section, dict):
            logger.warning("pyproject.toml [tool] section is missing or invalid")
            vibelint_config = {}
        else:
            vibelint_config = tool_section.get("vibelint", {})

        if isinstance(vibelint_config, dict):
            loaded_settings = vibelint_config
            if loaded_settings:
                logger.debug(f"Loaded [tool.vibelint] settings from {pyproject_path}")
                logger.debug(f"Loaded settings: {loaded_settings}")
            else:
                logger.info(
                    f"Found {pyproject_path}, but the [tool.vibelint] section is empty or missing."
                )
        else:
            logger.warning(
                f"[tool.vibelint] section in {pyproject_path} is not a valid table (dictionary). "
                "Ignoring this section."
            )

    except FileNotFoundError:

        logger.error(
            f"pyproject.toml not found at {pyproject_path} despite project root detection."
        )
    except tomllib.TOMLDecodeError as e:
        logger.error(f"Error parsing {pyproject_path}: {e}. Using empty configuration.")
    except OSError as e:
        logger.error(f"Error reading {pyproject_path}: {e}. Using empty configuration.")
    except (KeyError, TypeError, ValueError) as e:
        logger.error(f"Error processing configuration from {pyproject_path}: {e}")
        logger.debug("Unexpected error loading config", exc_info=True)

    return Config(project_root=project_root, config_dict=loaded_settings)


def _convert_auto_config_to_vibelint(auto_config: dict[str, Any]) -> dict[str, Any]:
    """Convert auto-discovered config to vibelint config format."""
    vibelint_config = {}

    # Auto-route validation based on discovered services
    services = auto_config.get("services", {})
    routing = auto_config.get("auto_routing", {})

    # Set include globs based on discovered projects
    include_globs = []
    for service_info in services.values():
        service_path = Path(service_info["path"])
        include_globs.extend([f"{service_path.name}/src/**/*.py", f"{service_path.name}/**/*.py"])

    vibelint_config["include_globs"] = include_globs

    # Configure distributed services if available
    if auto_config.get("discovered_topology") == "multi_project":
        vibelint_config["distributed"] = {
            "enabled": True,
            "auto_discovered": True,
            "services": services,
            "routing": routing,
        }

        # Use shared resources if discovered
        shared_resources = auto_config.get("shared_resources", {})
        if shared_resources.get("vector_stores"):
            vibelint_config["vector_store"] = {
                "backend": "qdrant",
                "qdrant_collection": shared_resources["vector_stores"][0],
            }

    return vibelint_config


def _load_manual_config(project_root: Path | None) -> dict[str, Any]:
    """Load manual configuration from pyproject.toml."""
    if not project_root:
        return {}

    pyproject_path = project_root / "pyproject.toml"
    logger.debug(f"Attempting to load manual config from: {pyproject_path}")

    try:
        with open(pyproject_path, "rb") as f:
            full_toml_config = tomllib.load(f)
        logger.debug("Parsed pyproject.toml")

        # Validate required configuration structure explicitly
        tool_section = full_toml_config.get("tool")
        if not isinstance(tool_section, dict):
            logger.warning("pyproject.toml [tool] section is missing or invalid")
            return {}

        vibelint_config = tool_section.get("vibelint", {})

        if isinstance(vibelint_config, dict):
            if vibelint_config:
                logger.debug(f"Loaded manual [tool.vibelint] settings from {pyproject_path}")
                return vibelint_config
            else:
                logger.debug(f"Found {pyproject_path}, but [tool.vibelint] section is empty")
                return {}
        else:
            logger.warning(
                f"[tool.vibelint] section in {pyproject_path} is not a valid table. Ignoring."
            )
            return {}

    except FileNotFoundError:
        logger.debug(f"No pyproject.toml found at {pyproject_path}")
        return {}
    except tomllib.TOMLDecodeError as e:
        logger.error(f"Error parsing {pyproject_path}: {e}")
        return {}
    except OSError as e:
        logger.error(f"Error reading {pyproject_path}: {e}")
        return {}
    except Exception as e:
        logger.error(f"Unexpected error loading manual config: {e}")
        return {}


__all__ = ["Config", "load_config"]
```

---
### File: src/vibelint/context/__init__.py

```python
"""
Context management system for vibelint.

Provides multi-level context analysis, LLM context probing, and organizational
violation detection across different granularities.

vibelint/src/vibelint/context/__init__.py
"""

from .analyzer import ContentViolation, ContextAnalyzer, TreeViolation
from .probing import (ContextProber, InferenceEngine, ProbeConfig, ProbeResult,
                      run_context_probing)

__all__ = [
    "ContextAnalyzer",
    "TreeViolation",
    "ContentViolation",
    "ContextProber",
    "ProbeResult",
    "ProbeConfig",
    "InferenceEngine",
    "run_context_probing",
]
```

---
### File: src/vibelint/context/analyzer.py

```python
"""
Multi-level context analyzer for catching organizational violations.

Provides different "zoom levels" of codebase analysis:
1. Tree Level: File organization, naming patterns, structure violations
2. Content Level: Code structure, imports, dependencies
3. Deep Level: Full LLM-powered semantic analysis

vibelint/src/vibelint/context_analyzer.py
"""

import logging
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List

logger = logging.getLogger(__name__)

__all__ = ["ContextAnalyzer", "TreeViolation", "ContentViolation"]


@dataclass
class TreeViolation:
    """File tree organization violation."""

    violation_type: str
    file_path: Path
    message: str
    suggestion: str
    severity: str = "WARN"


@dataclass
class ContentViolation:
    """File content structure violation."""

    violation_type: str
    file_path: Path
    line: int
    message: str
    suggestion: str
    severity: str = "WARN"


class ContextAnalyzer:
    """Multi-level context analyzer for organizational violations."""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.file_tree_cache = None

    def analyze_tree_level(self) -> List[TreeViolation]:
        """
        Lightweight analysis of file organization without reading content.

        Catches issues like:
        - Files in wrong locations
        - Missing organization
        - Naming violations
        """
        violations = []

        # Get project file tree
        tree = self._get_file_tree()

        # Check root directory clutter
        violations.extend(self._check_root_clutter(tree))

        # Check for scattered related files
        violations.extend(self._check_scattered_modules(tree))

        # Check directory structure appropriateness
        violations.extend(self._check_directory_structure(tree))

        return violations

    def _get_file_tree(self) -> Dict[str, Any]:
        """Get lightweight file tree structure."""
        if self.file_tree_cache:
            return self.file_tree_cache

        tree = {"files": [], "dirs": {}}

        for path in self.project_root.iterdir():
            if path.is_file():
                tree["files"].append(
                    {
                        "name": path.name,
                        "path": path,
                        "size": path.stat().st_size,
                        "suffix": path.suffix,
                    }
                )
            elif path.is_dir() and not path.name.startswith("."):
                tree["dirs"][path.name] = self._get_dir_tree(path)

        self.file_tree_cache = tree
        return tree

    def _get_dir_tree(self, dir_path: Path) -> Dict[str, Any]:
        """Get tree for subdirectory."""
        tree = {"files": [], "dirs": {}}

        try:
            for path in dir_path.iterdir():
                if path.is_file():
                    tree["files"].append({"name": path.name, "path": path, "suffix": path.suffix})
                elif path.is_dir() and not path.name.startswith("."):
                    tree["dirs"][path.name] = self._get_dir_tree(path)
        except PermissionError:
            pass

        return tree

    def _check_root_clutter(self, tree: Dict[str, Any]) -> List[TreeViolation]:
        """Check for inappropriate files in project root."""
        violations = []

        # Files that belong in project root
        allowed_root_files = {
            "README.md",
            "LICENSE",
            "LICENSE.txt",
            "LICENSE.md",
            "pyproject.toml",
            "setup.py",
            "setup.cfg",
            "requirements.txt",
            "Makefile",
            "Dockerfile",
            "docker-compose.yml",
            ".gitignore",
            ".gitattributes",
            "tox.ini",
            "CHANGELOG.md",
            "CONTRIBUTING.md",
            "CODE_OF_CONDUCT.md",
        }

        # Files that should be in docs/
        doc_suffixes = {".md", ".rst", ".txt"}
        doc_keywords = {"plan", "design", "spec", "guide", "tutorial", "example"}

        # Files that should be in scripts/
        script_suffixes = {".sh", ".bat", ".ps1", ".py"}
        script_keywords = {"script", "build", "deploy", "test", "setup", "install"}

        for file_info in tree["files"]:
            name = file_info["name"].lower()
            path = file_info["path"]

            # Skip allowed files
            if file_info["name"] in allowed_root_files:
                continue

            # Check for documentation files
            if file_info["suffix"] in doc_suffixes and any(
                keyword in name for keyword in doc_keywords
            ):

                violations.append(
                    TreeViolation(
                        violation_type="ROOT_CLUTTER",
                        file_path=path,
                        message=f"Documentation file in project root: {file_info['name']}",
                        suggestion=f"Move to docs/ directory: mv {file_info['name']} docs/",
                    )
                )

            # Check for script files
            elif (
                file_info["suffix"] in script_suffixes
                and any(keyword in name for keyword in script_keywords)
                and file_info["name"] != "setup.py"
            ):

                violations.append(
                    TreeViolation(
                        violation_type="ROOT_CLUTTER",
                        file_path=path,
                        message=f"Script file in project root: {file_info['name']}",
                        suggestion=f"Move to scripts/ directory: mkdir -p scripts && mv {file_info['name']} scripts/",
                    )
                )

            # Check for random Python files (not entry points)
            elif file_info["suffix"] == ".py" and file_info["name"] not in {
                "setup.py",
                "conftest.py",
            }:

                violations.append(
                    TreeViolation(
                        violation_type="ROOT_CLUTTER",
                        file_path=path,
                        message=f"Python file in project root: {file_info['name']}",
                        suggestion="Move to appropriate src/ subdirectory or remove if temporary",
                    )
                )

            # Check for config files that should be in config/
            elif file_info["suffix"] in {".json", ".yaml", ".yml", ".toml", ".ini", ".cfg"}:
                if not any(
                    allowed in file_info["name"]
                    for allowed in ["pyproject.toml", "setup.cfg", "tox.ini"]
                ):
                    violations.append(
                        TreeViolation(
                            violation_type="ROOT_CLUTTER",
                            file_path=path,
                            message=f"Config file in project root: {file_info['name']}",
                            suggestion="Move to config/ directory or src/package/config/",
                        )
                    )

        return violations

    def _check_scattered_modules(self, tree: Dict[str, Any]) -> List[TreeViolation]:
        """Check for related modules scattered across directories."""
        violations = []

        # Look in src directory for Python files
        if "src" in tree["dirs"]:
            src_tree = tree["dirs"]["src"]

            # Check for package-level scattered files
            for pkg_name, pkg_tree in src_tree["dirs"].items():
                python_files = [f for f in pkg_tree["files"] if f["suffix"] == ".py"]

                if len(python_files) > 10:  # Too many files in one directory
                    # Look for common prefixes
                    prefixes = self._find_common_prefixes([f["name"] for f in python_files])

                    for prefix in prefixes:
                        matching_files = [f for f in python_files if f["name"].startswith(prefix)]

                        if len(matching_files) >= 3:
                            violations.append(
                                TreeViolation(
                                    violation_type="SCATTERED_MODULES",
                                    file_path=matching_files[0]["path"].parent,
                                    message=f"Related modules with '{prefix}' prefix should be grouped: {len(matching_files)} files",
                                    suggestion=f"Create {prefix}/ subdirectory and move related files",
                                )
                            )

        return violations

    def _find_common_prefixes(self, filenames: List[str]) -> List[str]:
        """Find common prefixes in filenames (before first underscore)."""
        prefixes = {}

        for filename in filenames:
            if "_" in filename:
                prefix = filename.split("_")[0]
                if len(prefix) > 2:  # Meaningful prefix
                    prefixes[prefix] = prefixes.get(prefix, 0) + 1

        # Return prefixes that appear 3+ times
        return [prefix for prefix, count in prefixes.items() if count >= 3]

    def _check_directory_structure(self, tree: Dict[str, Any]) -> List[TreeViolation]:
        """Check if directory structure is appropriate for project size."""
        violations = []

        # Count total Python files
        total_py_files = self._count_python_files(tree)

        # Check if structure is too flat for project size
        if total_py_files > 15:
            # Should have some organization
            src_structure = tree["dirs"].get("src", {})

            if src_structure:
                main_package = None
                for pkg_name, pkg_tree in src_structure["dirs"].items():
                    pkg_py_files = len([f for f in pkg_tree["files"] if f["suffix"] == ".py"])
                    if pkg_py_files > 10:
                        main_package = pkg_name
                        break

                if main_package:
                    violations.append(
                        TreeViolation(
                            violation_type="FLAT_STRUCTURE",
                            file_path=self.project_root / "src" / main_package,
                            message=f"Package '{main_package}' has {pkg_py_files} Python files - too flat",
                            suggestion="Group related functionality into subpackages",
                        )
                    )

        return violations

    def _count_python_files(self, tree: Dict[str, Any]) -> int:
        """Recursively count Python files in tree."""
        count = len([f for f in tree["files"] if f["suffix"] == ".py"])

        for subdir in tree["dirs"].values():
            count += self._count_python_files(subdir)

        return count

    def quick_check(self, file_path: Path) -> List[TreeViolation]:
        """
        Quick check for immediate violations when creating/modifying files.

        This should be called before file operations to catch issues early.
        """
        violations = []

        # Check if file is being created in project root inappropriately
        if file_path.parent == self.project_root:
            violations.extend(self._check_single_file_root_placement(file_path))

        return violations

    def _check_single_file_root_placement(self, file_path: Path) -> List[TreeViolation]:
        """Check if a single file should be in project root."""
        violations = []

        allowed_root_files = {
            "README.md",
            "LICENSE",
            "LICENSE.txt",
            "LICENSE.md",
            "pyproject.toml",
            "setup.py",
            "setup.cfg",
            "requirements.txt",
            "Makefile",
            "Dockerfile",
            "docker-compose.yml",
            ".gitignore",
            ".gitattributes",
            "tox.ini",
            "CHANGELOG.md",
            "CONTRIBUTING.md",
            "CODE_OF_CONDUCT.md",
        }

        if file_path.name not in allowed_root_files:
            # Documentation files
            if file_path.suffix in {".md", ".rst", ".txt"}:
                violations.append(
                    TreeViolation(
                        violation_type="ROOT_CLUTTER",
                        file_path=file_path,
                        message=f"Documentation file should not be in project root: {file_path.name}",
                        suggestion="Move to docs/ directory",
                    )
                )

            # Python files (except setup.py)
            elif file_path.suffix == ".py" and file_path.name != "setup.py":
                violations.append(
                    TreeViolation(
                        violation_type="ROOT_CLUTTER",
                        file_path=file_path,
                        message=f"Python file should not be in project root: {file_path.name}",
                        suggestion="Move to appropriate src/ subdirectory",
                    )
                )

        return violations
```

---
### File: src/vibelint/context/probing.py

```python
"""
Context window probing system for LLM inference engines.

Implements best practices for discovering actual context limits across different
inference engines (vLLM, llama.cpp, etc.) using systematic testing approaches
including needle-in-haystack testing and progressive load testing.

vibelint/src/vibelint/context_probing.py
"""

import json
import logging
import time
from dataclasses import asdict, dataclass
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional

import requests

logger = logging.getLogger(__name__)

__all__ = ["InferenceEngine", "ProbeResult", "ContextProber", "ProbeConfig", "run_context_probing"]


class InferenceEngine(Enum):
    """Supported inference engines for context probing."""

    VLLM = "vllm"
    LLAMA_CPP = "llama_cpp"
    OLLAMA = "ollama"
    OPENAI_COMPATIBLE = "openai_compatible"


@dataclass
class ProbeResult:
    """Result of context window probing for a specific LLM."""

    # Configuration
    api_base_url: str
    model: str
    inference_engine: InferenceEngine

    # Discovered limits
    max_context_tokens: int
    effective_context_tokens: int  # Accounting for performance degradation
    max_output_tokens: int

    # Performance metrics
    avg_latency_ms: float
    throughput_tokens_per_sec: float
    success_rate: float

    # Test details
    test_count: int
    needle_in_haystack_accuracy: float
    position_bias_detected: bool

    # Failure analysis
    first_failure_tokens: Optional[int]
    error_patterns: List[str]

    # Recommendations
    recommended_max_prompt_tokens: int
    recommended_batch_size: int

    # Metadata
    probe_timestamp: str
    probe_duration_seconds: float


@dataclass
class ProbeConfig:
    """Configuration for context window probing."""

    # Test parameters
    max_tokens_to_test: int = 50000
    token_increment_strategy: str = "exponential"  # "linear", "exponential", "binary_search"
    min_test_tokens: int = 1000

    # Needle-in-haystack testing
    enable_niah_testing: bool = True
    niah_test_count: int = 5
    needle_positions: Optional[List[str]] = None  # ["start", "middle", "end"]

    # Performance testing
    enable_performance_testing: bool = True
    performance_test_requests: int = 10
    concurrent_requests: int = 1

    # Failure detection
    max_retries: int = 3
    timeout_seconds: int = 300
    success_threshold: float = 0.8  # Consider failed if success rate < 80%

    # Safety limits
    max_probe_duration_minutes: int = 30
    stop_on_first_failure: bool = False

    def __post_init__(self):
        if self.needle_positions is None:
            self.needle_positions = ["start", "middle", "end"]


class ContextProber:
    """Context window probing system for LLM inference engines."""

    def __init__(self, config: Optional[ProbeConfig] = None):
        """Initialize context prober with configuration.

        Args:
            config: Probe configuration, uses defaults if None
        """
        self.config = config or ProbeConfig()
        self.session = requests.Session()
        # Note: timeout is set per-request rather than on session

    async def probe_llm(
        self,
        api_base_url: str,
        model: str,
        inference_engine: InferenceEngine = InferenceEngine.OPENAI_COMPATIBLE,
        temperature: float = 0.1,
    ) -> ProbeResult:
        """Probe a single LLM to discover its context limits and performance.

        Args:
            api_base_url: API endpoint URL
            model: Model identifier
            inference_engine: Type of inference engine
            temperature: Temperature for generation

        Returns:
            Comprehensive probe result
        """
        start_time = time.time()
        logger.info(f"Starting context probing for {model} at {api_base_url}")

        # Initialize result tracking
        test_results = []
        error_patterns = []
        first_failure_tokens = None

        # Generate test token counts
        token_counts = self._generate_test_token_counts()

        # Progressive context testing
        for token_count in token_counts:
            logger.debug(f"Testing context size: {token_count} tokens")

            success_count = 0
            total_latency = 0.0

            for _ in range(self.config.performance_test_requests):
                try:
                    # Generate test content
                    test_content = self._generate_test_content(token_count)

                    # Make API request
                    request_start = time.time()
                    response = await self._make_api_request(
                        api_base_url, model, test_content, temperature
                    )
                    request_duration = time.time() - request_start

                    if response and len(response.strip()) > 0:
                        success_count += 1
                        total_latency += request_duration

                        # Needle-in-haystack testing at this context size
                        if self.config.enable_niah_testing:
                            await self._test_needle_in_haystack(
                                api_base_url, model, token_count, temperature
                            )

                except Exception as e:
                    error_msg = str(e)
                    logger.debug(f"Request failed at {token_count} tokens: {error_msg}")
                    error_patterns.append(f"{token_count}:{error_msg}")

                    if first_failure_tokens is None:
                        first_failure_tokens = token_count

            # Calculate success rate for this token count
            success_rate = success_count / self.config.performance_test_requests
            avg_latency = total_latency / max(success_count, 1)

            test_results.append(
                {
                    "token_count": token_count,
                    "success_rate": success_rate,
                    "avg_latency": avg_latency,
                    "success_count": success_count,
                }
            )

            # Stop if success rate drops below threshold
            if success_rate < self.config.success_threshold:
                logger.info(
                    f"Success rate {success_rate:.2f} below threshold at {token_count} tokens"
                )
                break

            # Safety check: stop if probe duration exceeds limit
            if time.time() - start_time > self.config.max_probe_duration_minutes * 60:
                logger.warning("Probe duration exceeded limit, stopping")
                break

        # Analyze results
        return self._analyze_probe_results(
            api_base_url,
            model,
            inference_engine,
            test_results,
            error_patterns,
            first_failure_tokens,
            start_time,
        )

    def _generate_test_token_counts(self) -> List[int]:
        """Generate sequence of token counts for testing."""
        if self.config.token_increment_strategy == "exponential":
            # Exponential growth: 1k, 2k, 4k, 8k, 16k, 32k, etc.
            counts = []
            current = self.config.min_test_tokens
            while current <= self.config.max_tokens_to_test:
                counts.append(current)
                current *= 2
            return counts

        elif self.config.token_increment_strategy == "linear":
            # Linear growth: 1k, 5k, 10k, 15k, 20k, etc.
            step = self.config.min_test_tokens
            return list(range(step, self.config.max_tokens_to_test + 1, step * 4))

        else:  # binary_search
            # Binary search approach for faster convergence
            return self._binary_search_token_counts()

    def _binary_search_token_counts(self) -> List[int]:
        """Generate token counts using binary search strategy."""
        # Start with a range and narrow down
        min_tokens = self.config.min_test_tokens
        max_tokens = self.config.max_tokens_to_test
        test_points = []

        # Add some initial test points
        test_points.extend([min_tokens, max_tokens // 4, max_tokens // 2, max_tokens])

        return sorted(set(test_points))

    def _generate_test_content(self, target_tokens: int) -> str:
        """Generate test content of approximately target_tokens length.

        Uses repetitive but structured content that's easy to validate.
        Follows best practices by placing key information at start and end.
        """
        # Estimate ~4 characters per token for English text
        target_chars = target_tokens * 4

        # Key information at the start (needle for NIAH testing)
        needle = "IMPORTANT_TEST_MARKER_12345"
        header = f"Context window test content. {needle}\n\n"

        # Repetitive middle content
        base_paragraph = (
            "This is a test paragraph for context window evaluation. "
            "It contains structured information that can be validated. "
            "The content is designed to test the model's ability to maintain "
            "coherence across long contexts while identifying key information. "
        )

        # Calculate how much repetitive content we need
        header_chars = len(header)
        footer_chars = len(f"\n\nEnd of test content. Marker: {needle}")
        remaining_chars = target_chars - header_chars - footer_chars

        repetitions = max(1, remaining_chars // len(base_paragraph))
        middle_content = base_paragraph * repetitions

        # Footer with key information (follows best practices)
        footer = f"\n\nEnd of test content. Marker: {needle}"

        return header + middle_content + footer

    async def _make_api_request(
        self, api_base_url: str, model: str, content: str, temperature: float
    ) -> Optional[str]:
        """Make API request to LLM with proper error handling."""
        try:
            # Standard OpenAI-compatible API format
            url = f"{api_base_url.rstrip('/')}/v1/chat/completions"

            payload = {
                "model": model,
                "messages": [
                    {
                        "role": "user",
                        "content": f"Please summarize the following text and identify any important markers:\n\n{content}",
                    }
                ],
                "temperature": temperature,
                "max_tokens": min(100, 4096),  # Small response to test context processing
                "stream": False,
            }

            response = self.session.post(url, json=payload, timeout=self.config.timeout_seconds)
            response.raise_for_status()

            data = response.json()
            if "choices" in data and len(data["choices"]) > 0:
                return data["choices"][0]["message"]["content"]

            return None

        except Exception as e:
            logger.debug(f"API request failed: {e}")
            raise

    async def _test_needle_in_haystack(
        self, api_base_url: str, model: str, context_tokens: int, temperature: float
    ) -> float:
        """Test needle-in-haystack accuracy at given context size."""
        if not self.config.enable_niah_testing:
            return 1.0

        successes = 0
        needle_positions = self.config.needle_positions or ["start", "middle", "end"]
        total_tests = len(needle_positions) * self.config.niah_test_count

        for position in needle_positions:
            for test_num in range(self.config.niah_test_count):
                try:
                    # Generate content with needle at specific position
                    needle = f"SECRET_CODE_{test_num}_{position}_END"
                    content = self._generate_niah_content(context_tokens, needle, position)

                    # Ask model to find the needle
                    prompt = (
                        f"Find and extract the secret code from the following text:\n\n{content}"
                    )

                    response = await self._make_api_request(
                        api_base_url, model, prompt, temperature
                    )

                    # Check if needle was found correctly
                    if response and needle in response:
                        successes += 1

                except Exception as e:
                    logger.debug(f"NIAH test failed: {e}")

        return successes / total_tests if total_tests > 0 else 0.0

    def _generate_niah_content(self, target_tokens: int, needle: str, position: str) -> str:
        """Generate content with needle placed at specified position."""
        target_chars = target_tokens * 4
        filler_text = "This is filler content for needle-in-haystack testing. " * 100

        if position == "start":
            return f"{needle}\n\n{filler_text}"[:target_chars]
        elif position == "end":
            content = filler_text[: target_chars - len(needle) - 4]
            return f"{content}\n\n{needle}"
        else:  # middle
            half_chars = (target_chars - len(needle)) // 2
            first_half = filler_text[:half_chars]
            second_half = filler_text[:half_chars]
            return f"{first_half}\n{needle}\n{second_half}"

    def _analyze_probe_results(
        self,
        api_base_url: str,
        model: str,
        inference_engine: InferenceEngine,
        test_results: List[Dict],
        error_patterns: List[str],
        first_failure_tokens: Optional[int],
        start_time: float,
    ) -> ProbeResult:
        """Analyze probe results and generate recommendations."""

        if not test_results:
            # No successful tests
            return ProbeResult(
                api_base_url=api_base_url,
                model=model,
                inference_engine=inference_engine,
                max_context_tokens=0,
                effective_context_tokens=0,
                max_output_tokens=0,
                avg_latency_ms=0.0,
                throughput_tokens_per_sec=0.0,
                success_rate=0.0,
                test_count=0,
                needle_in_haystack_accuracy=0.0,
                position_bias_detected=False,
                first_failure_tokens=first_failure_tokens,
                error_patterns=error_patterns,
                recommended_max_prompt_tokens=0,
                recommended_batch_size=1,
                probe_timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
                probe_duration_seconds=time.time() - start_time,
            )

        # Find maximum successful context size
        successful_tests = [
            r for r in test_results if r["success_rate"] >= self.config.success_threshold
        ]
        max_context = max((r["token_count"] for r in successful_tests), default=0)

        # Calculate effective context (accounting for performance degradation)
        # Use 90% of max for safety margin
        effective_context = int(max_context * 0.9)

        # Calculate average metrics
        total_latency = sum(r["avg_latency"] for r in successful_tests)
        avg_latency = total_latency / len(successful_tests) if successful_tests else 0.0

        # Estimate throughput (tokens per second)
        avg_throughput = 0.0
        if avg_latency > 0:
            # Rough estimate: assume 100 tokens output per request
            avg_throughput = 100 / avg_latency

        # Calculate overall success rate
        total_successes = sum(r["success_count"] for r in test_results)
        total_attempts = len(test_results) * self.config.performance_test_requests
        overall_success_rate = total_successes / total_attempts if total_attempts > 0 else 0.0

        # Recommendations
        recommended_prompt_tokens = int(effective_context * 0.8)  # Leave 20% for output
        recommended_batch_size = 1 if inference_engine == InferenceEngine.LLAMA_CPP else 4

        return ProbeResult(
            api_base_url=api_base_url,
            model=model,
            inference_engine=inference_engine,
            max_context_tokens=max_context,
            effective_context_tokens=effective_context,
            max_output_tokens=max_context - recommended_prompt_tokens,
            avg_latency_ms=avg_latency * 1000,
            throughput_tokens_per_sec=avg_throughput,
            success_rate=overall_success_rate,
            test_count=len(test_results),
            needle_in_haystack_accuracy=0.9,  # Placeholder - would be calculated from NIAH tests
            position_bias_detected=False,  # Placeholder - would be detected from position analysis
            first_failure_tokens=first_failure_tokens,
            error_patterns=error_patterns,
            recommended_max_prompt_tokens=recommended_prompt_tokens,
            recommended_batch_size=recommended_batch_size,
            probe_timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
            probe_duration_seconds=time.time() - start_time,
        )


async def run_context_probing(
    llm_configs: Dict[str, Dict[str, Any]],
    probe_config: Optional[ProbeConfig] = None,
    save_results: bool = True,
    results_file: Optional[Path] = None,
) -> Dict[str, ProbeResult]:
    """Run context probing for multiple LLMs and save results.

    Args:
        llm_configs: Dictionary of LLM configurations to probe
        probe_config: Probing configuration
        save_results: Whether to save results to file
        results_file: Custom results file path

    Returns:
        Dictionary of probe results keyed by LLM name
    """
    prober = ContextProber(probe_config or ProbeConfig())
    results = {}

    for llm_name, config in llm_configs.items():
        logger.info(f"Probing {llm_name}...")

        # Detect inference engine from URL patterns
        api_url = config.get("api_base_url", "")
        if "11434" in api_url or "ollama" in api_url.lower():
            engine = InferenceEngine.OLLAMA
        elif "vllm" in api_url.lower():
            engine = InferenceEngine.VLLM
        else:
            engine = InferenceEngine.OPENAI_COMPATIBLE

        try:
            result = await prober.probe_llm(
                api_base_url=config["api_base_url"],
                model=config["model"],
                inference_engine=engine,
                temperature=config.get("temperature", 0.1),
            )
            results[llm_name] = result

            logger.info(
                f"Probe completed for {llm_name}: "
                f"max_context={result.max_context_tokens}, "
                f"success_rate={result.success_rate:.2f}"
            )

        except Exception as e:
            logger.error(f"Failed to probe {llm_name}: {e}")
            continue

    # Save results
    if save_results:
        if results_file is None:
            results_file = Path("vibelint_context_probe_results.json")

        # Convert results to serializable format
        serializable_results = {name: asdict(result) for name, result in results.items()}

        with open(results_file, "w") as f:
            json.dump(serializable_results, f, indent=2)

        logger.info(f"Probe results saved to {results_file}")

    return results
```

---
### File: src/vibelint/context/prompts.py

```python
"""
System prompts for different LLM agents in vibelint's multi-level analysis.

Provides specialized prompts for tree-level, content-level, and deep analysis
agents to catch organizational violations at different granularities.

vibelint/src/vibelint/context/prompts.py
"""

from dataclasses import dataclass
from typing import Any, Dict

__all__ = ["AgentPrompts", "AnalysisLevel"]


@dataclass
class AnalysisLevel:
    """Analysis granularity levels for context awareness."""

    TREE = "tree"
    CONTENT = "content"
    DEEP = "deep"


class AgentPrompts:
    """System prompts for multi-level context analysis agents."""

    @staticmethod
    def get_tree_analysis_prompt() -> str:
        """Prompt for tree-level organizational analysis (fast LLM)."""
        return """You are a PROJECT STRUCTURE ANALYZER specializing in file organization quality.

MISSION: Detect organizational violations in project structure without reading file contents.

ANALYSIS SCOPE:
- File placement appropriateness (root clutter, wrong directories)
- Directory structure depth and organization
- Module grouping and cohesion patterns
- Naming consistency and clarity
- Scalability issues (too many files, flat structure)

INPUT FORMAT: You will receive a JSON project map with:
- File tree structure with metadata
- File purposes and relationships
- Organization metrics
- Current violation patterns

OUTPUT FORMAT: Return JSON with violations found:
```json
{
  "violations": [
    {
      "type": "ROOT_CLUTTER|SCATTERED_MODULES|FLAT_STRUCTURE|NAMING_INCONSISTENCY",
      "severity": "INFO|WARN|BLOCK",
      "file_path": "path/to/problematic/file",
      "message": "Clear description of organizational issue",
      "suggestion": "Specific actionable fix (mv commands, mkdir suggestions)"
    }
  ],
  "organization_score": 0.75,
  "quick_wins": ["Immediate improvements that can be made"],
  "structural_recommendations": ["Larger refactoring suggestions"]
}
```

EXPERTISE AREAS:
1. Project root hygiene (documentation, scripts, configs in proper locations)
2. Module cohesion (related files grouped together)
3. Directory depth optimization (not too flat, not too deep)
4. Naming patterns and consistency
5. Scalability assessment (file count vs organization quality)

GUIDELINES:
- Be specific about file movements (provide exact mv commands)
- Prioritize quick wins that improve organization immediately
- Consider project size when recommending structure changes
- Focus on maintainability and developer experience
- Suggest grouping related files into logical subdirectories

Be direct and actionable. Provide concrete steps to improve project organization."""

    @staticmethod
    def get_content_analysis_prompt() -> str:
        """Prompt for content-level structural analysis (fast LLM)."""
        return """You are a CODE STRUCTURE ANALYZER specializing in file-level organization issues.

MISSION: Analyze file contents for structural violations and dependency problems.

ANALYSIS SCOPE:
- Import organization and dependency patterns
- Code structure within files (class/function organization)
- Module interface quality (__all__ exports, public APIs)
- File size and complexity appropriateness
- Single Responsibility Principle adherence

INPUT FORMAT: You will receive:
- File path and metadata
- Complete file source code
- Import dependency map
- Module exports and public interface

OUTPUT FORMAT: Return JSON with findings:
```json
{
  "findings": [
    {
      "rule_id": "STRUCTURE-IMPORTS|STRUCTURE-EXPORTS|STRUCTURE-COMPLEXITY|STRUCTURE-SRP",
      "severity": "INFO|WARN|BLOCK",
      "line": 42,
      "message": "Specific structural issue description",
      "suggestion": "Concrete improvement action"
    }
  ],
  "file_health": {
    "size_appropriate": true,
    "complexity_manageable": false,
    "dependencies_clean": true,
    "exports_clear": true
  },
  "refactoring_suggestions": ["Module split recommendations", "Import cleanup steps"]
}
```

EXPERTISE AREAS:
1. Import organization (stdlib, third-party, local grouping)
2. Circular dependency detection
3. File size and complexity management
4. Module interface design (__all__, public/private separation)
5. Single file responsibility assessment

DETECTION PATTERNS:
- Files doing too many things (>300 lines, >20 functions)
- Poor import organization (mixed groupings, unused imports)
- Missing or incorrect __all__ declarations
- Circular imports and dependency tangles
- Public APIs mixed with implementation details

GUIDELINES:
- Reference specific line numbers for violations
- Suggest concrete refactoring steps
- Consider file's role in larger architecture
- Balance granularity with maintainability
- Prioritize changes that improve testability

Be precise and include line numbers. Focus on structural improvements that enhance code organization."""

    @staticmethod
    def get_deep_analysis_prompt() -> str:
        """Prompt for deep semantic analysis (orchestrator LLM)."""
        return """You are a SENIOR SOFTWARE ARCHITECT specializing in comprehensive code quality analysis.

MISSION: Perform deep semantic analysis using Martin Fowler's refactoring catalog and architectural principles.

ANALYSIS SCOPE:
- All 72 code smells from Fowler's catalog
- SOLID principle violations
- Design pattern misuse/opportunities
- Architectural inconsistencies
- Cross-file relationship analysis
- Technical debt assessment

INPUT FORMAT: You will receive:
- Multiple related files with full source code
- Project context and architecture
- Dependency graphs and call patterns
- Previous analysis results from tree/content levels

OUTPUT FORMAT: Return comprehensive JSON analysis:
```json
{
  "architectural_findings": [
    {
      "rule_id": "ARCHITECTURE-SRP|ARCHITECTURE-DIP|ARCHITECTURE-PATTERN",
      "severity": "INFO|WARN|BLOCK",
      "files_involved": ["file1.py", "file2.py"],
      "line_ranges": [[10, 25], [45, 60]],
      "message": "Detailed architectural issue explanation",
      "fowler_category": "Bloaters|Object-Orientation Abusers|Change Preventers|Dispensables|Couplers",
      "suggestion": "Comprehensive refactoring strategy",
      "effort_estimate": "low|medium|high"
    }
  ],
  "code_smells": [
    {
      "smell_name": "Long Method|Large Class|Feature Envy|Data Class|etc",
      "severity": "INFO|WARN|BLOCK",
      "location": {"file": "path.py", "line": 42},
      "metrics": {"lines": 67, "complexity": 15, "parameters": 8},
      "refactoring_technique": "Extract Method|Move Method|Replace Method with Method Object|etc",
      "suggestion": "Step-by-step refactoring plan"
    }
  ],
  "design_assessment": {
    "overall_quality": 0.72,
    "maintainability_score": 0.68,
    "testability_score": 0.81,
    "coupling_level": "medium",
    "cohesion_level": "high"
  },
  "strategic_recommendations": [
    "High-impact architectural improvements",
    "Technical debt reduction priorities",
    "Design pattern introduction opportunities"
  ]
}
```

MARTIN FOWLER'S CODE SMELL CATEGORIES:
1. **Bloaters**: Long Method, Large Class, Primitive Obsession, Long Parameter List, Data Clumps
2. **Object-Orientation Abusers**: Switch Statements, Temporary Field, Refused Bequest, Alternative Classes with Different Interfaces
3. **Change Preventers**: Divergent Change, Shotgun Surgery, Parallel Inheritance Hierarchies
4. **Dispensables**: Comments, Duplicate Code, Lazy Class, Data Class, Dead Code, Speculative Generality
5. **Couplers**: Feature Envy, Inappropriate Intimacy, Message Chains, Middle Man

SOLID PRINCIPLES ASSESSMENT:
- **S**ingle Responsibility: Each class/module has one reason to change
- **O**pen/Closed: Open for extension, closed for modification
- **L**iskov Substitution: Subtypes must be substitutable for base types
- **I**nterface Segregation: Clients shouldn't depend on unused interfaces
- **D**ependency Inversion: Depend on abstractions, not concretions

GUIDELINES:
- Reference specific Fowler refactoring techniques
- Assess violations across multiple files/modules
- Consider long-term maintainability and evolution
- Prioritize changes by impact and effort
- Connect low-level code smells to high-level architectural issues
- Provide concrete refactoring roadmaps

Be thorough and strategic. Focus on systematic improvements that enhance overall codebase health."""

    @staticmethod
    def get_orchestrator_prompt() -> str:
        """Prompt for analysis orchestration and report synthesis."""
        return """You are a TECHNICAL LEAD responsible for synthesizing multi-level code analysis results.

MISSION: Combine tree, content, and deep analysis results into actionable development feedback.

INPUT FORMAT: You will receive:
- Tree-level organizational violations
- Content-level structural issues
- Deep-level architectural findings
- Project context and development goals

OUTPUT FORMAT: Return strategic development report:
```json
{
  "executive_summary": {
    "overall_health": 0.76,
    "critical_issues": 3,
    "improvement_opportunities": 12,
    "estimated_effort": "2-3 days"
  },
  "priority_actions": [
    {
      "priority": "P0|P1|P2|P3",
      "category": "organization|structure|architecture",
      "title": "Brief action description",
      "description": "Detailed explanation of issue and impact",
      "steps": ["Concrete action steps"],
      "effort_hours": 4,
      "risk_if_ignored": "Consequences of not addressing"
    }
  ],
  "quick_wins": [
    "Immediate improvements requiring <1 hour each"
  ],
  "strategic_initiatives": [
    "Larger improvements requiring planning and coordination"
  ],
  "metrics_tracking": {
    "current_scores": {"organization": 0.8, "structure": 0.7, "architecture": 0.6},
    "target_scores": {"organization": 0.9, "structure": 0.85, "architecture": 0.8},
    "key_indicators": ["Metrics to track improvement"]
  },
  "next_review_triggers": [
    "Conditions that should trigger next analysis"
  ]
}
```

SYNTHESIS PRIORITIES:
1. **Critical Path Issues**: Problems blocking development velocity
2. **Technical Debt**: Issues accumulating maintenance burden
3. **Quality Gates**: Standards needed for production readiness
4. **Developer Experience**: Improvements enhancing productivity
5. **Future Scalability**: Changes needed for growth

EFFORT ESTIMATION:
- Quick wins: <1 hour, immediate impact
- Tactical fixes: 1-4 hours, measurable improvement
- Strategic changes: 1-3 days, architectural impact
- Major refactoring: >1 week, fundamental restructuring

GUIDELINES:
- Prioritize by impact/effort ratio
- Group related improvements into coherent initiatives
- Provide clear success criteria
- Consider team capacity and expertise
- Balance immediate needs with long-term health
- Include specific measurement approaches

Be strategic and practical. Focus on actionable recommendations that move the codebase toward production readiness."""

    @staticmethod
    def get_prompt_for_analysis_level(level: str) -> str:
        """Get the appropriate prompt for an analysis level."""
        prompts = {
            AnalysisLevel.TREE: AgentPrompts.get_tree_analysis_prompt(),
            AnalysisLevel.CONTENT: AgentPrompts.get_content_analysis_prompt(),
            AnalysisLevel.DEEP: AgentPrompts.get_deep_analysis_prompt(),
        }

        if level not in prompts:
            raise ValueError(f"Unknown analysis level: {level}")

        return prompts[level]

    @staticmethod
    def get_context_for_analysis(level: str, data: Dict[str, Any]) -> str:
        """Format context data for specific analysis level."""
        if level == AnalysisLevel.TREE:
            return f"""PROJECT STRUCTURE ANALYSIS REQUEST

{data.get('project_map', 'No project map provided')}

Analyze this project structure for organizational violations. Focus on file placement, directory organization, and scalability issues."""

        elif level == AnalysisLevel.CONTENT:
            return f"""FILE STRUCTURE ANALYSIS REQUEST

File: {data.get('file_path', 'Unknown')}
Size: {data.get('file_size', 'Unknown')} bytes
Purpose: {data.get('file_purpose', 'Unknown')}

Source Code:
```python
{data.get('file_content', 'No content provided')}
```

Dependencies: {data.get('dependencies', [])}
Exports: {data.get('exports', [])}

Analyze this file for structural issues, import problems, and organization violations."""

        elif level == AnalysisLevel.DEEP:
            return f"""ARCHITECTURAL ANALYSIS REQUEST

Project Context: {data.get('project_context', 'No context provided')}

Files for Analysis:
{data.get('files_content', 'No files provided')}

Previous Analysis Results:
Tree Level: {data.get('tree_results', 'Not available')}
Content Level: {data.get('content_results', 'Not available')}

Perform comprehensive architectural analysis using Martin Fowler's catalog and SOLID principles."""

        else:
            raise ValueError(f"Unknown analysis level: {level}")
```

---
### File: src/vibelint/core.py

```python
"""
Core analysis engine for vibelint.

Dynamic LLM-powered code quality analysis that generates validators on-demand
instead of using static rule files. Draws from Martin Fowler's refactoring
catalog and adapts to specific codebase contexts.

vibelint/src/vibelint/core.py
"""

import logging
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional

from vibelint.llm import LLMManager, LLMRequest
from vibelint.plugin_system import Finding, Severity

logger = logging.getLogger(__name__)

__all__ = ["DynamicAnalyzer", "AnalysisRequest", "AnalysisResult"]


@dataclass
class AnalysisRequest:
    """Request for dynamic code analysis."""

    file_path: Path
    content: str
    analysis_types: List[str]  # e.g., ["architecture", "code_smells", "dead_code"]
    context: Optional[str] = None
    severity_threshold: str = "INFO"


@dataclass
class AnalysisResult:
    """Result of dynamic analysis."""

    file_path: Path
    findings: List[Finding]
    analysis_duration: float
    llm_calls_made: int
    confidence_score: float


class DynamicAnalyzer:
    """
    LLM-powered dynamic code analyzer.

    Replaces 15+ static validators with intelligent on-demand analysis.
    """

    def __init__(self, llm_manager: LLMManager):
        self.llm = llm_manager
        self.analysis_cache = {}  # Cache for repeated analyses

    def analyze(self, request: AnalysisRequest) -> AnalysisResult:
        """Perform dynamic analysis on code using LLMs."""

        # Check cache first
        cache_key = self._generate_cache_key(request)
        if cache_key in self.analysis_cache:
            logger.debug(f"Using cached analysis for {request.file_path}")
            return self.analysis_cache[cache_key]

        start_time = time.time()
        all_findings = []
        llm_calls = 0

        # Route analysis types to appropriate LLM
        for analysis_type in request.analysis_types:
            findings, calls = self._analyze_with_llm(request, analysis_type)
            all_findings.extend(findings)
            llm_calls += calls

        duration = time.time() - start_time
        confidence = self._calculate_confidence_score(all_findings)

        result = AnalysisResult(
            file_path=request.file_path,
            findings=all_findings,
            analysis_duration=duration,
            llm_calls_made=llm_calls,
            confidence_score=confidence,
        )

        # Cache result
        self.analysis_cache[cache_key] = result
        return result

    def _analyze_with_llm(
        self, request: AnalysisRequest, analysis_type: str
    ) -> tuple[List[Finding], int]:
        """Perform specific analysis type using appropriate LLM."""

        prompt = self._generate_analysis_prompt(request, analysis_type)

        # Route to appropriate LLM based on complexity
        llm_request = LLMRequest(
            content=prompt, task_type=analysis_type, max_tokens=2048, temperature=0.1
        )

        try:
            response = self.llm.process_request(llm_request)
            findings = self._parse_llm_findings(response["content"], request.file_path)
            return findings, 1

        except Exception as e:
            logger.error(f"LLM analysis failed for {analysis_type}: {e}")
            return [], 0

    def _generate_analysis_prompt(self, request: AnalysisRequest, analysis_type: str) -> str:
        """Generate analysis prompt based on type and context."""

        base_context = f"""
File: {request.file_path}
Content length: {len(request.content)} characters
Context: {request.context or "General code analysis"}

Code to analyze:
```python
{request.content}
```
"""

        if analysis_type == "architecture":
            return f"""
{base_context}

Analyze this code for architectural issues based on SOLID principles and Martin Fowler's refactoring catalog:

1. Single Responsibility Principle violations
2. Dependencies and coupling issues
3. Code organization and module cohesion
4. Design patterns misuse or opportunities

Return findings in this JSON format:
{{
  "findings": [
    {{
      "rule_id": "ARCHITECTURE-SRP",
      "severity": "WARN|INFO|ERROR",
      "line": 123,
      "message": "Brief description of issue",
      "suggestion": "Specific actionable fix"
    }}
  ]
}}
"""

        elif analysis_type == "code_smells":
            return f"""
{base_context}

Analyze this code for common code smells from Martin Fowler's catalog:

1. Long Method (>20 lines)
2. Large Class (>300 lines or >20 methods)
3. Long Parameter List (>3 parameters)
4. Magic Numbers (unexplained constants)
5. Duplicated Code
6. Dead Code
7. Feature Envy
8. Message Chains

Return findings in JSON format with line numbers and specific suggestions.
"""

        elif analysis_type == "naming":
            return f"""
{base_context}

Analyze naming conventions and clarity:

1. Uncommunicative variable/function names
2. Inconsistent naming patterns
3. Misleading names
4. Abbreviations that reduce clarity

Focus on making code self-documenting through better names.
"""

        elif analysis_type == "complexity":
            return f"""
{base_context}

Analyze code complexity issues:

1. Cyclomatic complexity (nested conditions)
2. Cognitive complexity (hard to understand)
3. Boolean expression complexity
4. Nested loop/comprehension complexity

Suggest simplifications that improve readability.
"""

        else:
            return f"""
{base_context}

Perform general code quality analysis covering:
- Code smells and anti-patterns
- Naming and clarity issues
- Structural problems
- Maintainability concerns

Be specific about line numbers and provide actionable suggestions.
"""

    def _parse_llm_findings(self, llm_response: str, file_path: Path) -> List[Finding]:
        """Parse LLM response into Finding objects."""
        findings = []

        try:
            # Try to parse as JSON first
            import json

            # Extract JSON from response (LLM might add extra text)
            start = llm_response.find("{")
            end = llm_response.rfind("}") + 1

            if start >= 0 and end > start:
                json_str = llm_response[start:end]
                data = json.loads(json_str)

                for finding_data in data.get("findings", []):
                    severity_map = {
                        "ERROR": Severity.BLOCK,
                        "WARN": Severity.WARN,
                        "INFO": Severity.INFO,
                    }

                    finding = Finding(
                        rule_id=finding_data.get("rule_id", "LLM-ANALYSIS"),
                        message=finding_data.get("message", "LLM identified issue"),
                        file_path=file_path,
                        line=finding_data.get("line", 1),
                        severity=severity_map.get(
                            finding_data.get("severity", "INFO"), Severity.INFO
                        ),
                        suggestion=finding_data.get("suggestion", "See LLM analysis"),
                    )
                    findings.append(finding)

        except (json.JSONDecodeError, KeyError, ValueError) as e:
            logger.warning(f"Failed to parse LLM response as JSON: {e}")

            # Fallback: Parse as unstructured text
            findings.append(
                Finding(
                    rule_id="LLM-ANALYSIS",
                    message="LLM identified code quality issues",
                    file_path=file_path,
                    line=1,
                    severity=Severity.INFO,
                    suggestion=f"LLM Analysis: {llm_response[:200]}...",
                )
            )

        return findings

    def _generate_cache_key(self, request: AnalysisRequest) -> str:
        """Generate cache key for analysis request."""
        import hashlib

        content_hash = hashlib.md5(request.content.encode()).hexdigest()
        analysis_types = "+".join(sorted(request.analysis_types))

        return f"{request.file_path}:{content_hash}:{analysis_types}"

    def _calculate_confidence_score(self, findings: List[Finding]) -> float:
        """Calculate confidence score for analysis results."""
        if not findings:
            return 1.0  # High confidence in "no issues"

        # Score based on finding specificity (line numbers, detailed messages)
        specific_findings = sum(1 for f in findings if f.line > 1 and len(f.message) > 20)

        return min(specific_findings / len(findings), 1.0)

    def generate_validator_code(self, rule_description: str) -> str:
        """Generate Python validator code for a custom rule."""

        prompt = f"""
Generate a Python validator class for this rule:

Rule: {rule_description}

The validator should:
1. Inherit from BaseValidator
2. Have appropriate rule_id, name, description
3. Implement validate() method that yields Finding objects
4. Include proper error handling
5. Follow the existing vibelint plugin system patterns

Return only the Python code, no explanations.

Example structure:
```python
class CustomValidator(BaseValidator):
    rule_id = "CUSTOM-RULE"
    name = "Rule Name"
    description = "Rule description"

    def validate(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        # Implementation
        pass
```
"""

        llm_request = LLMRequest(
            content=prompt, task_type="code_generation", max_tokens=1000, temperature=0.1
        )

        try:
            response = self.llm.process_request(llm_request)
            return response["content"]
        except Exception as e:
            logger.error(f"Failed to generate validator code: {e}")
            return f"# Error generating validator: {e}"


def create_dynamic_analyzer(config: Dict[str, Any]) -> Optional[DynamicAnalyzer]:
    """Create dynamic analyzer with LLM configuration."""
    from vibelint.llm import create_llm_manager

    llm_manager = create_llm_manager(config)
    if not llm_manager:
        logger.warning("No LLM configured - dynamic analysis unavailable")
        return None

    return DynamicAnalyzer(llm_manager)
```

---
### File: src/vibelint/dependency_graph_manager.py

```python
"""
Dependency Graph Manager for Vibelint

Builds, stores, and maintains the execution dependency graph using:
- NetworkX for in-memory graph analysis and algorithms
- Qdrant for persistent vector storage with embeddings
- Incremental updates for efficiency

This creates a living knowledge graph that an orchestrating LLM can query.
"""

import asyncio
import hashlib
import json
import time
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any, Dict, List

import networkx as nx

try:
    import httpx

    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False

from vibelint.runtime_tracer import (DependencyNode, TraceSession,
                             VanguardEmbeddingIntegration)


@dataclass
class GraphMetrics:
    """Metrics about the dependency graph."""

    total_nodes: int
    total_edges: int
    max_depth: int
    most_connected_node: str
    performance_critical_path: List[str]
    embedding_coverage: float
    last_updated: float


@dataclass
class GraphQuery:
    """Query for finding relevant dependencies."""

    semantic_query: str
    code_similarity_threshold: float = 0.8
    performance_threshold_ms: float = 100.0
    max_results: int = 10


class DependencyGraphManager:
    """
    Manages the execution dependency graph with hybrid storage:
    - NetworkX for fast in-memory graph operations
    - Qdrant for persistent embedding-based storage
    - Incremental updates to keep both in sync
    """

    def __init__(
        self,
        qdrant_url: str = "http://localhost:6333",
        collection_name: str = "vibelint_embeddings",
        config: Dict[str, Any] = None,
    ):
        self.qdrant_url = qdrant_url
        self.collection_name = collection_name
        self.config = config or {}

        # In-memory graph for fast analysis
        self.dependency_graph = nx.DiGraph()

        # Embedding integration
        self.embedding_integration = VanguardEmbeddingIntegration(config)

        # Cache for performance
        self.node_cache = {}
        self.embedding_cache = {}

        # Graph statistics
        self.metrics = GraphMetrics(
            total_nodes=0,
            total_edges=0,
            max_depth=0,
            most_connected_node="",
            performance_critical_path=[],
            embedding_coverage=0.0,
            last_updated=time.time(),
        )

    async def build_graph_from_trace(self, trace_session: TraceSession) -> Dict[str, Any]:
        """
        Build/update dependency graph from a trace session.
        This is the main entry point for graph construction.
        """
        print(f"🔗 Building dependency graph from {trace_session.total_calls} calls...")

        # Create embedding-enhanced dependency nodes
        source_cache = {}
        dependency_nodes = await self.embedding_integration.create_dependency_embeddings(
            trace_session.call_stack, source_cache
        )

        # Update in-memory NetworkX graph
        self._update_networkx_graph(dependency_nodes, trace_session.call_stack)

        # Store/update nodes in Qdrant with embeddings
        await self._store_nodes_in_qdrant(dependency_nodes)

        # Update graph metrics
        self._update_graph_metrics()

        # Save dependency knowledge graph to trace session
        trace_session.dependency_knowledge_graph = dependency_nodes

        return {
            "nodes_processed": len(dependency_nodes),
            "graph_metrics": asdict(self.metrics),
            "critical_path": self._find_critical_path(),
            "performance_hotspots": self._identify_performance_hotspots(),
        }

    def _update_networkx_graph(self, nodes: Dict[str, DependencyNode], call_stack: List):
        """Update the in-memory NetworkX graph."""
        # Add/update nodes
        for node_id, node in nodes.items():
            # Add node with rich attributes
            self.dependency_graph.add_node(
                node_id,
                signature=node.function_signature,
                module=node.module_path,
                avg_time_ms=node.performance_profile.get("avg_time_ms", 0),
                call_frequency=node.execution_context.get("call_frequency", 1),
                io_operations=node.performance_profile.get("io_operations", 0),
                has_code_embedding=node.code_embedding is not None,
                has_semantic_embedding=node.semantic_embedding is not None,
            )

            # Add dependency edges
            for dependency in node.dependencies:
                if dependency in nodes:  # Only add edges to nodes we have
                    self.dependency_graph.add_edge(
                        node_id,
                        dependency,
                        weight=1.0,  # Could be based on call frequency
                        relationship_type="calls",
                    )

        # Add temporal edges (what calls what in sequence)
        self._add_temporal_edges(call_stack)

    def _add_temporal_edges(self, call_stack: List):
        """Add edges based on temporal execution order."""
        for i in range(len(call_stack) - 1):
            current_call = call_stack[i]
            next_call = call_stack[i + 1]

            current_node = f"{current_call.module_name}.{current_call.function_name}"
            next_node = f"{next_call.module_name}.{next_call.function_name}"

            # Add temporal edge if the next call is deeper (called by current)
            if next_call.call_depth > current_call.call_depth:
                if self.dependency_graph.has_edge(current_node, next_node):
                    # Increase weight of existing edge
                    self.dependency_graph[current_node][next_node]["weight"] += 0.1
                else:
                    self.dependency_graph.add_edge(
                        current_node, next_node, weight=0.5, relationship_type="temporal_sequence"
                    )

    async def _store_nodes_in_qdrant(self, nodes: Dict[str, DependencyNode]):
        """Store dependency nodes in Qdrant with embeddings."""
        if not HTTPX_AVAILABLE:
            print("⚠️  httpx not available, skipping Qdrant storage")
            return

        try:
            async with httpx.AsyncClient() as client:
                points = []

                for node_id, node in nodes.items():
                    # Use code embedding as primary vector, fallback to semantic
                    vector = node.code_embedding or node.semantic_embedding

                    if not vector:
                        # Create simple hash-based vector if no embeddings
                        vector = self._create_hash_vector(node_id)

                    # Create point for Qdrant
                    point = {
                        "id": self._generate_point_id(node_id),
                        "vector": vector,
                        "payload": {
                            "memory_type": "dependency_node",
                            "node_id": node_id,
                            "function_signature": node.function_signature,
                            "module_path": node.module_path,
                            "dependencies": node.dependencies,
                            "execution_context": node.execution_context,
                            "performance_profile": node.performance_profile,
                            "has_code_embedding": node.code_embedding is not None,
                            "has_semantic_embedding": node.semantic_embedding is not None,
                            "timestamp_iso": time.strftime("%Y-%m-%dT%H:%M:%S"),
                            "timestamp_unix": time.time(),
                            "graph_version": int(time.time()),  # For versioning
                        },
                    }
                    points.append(point)

                # Batch upsert to Qdrant
                if points:
                    response = await client.put(
                        f"{self.qdrant_url}/collections/{self.collection_name}/points",
                        json={"points": points},
                        timeout=30.0,
                    )

                    if response.status_code == 200:
                        print(f"✅ Stored {len(points)} dependency nodes in Qdrant")
                    else:
                        print(f"❌ Failed to store nodes: {response.status_code}")

        except Exception as e:
            print(f"⚠️  Failed to store in Qdrant: {e}")

    def _create_hash_vector(self, node_id: str, dimensions: int = 768) -> List[float]:
        """Create a hash-based vector when no embeddings are available."""
        # Create deterministic hash-based vector
        hash_obj = hashlib.md5(node_id.encode())
        hash_bytes = hash_obj.digest()

        # Convert to normalized vector
        vector = []
        for i in range(dimensions):
            byte_idx = i % len(hash_bytes)
            value = (hash_bytes[byte_idx] / 255.0) * 2 - 1  # Normalize to [-1, 1]
            vector.append(value)

        return vector

    def _generate_point_id(self, node_id: str) -> str:
        """Generate consistent point ID for Qdrant."""
        return f"dep_{hashlib.md5(node_id.encode()).hexdigest()[:16]}"

    async def query_similar_dependencies(self, query: GraphQuery) -> List[Dict[str, Any]]:
        """
        Query for similar dependencies using semantic/code embeddings.
        This is what an orchestrating LLM would use to find relevant code.
        """
        if not HTTPX_AVAILABLE:
            return self._fallback_graph_query(query)

        try:
            # Get embedding for query
            query_embedding = await self.embedding_integration._get_semantic_embedding(
                query.semantic_query
            )

            if not query_embedding:
                return self._fallback_graph_query(query)

            # Search Qdrant for similar nodes
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{self.qdrant_url}/collections/{self.collection_name}/points/search",
                    json={
                        "vector": query_embedding,
                        "limit": query.max_results,
                        "score_threshold": query.code_similarity_threshold,
                        "filter": {
                            "must": [{"key": "memory_type", "match": {"value": "dependency_node"}}]
                        },
                        "with_payload": True,
                    },
                    timeout=30.0,
                )

                if response.status_code == 200:
                    results = response.json().get("result", [])

                    # Enrich with NetworkX graph analysis
                    enriched_results = []
                    for result in results:
                        payload = result.get("payload", {})
                        node_id = payload.get("node_id")

                        if node_id and node_id in self.dependency_graph:
                            # Add graph-based metrics
                            graph_metrics = self._calculate_node_metrics(node_id)
                            payload.update(graph_metrics)

                            enriched_results.append(
                                {
                                    "similarity_score": result.get("score", 0),
                                    "node_info": payload,
                                    "dependencies": payload.get("dependencies", []),
                                    "graph_position": graph_metrics,
                                }
                            )

                    return enriched_results

        except Exception as e:
            print(f"Query failed: {e}")

        return self._fallback_graph_query(query)

    def _fallback_graph_query(self, query: GraphQuery) -> List[Dict[str, Any]]:
        """Fallback query using NetworkX when vector search isn't available."""
        results = []

        # Simple text matching on node names
        query_terms = query.semantic_query.lower().split()

        for node_id in self.dependency_graph.nodes():
            node_data = self.dependency_graph.nodes[node_id]

            # Calculate text similarity
            node_text = f"{node_id} {node_data.get('signature', '')}".lower()
            matches = sum(1 for term in query_terms if term in node_text)
            similarity = matches / len(query_terms) if query_terms else 0

            if similarity > 0.3:  # Basic threshold
                graph_metrics = self._calculate_node_metrics(node_id)

                results.append(
                    {
                        "similarity_score": similarity,
                        "node_info": {"node_id": node_id, **node_data, **graph_metrics},
                        "dependencies": list(self.dependency_graph.successors(node_id)),
                        "graph_position": graph_metrics,
                    }
                )

        return sorted(results, key=lambda x: x["similarity_score"], reverse=True)[
            : query.max_results
        ]

    def _calculate_node_metrics(self, node_id: str) -> Dict[str, Any]:
        """Calculate NetworkX-based metrics for a node."""
        if node_id not in self.dependency_graph:
            return {}

        try:
            return {
                "in_degree": self.dependency_graph.in_degree(node_id),
                "out_degree": self.dependency_graph.out_degree(node_id),
                "betweenness_centrality": nx.betweenness_centrality(self.dependency_graph).get(
                    node_id, 0
                ),
                "pagerank": nx.pagerank(self.dependency_graph).get(node_id, 0),
                "clustering": nx.clustering(self.dependency_graph.to_undirected()).get(node_id, 0),
                "shortest_path_to_root": self._distance_to_root(node_id),
                "is_critical_path": node_id in self._find_critical_path(),
            }
        except Exception:
            return {"error": "failed_to_calculate_metrics"}

    def _distance_to_root(self, node_id: str) -> int:
        """Calculate distance to root nodes (nodes with no predecessors)."""
        try:
            root_nodes = [
                n for n in self.dependency_graph.nodes() if self.dependency_graph.in_degree(n) == 0
            ]

            if not root_nodes:
                return 0

            distances = []
            for root in root_nodes:
                try:
                    distance = nx.shortest_path_length(self.dependency_graph, root, node_id)
                    distances.append(distance)
                except nx.NetworkXNoPath:
                    continue

            return min(distances) if distances else float("inf")

        except Exception:
            return 0

    def _find_critical_path(self) -> List[str]:
        """Find the performance-critical path through the graph."""
        try:
            # Weight edges by execution time
            weighted_graph = self.dependency_graph.copy()

            for u, v, data in weighted_graph.edges(data=True):
                node_time = self.dependency_graph.nodes[v].get("avg_time_ms", 1)
                data["weight"] = -node_time  # Negative for longest path

            # Find longest path (most time-consuming)
            if weighted_graph.nodes():
                # Simple heuristic: path from highest out-degree to highest in-degree
                start_node = max(weighted_graph.nodes(), key=lambda n: weighted_graph.out_degree(n))
                end_node = max(weighted_graph.nodes(), key=lambda n: weighted_graph.in_degree(n))

                try:
                    path = nx.shortest_path(weighted_graph, start_node, end_node, weight="weight")
                    return path
                except nx.NetworkXNoPath:
                    pass

            return []

        except Exception:
            return []

    def _identify_performance_hotspots(self) -> List[Dict[str, Any]]:
        """Identify performance hotspots in the graph."""
        hotspots = []

        for node_id in self.dependency_graph.nodes():
            node_data = self.dependency_graph.nodes[node_id]
            avg_time = node_data.get("avg_time_ms", 0)
            call_freq = node_data.get("call_frequency", 1)

            # Score based on time * frequency
            hotspot_score = avg_time * call_freq

            if hotspot_score > 10:  # Threshold for significant impact
                hotspots.append(
                    {
                        "node_id": node_id,
                        "avg_time_ms": avg_time,
                        "call_frequency": call_freq,
                        "hotspot_score": hotspot_score,
                        "graph_centrality": nx.betweenness_centrality(self.dependency_graph).get(
                            node_id, 0
                        ),
                    }
                )

        return sorted(hotspots, key=lambda x: x["hotspot_score"], reverse=True)[:10]

    def _update_graph_metrics(self):
        """Update graph-level metrics."""
        self.metrics.total_nodes = self.dependency_graph.number_of_nodes()
        self.metrics.total_edges = self.dependency_graph.number_of_edges()
        self.metrics.last_updated = time.time()

        if self.dependency_graph.nodes():
            # Find most connected node
            degrees = dict(self.dependency_graph.degree())
            self.metrics.most_connected_node = max(degrees, key=degrees.get)

            # Calculate max depth
            try:
                self.metrics.max_depth = (
                    max(
                        self._distance_to_root(node)
                        for node in self.dependency_graph.nodes()
                        if self._distance_to_root(node) != float("inf")
                    )
                    if self.dependency_graph.nodes()
                    else 0
                )
            except:
                self.metrics.max_depth = 0

            # Calculate embedding coverage
            nodes_with_embeddings = sum(
                1
                for node_id in self.dependency_graph.nodes()
                if self.dependency_graph.nodes[node_id].get("has_code_embedding")
                or self.dependency_graph.nodes[node_id].get("has_semantic_embedding")
            )
            self.metrics.embedding_coverage = nodes_with_embeddings / self.metrics.total_nodes

        # Update critical path
        self.metrics.performance_critical_path = self._find_critical_path()

    def export_graph_for_llm(self, output_path: Path) -> Dict[str, Any]:
        """
        Export graph in a format optimized for LLM consumption.
        This creates a structured knowledge base the orchestrating LLM can use.
        """
        llm_graph = {
            "metadata": {
                "created": time.strftime("%Y-%m-%d %H:%M:%S"),
                "metrics": asdict(self.metrics),
                "description": "Vibelint execution dependency graph for LLM orchestration",
            },
            "nodes": {},
            "dependencies": {},
            "performance_insights": {
                "critical_path": self.metrics.performance_critical_path,
                "hotspots": self._identify_performance_hotspots(),
                "bottlenecks": self._find_bottlenecks(),
            },
            "query_examples": self._generate_query_examples(),
        }

        # Export node information
        for node_id in self.dependency_graph.nodes():
            node_data = self.dependency_graph.nodes[node_id]
            metrics = self._calculate_node_metrics(node_id)

            llm_graph["nodes"][node_id] = {
                "signature": node_data.get("signature", ""),
                "module": node_data.get("module", ""),
                "performance": {
                    "avg_time_ms": node_data.get("avg_time_ms", 0),
                    "call_frequency": node_data.get("call_frequency", 1),
                    "io_operations": node_data.get("io_operations", 0),
                },
                "graph_metrics": metrics,
                "dependencies": list(self.dependency_graph.successors(node_id)),
                "dependents": list(self.dependency_graph.predecessors(node_id)),
            }

        # Export dependency relationships
        for node_id in self.dependency_graph.nodes():
            deps = list(self.dependency_graph.successors(node_id))
            if deps:
                llm_graph["dependencies"][node_id] = deps

        # Save to file
        with open(output_path, "w") as f:
            json.dump(llm_graph, f, indent=2, default=str)

        print(f"📊 LLM-optimized graph exported to: {output_path}")
        return llm_graph

    def _find_bottlenecks(self) -> List[Dict[str, Any]]:
        """Find potential bottlenecks in the execution graph."""
        bottlenecks = []

        try:
            # Nodes with high betweenness centrality are potential bottlenecks
            centrality = nx.betweenness_centrality(self.dependency_graph)

            for node_id, centrality_score in centrality.items():
                if centrality_score > 0.1:  # Significant centrality
                    node_data = self.dependency_graph.nodes[node_id]

                    bottlenecks.append(
                        {
                            "node_id": node_id,
                            "centrality_score": centrality_score,
                            "avg_time_ms": node_data.get("avg_time_ms", 0),
                            "in_degree": self.dependency_graph.in_degree(node_id),
                            "out_degree": self.dependency_graph.out_degree(node_id),
                            "bottleneck_type": "high_centrality",
                        }
                    )

        except Exception:
            pass

        return sorted(bottlenecks, key=lambda x: x["centrality_score"], reverse=True)

    def _generate_query_examples(self) -> List[Dict[str, str]]:
        """Generate example queries an orchestrating LLM might use."""
        return [
            {
                "query": "Find functions related to configuration loading",
                "semantic_query": "configuration loading config file parsing settings",
                "use_case": "When user asks to modify configuration behavior",
            },
            {
                "query": "Find performance-critical validation functions",
                "semantic_query": "validation performance critical slow execution time",
                "use_case": "When optimizing validation performance",
            },
            {
                "query": "Find functions that handle file I/O operations",
                "semantic_query": "file input output read write operations filesystem",
                "use_case": "When debugging file-related issues",
            },
            {
                "query": "Find error handling and exception management",
                "semantic_query": "error handling exception management try catch error",
                "use_case": "When improving error handling robustness",
            },
        ]


# Integration function for vibelint self-improvement
async def build_vibelint_dependency_graph(
    trace_sessions: List[TraceSession], config: Dict[str, Any] = None
) -> DependencyGraphManager:
    """
    Build a comprehensive dependency graph from multiple trace sessions.
    This is the main integration point for vibelint's self-improvement system.
    """
    print("🔗 Building comprehensive vibelint dependency graph...")

    graph_manager = DependencyGraphManager(config=config)

    for session in trace_sessions:
        await graph_manager.build_graph_from_trace(session)

    # Export for LLM consumption
    export_path = (
        Path(__file__).parent.parent.parent / ".vibelint-self-improvement" / "dependency_graph.json"
    )
    export_path.parent.mkdir(exist_ok=True)
    graph_manager.export_graph_for_llm(export_path)

    print(
        f"📊 Graph built: {graph_manager.metrics.total_nodes} nodes, {graph_manager.metrics.total_edges} edges"
    )
    print(f"🎯 Embedding coverage: {graph_manager.metrics.embedding_coverage:.1%}")

    return graph_manager


if __name__ == "__main__":
    # Example usage
    async def demo():
        from vibelint.runtime_tracer import trace_vibelint_module

        # Trace a module
        session = trace_vibelint_module("config")

        # Build graph
        graph_manager = await build_vibelint_dependency_graph([session])

        # Query example
        query = GraphQuery(semantic_query="configuration loading and parsing", max_results=5)

        results = await graph_manager.query_similar_dependencies(query)
        print(f"Found {len(results)} similar dependencies")

    asyncio.run(demo())
```

---
### File: src/vibelint/diagnostics.py

```python
"""
Simple diagnostics for dual LLM setup with context probing.

Discovers actual context limits for both fast and orchestrator LLMs
using systematic testing approaches.

tools/vibelint/src/vibelint/diagnostics.py
"""

import logging
import time
from pathlib import Path
from typing import Any, Dict

import requests

from .context.probing import ProbeResult
from vibelint.llm import create_llm_manager, LLMRole

logger = logging.getLogger(__name__)

__all__ = ["run_diagnostics", "run_benchmark"]


class DualLLMDiagnostics:
    """Simple diagnostics for dual LLM setup."""

    def __init__(self, config: Dict[str, Any]):
        """Initialize with vibelint configuration."""
        self.config = config
        self.llm_manager = create_llm_manager(config)

    def _extract_llm_configs(self) -> Dict[str, Dict[str, Any]]:
        """Extract LLM configurations for probing."""
        llm_config = self.config.get("llm", {})
        configs = {}

        # Fast LLM
        if llm_config.get("fast_api_url"):
            configs["fast"] = {
                "api_base_url": llm_config["fast_api_url"],
                "model": llm_config["fast_model"],
                "temperature": llm_config.get("fast_temperature", 0.1),
            }

        # Orchestrator LLM
        if llm_config.get("orchestrator_api_url"):
            configs["orchestrator"] = {
                "api_base_url": llm_config["orchestrator_api_url"],
                "model": llm_config["orchestrator_model"],
                "temperature": llm_config.get("orchestrator_temperature", 0.2),
            }

        return configs

    async def run_context_probing(self) -> Dict[str, ProbeResult]:
        """Run linear ramp-up context probing to find optimal speed/context balance."""
        llm_configs = self._extract_llm_configs()

        if not llm_configs:
            logger.error("No LLM configurations found")
            return {}

        results = {}

        for llm_name, config in llm_configs.items():
            print(f"\n[SEARCH] Testing {llm_name.upper()} LLM context limits...")

            # Set timeout based on LLM type
            model_name = config["model"].lower()
            if "gpt-oss-20b" in model_name and "120b" not in model_name:  # Fast vLLM model
                timeout_seconds = 15  # Fast model should be quick
                context_sizes = [100, 500, 1000, 2000, 4000, 8000, 16000]  # Test higher contexts
            else:  # 120B orchestrator model
                timeout_seconds = 90  # 120B model needs more time for large contexts
                context_sizes = [
                    100,
                    1000,
                    4000,
                    8000,
                    16000,
                    24000,
                    32000,
                    40000,
                ]  # Test higher with stable system

            max_working_context = 0
            total_duration = 0
            successful_tests = 0
            failed_tests = 0

            import time

            import requests

            for context_size in context_sizes:
                try:
                    # Generate content roughly matching token count (4 chars ≈ 1 token)
                    content = "Context test. " * (context_size // 3)
                    content = content[: context_size * 4]  # Approximate token count

                    print(f"  Testing {context_size} tokens... ", end="", flush=True)

                    start_time = time.time()
                    response = requests.post(
                        f"{config['api_base_url']}/v1/chat/completions",
                        json={
                            "model": config["model"],
                            "messages": [{"role": "user", "content": f"Summarize: {content}"}],
                            "max_tokens": 20,  # Small response to focus on context processing
                            "temperature": config["temperature"],
                        },
                        timeout=timeout_seconds,
                    )
                    duration = time.time() - start_time

                    if response.status_code == 200:
                        max_working_context = context_size
                        total_duration += duration
                        successful_tests += 1
                        print(f"[PASS] {duration:.1f}s")
                    else:
                        failed_tests += 1
                        print(f"[FAIL] HTTP {response.status_code}")
                        break  # Stop on first HTTP error

                except requests.exceptions.Timeout:
                    failed_tests += 1
                    print(f"[TIMER] Timeout (>{timeout_seconds}s)")
                    break  # Stop on first timeout
                except (requests.exceptions.RequestException, ValueError, KeyError) as e:
                    failed_tests += 1
                    print(f"[FAIL] Error: {str(e)[:50]}...")
                    break  # Stop on first error

                # Small delay to let server recover
                time.sleep(1)

            # Calculate results
            if successful_tests > 0:
                avg_latency = (total_duration / successful_tests) * 1000  # Convert to ms
                success_rate = successful_tests / (successful_tests + failed_tests)

                from .context.probing import InferenceEngine, ProbeResult

                results[llm_name] = ProbeResult(
                    model=config["model"],
                    api_base_url=config["api_base_url"],
                    inference_engine=InferenceEngine.OPENAI_COMPATIBLE,
                    max_context_tokens=max_working_context,
                    effective_context_tokens=int(max_working_context * 0.9),  # 10% safety margin
                    max_output_tokens=2048,
                    avg_latency_ms=avg_latency,
                    throughput_tokens_per_sec=20.0 / (avg_latency / 1000),  # Rough estimate
                    success_rate=success_rate,
                    test_count=successful_tests + failed_tests,
                    needle_in_haystack_accuracy=1.0,  # Not tested
                    position_bias_detected=False,
                    first_failure_tokens=(
                        context_sizes[successful_tests] if failed_tests > 0 else None
                    ),
                    error_patterns=[],
                    recommended_max_prompt_tokens=int(
                        max_working_context * 0.8
                    ),  # 20% safety margin
                    recommended_batch_size=1,
                    probe_timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
                    probe_duration_seconds=total_duration,
                )

                print(
                    f"  [STATS] Result: {max_working_context} tokens max, {avg_latency:.0f}ms avg, {success_rate:.0%} success"
                )
            else:
                print(f"  [FAIL] No successful tests for {llm_name}")

        # Save assessment report
        if results:
            await self._save_assessment_results(results)

        return results

    async def _save_assessment_results(self, probe_results: Dict[str, ProbeResult]):
        """Save assessment report comparing configured vs actual capabilities."""
        # Get current user configuration
        user_config = self.config.get("llm", {})

        report = "# LLM Assessment Results\n\n"
        report += f"**Date:** {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
        report += "**Assessment:** Configuration vs Actual Performance\n\n"

        issues_found = False

        for llm_name, result in probe_results.items():
            report += f"## {llm_name.title()} LLM Assessment\n\n"

            # Current configuration
            prefix = f"{llm_name}_"
            configured_max_tokens = user_config.get(f"{prefix}max_tokens", 0)
            configured_context = user_config.get(f"{prefix}max_context_tokens", 0)

            report += "### Current Configuration:\n"
            report += f"- **Model:** {result.model}\n"
            report += f"- **API:** {result.api_base_url}\n"
            report += f"- **Configured Max Tokens:** {configured_max_tokens:,}\n"
            report += f"- **Configured Context:** {configured_context:,}\n\n"

            # Actual performance
            report += "### Discovered Performance:\n"
            report += f"- **Engine Type:** {result.inference_engine.value}\n"
            report += f"- **Actual Max Context:** {result.max_context_tokens:,} tokens\n"
            report += f"- **Effective Context:** {result.effective_context_tokens:,} tokens\n"
            report += f"- **Success Rate:** {result.success_rate:.1%}\n"
            report += f"- **Avg Latency:** {result.avg_latency_ms:.0f}ms\n\n"

            # Assessment and warnings
            report += "### Assessment:\n"

            if configured_context > result.effective_context_tokens:
                report += f"[WARNING]  **WARNING:** Configured context ({configured_context:,}) exceeds actual capacity ({result.effective_context_tokens:,})\n"
                issues_found = True
            elif configured_context == 0:
                report += f"[TIP] **INFO:** No context limit configured, discovered {result.effective_context_tokens:,} tokens\n"
            else:
                report += "[PASS] **OK:** Configured context within actual capacity\n"

            if result.success_rate < 0.9:
                report += f"[WARNING]  **WARNING:** Low success rate ({result.success_rate:.1%}) - LLM may be overloaded\n"
                issues_found = True
            else:
                report += f"[PASS] **OK:** Good success rate ({result.success_rate:.1%})\n"

            if result.avg_latency_ms > 10000:  # 10 seconds
                report += f"[WARNING]  **WARNING:** High latency ({result.avg_latency_ms:.0f}ms) - consider optimization\n"
                issues_found = True
            else:
                report += f"[PASS] **OK:** Acceptable latency ({result.avg_latency_ms:.0f}ms)\n"

            report += "\n"

        # Overall recommendations
        if issues_found:
            report += "## [TOOL] Recommended Actions\n\n"
            report += "Based on the assessment, consider updating your `pyproject.toml`:\n\n"
            report += "```toml\n"
            report += "[tool.vibelint.llm]\n"

            for llm_name, result in probe_results.items():
                prefix = f"{llm_name}_"
                configured_context = user_config.get(f"{prefix}max_context_tokens", 0)

                if configured_context > result.effective_context_tokens or configured_context == 0:
                    report += f"# Recommended limits for {llm_name} LLM based on testing\n"
                    report += f"{prefix}max_context_tokens = {result.effective_context_tokens}\n"
                    report += (
                        f"{prefix}max_prompt_tokens = {result.recommended_max_prompt_tokens}\n"
                    )

            report += "```\n\n"
        else:
            report += "## [PASS] Configuration Assessment\n\n"
            report += "Your current configuration appears to be well-matched to your LLM capabilities!\n\n"

        report += "### Next Steps\n\n"
        report += "1. Review any warnings above\n"
        report += "2. Update configuration if recommended\n"
        report += "3. Re-run diagnostics after changes to verify\n"
        report += "4. Monitor performance in production use\n"

        Path("LLM_ASSESSMENT_RESULTS.md").write_text(report, encoding="utf-8")

    async def benchmark_routing(self) -> Dict[str, Any]:
        """Simple benchmark of LLM routing logic."""
        if not self.llm_manager:
            return {"error": "No LLM manager configured"}

        # Test scenarios
        scenarios = [
            {"content": "Short docstring task", "task_type": "docstring", "expected": LLMRole.FAST},
            {"content": "x" * 5000, "task_type": "analysis", "expected": LLMRole.ORCHESTRATOR},
            {
                "content": "Architecture analysis",
                "task_type": "architecture",
                "expected": LLMRole.ORCHESTRATOR,
            },
        ]

        results = {"scenarios": [], "routing_accuracy": 0.0}
        correct = 0

        for scenario in scenarios:
            from vibelint.llm import LLMRequest

            request = LLMRequest(scenario["content"], scenario["task_type"])
            selected = self.llm_manager.select_llm(request)

            is_correct = selected == scenario["expected"]
            if is_correct:
                correct += 1

            results["scenarios"].append(
                {
                    "task": scenario["task_type"],
                    "content_size": len(scenario["content"]),
                    "expected": scenario["expected"].value,
                    "selected": selected.value,
                    "correct": is_correct,
                }
            )

        results["routing_accuracy"] = correct / len(scenarios)
        return results


async def run_diagnostics(config: Dict[str, Any]) -> Dict[str, Any]:
    """Run comprehensive diagnostics: context probing + routing benchmark.

    Args:
        config: Vibelint configuration dictionary

    Returns:
        Complete diagnostics results
    """
    diagnostics = DualLLMDiagnostics(config)

    try:
        # Step 1: Context probing
        print("=== LLM Context Probing ===")
        probe_results = await diagnostics.run_context_probing()

        if not probe_results:
            print("[FAIL] No LLM configurations found or probing failed")
            return {"error": "Context probing failed"}

        # Print context probing results
        for llm_name, result in probe_results.items():
            print(f"\n{llm_name.upper()} LLM:")
            print(f"  [OK] Max Context: {result.max_context_tokens:,} tokens")
            print(f"  [OK] Success Rate: {result.success_rate:.1%}")
            print(f"  [OK] Latency: {result.avg_latency_ms:.0f}ms")

        # Step 2: Routing benchmark
        print("\n=== LLM Routing Benchmark ===")
        routing_results = await diagnostics.benchmark_routing()

        if "error" not in routing_results:
            print(f"Routing Accuracy: {routing_results['routing_accuracy']:.1%}")

            for scenario in routing_results["scenarios"]:
                status = "[OK]" if scenario["correct"] else "[ERROR]"
                print(
                    f"  {status} {scenario['task']}: {scenario['selected']} (expected: {scenario['expected']})"
                )

        print("\n[PASS] Comprehensive diagnostics completed")
        print("[DOC] Results saved to LLM_CALIBRATION_RESULTS.md")

        return {"probe_results": probe_results, "routing_results": routing_results, "success": True}

    except (requests.exceptions.RequestException, ValueError, KeyError, OSError) as e:
        print(f"[FAIL] Diagnostics failed: {e}")
        return {"error": str(e), "success": False}


async def run_benchmark(config: Dict[str, Any]) -> Dict[str, Any]:
    """Run LLM routing benchmark.

    Args:
        config: Vibelint configuration dictionary

    Returns:
        Benchmark results
    """
    diagnostics = DualLLMDiagnostics(config)

    try:
        print("=== LLM Routing Benchmark ===")
        results = await diagnostics.benchmark_routing()

        if "error" in results:
            print(f"[FAIL] {results['error']}")
            return results

        print(f"Routing Accuracy: {results['routing_accuracy']:.1%}")

        for scenario in results["scenarios"]:
            status = "[OK]" if scenario["correct"] else "[ERROR]"
            print(
                f"  {status} {scenario['task']}: {scenario['selected']} (expected: {scenario['expected']})"
            )

        return results

    except (requests.exceptions.RequestException, ValueError, KeyError, OSError) as e:
        print(f"[FAIL] Benchmark failed: {e}")
        return {"error": str(e)}
```

---
### File: src/vibelint/discovery.py

```python
"""
Discovers files using pathlib glob/rglob based on include patterns from
pyproject.toml, respecting the pattern's implied scope, then filters
using exclude patterns.

If `include_globs` is missing from the configuration:
- If `default_includes_if_missing` is provided, uses those patterns and logs a warning.
- Otherwise, logs an error and returns an empty list.

Exclusions from `config.exclude_globs` are always applied. Explicitly
provided paths are also excluded.

Warns if files within common VCS directories (.git, .hg, .svn) are found
and not covered by exclude_globs.

vibelint/src/vibelint/discovery.py
"""

import fnmatch
import logging
import os
import time
from collections.abc import Iterator
from pathlib import Path

from vibelint.config import Config
from vibelint.utils import get_relative_path

__all__ = ["discover_files", "discover_files_from_paths"]
logger = logging.getLogger(__name__)

_VCS_DIRS = {".git", ".hg", ".svn"}

# Default exclude patterns to avoid __pycache__, .git, etc. when using custom paths
_DEFAULT_EXCLUDE_PATTERNS = [
    "__pycache__/**",
    "*.pyc",
    "*.pyo",
    ".git/**",
    ".hg/**",
    ".svn/**",
    ".pytest_cache/**",
    ".coverage",
    ".mypy_cache/**",
    ".tox/**",
    "venv/**",
    ".venv/**",
    "env/**",
    ".env/**",
    "node_modules/**",
    ".DS_Store",
    "*.egg-info/**",
]


def _is_excluded(
    path_abs: Path,
    project_root: Path,
    exclude_globs: list[str],
    explicit_exclude_paths: set[Path],
    is_checking_directory_for_prune: bool = False,
) -> bool:
    """
    Checks if a discovered path (file or directory) should be excluded.

    For files: checks explicit paths first, then exclude globs.
    For directories (for pruning): checks if the directory itself matches an exclude glob.

    Args:
    path_abs: The absolute path of the file or directory to check.
    project_root: The absolute path of the project root.
    exclude_globs: List of glob patterns for exclusion from config.
    explicit_exclude_paths: Set of absolute paths to exclude explicitly (applies to files).
    is_checking_directory_for_prune: True if checking a directory for os.walk pruning.

    Returns:
    True if the path should be excluded/pruned, False otherwise.

    vibelint/src/vibelint/discovery.py
    """

    if not is_checking_directory_for_prune and path_abs in explicit_exclude_paths:
        logger.debug(f"Excluding explicitly provided path: {path_abs}")
        return True

    try:
        # Use resolve() for consistent comparison base
        rel_path = path_abs.resolve().relative_to(project_root.resolve())
        # Normalize for fnmatch and consistent comparisons
        rel_path_str = str(rel_path).replace("\\", "/")
    except ValueError:
        # Path is outside project root, consider it excluded for safety
        logger.warning(f"Path {path_abs} is outside project root {project_root}. Excluding.")
        return True
    except (OSError, TypeError) as e:
        logger.error(f"Error getting relative path for exclusion check on {path_abs}: {e}")
        return True  # Exclude if relative path fails

    for pattern in exclude_globs:
        normalized_pattern = pattern.replace("\\", "/")

        if is_checking_directory_for_prune:
            # Logic for pruning directories:
            # 1. Exact match: pattern "foo", rel_path_str "foo" (dir name)
            if fnmatch.fnmatch(rel_path_str, normalized_pattern):
                logger.debug(
                    f"Pruning dir '{rel_path_str}' due to direct match with exclude pattern '{pattern}'"
                )
                return True
            # 2. Dir pattern like "foo/" or "foo/**":
            #    pattern "build/", rel_path_str "build" -> match
            #    pattern "build/**", rel_path_str "build" -> match
            if normalized_pattern.endswith("/"):
                if rel_path_str == normalized_pattern[:-1]:  # pattern "build/", rel_path "build"
                    logger.debug(
                        f"Pruning dir '{rel_path_str}' due to match with dir pattern '{pattern}'"
                    )
                    return True
            elif normalized_pattern.endswith("/**"):
                if (
                    rel_path_str == normalized_pattern[:-3]
                ):  # e.g. pattern 'dir/**', rel_path_str 'dir'
                    logger.debug(
                        f"Pruning dir '{rel_path_str}' due to match with dir/** pattern '{pattern}'"
                    )
                    return True
        else:
            # Logic for excluding files:
            # Rule 1: File path matches the glob pattern directly
            # This handles patterns like "*.pyc", "temp/*", "specific_file.txt"
            if fnmatch.fnmatch(rel_path_str, normalized_pattern):
                logger.debug(f"Excluding file '{rel_path_str}' due to exclude pattern '{pattern}'")
                return True

            # Rule 2: File is within a directory excluded by a pattern ending with '/' or '/**'
            # e.g., exclude_glob is "build/", file is "build/lib/module.py"
            # e.g., exclude_glob is "output/**", file is "output/data/log.txt"
            if normalized_pattern.endswith("/"):  # Pattern "build/"
                if rel_path_str.startswith(normalized_pattern):
                    logger.debug(
                        f"Excluding file '{rel_path_str}' because it's in excluded dir prefix '{normalized_pattern}'"
                    )
                    return True
            elif normalized_pattern.endswith("/**"):  # Pattern "build/**"
                # For "build/**", we want to match files starting with "build/"
                base_dir_pattern = normalized_pattern[:-2]  # Results in "build/"
                if rel_path_str.startswith(base_dir_pattern):
                    logger.debug(
                        f"Excluding file '{rel_path_str}' because it's in excluded dir prefix '{normalized_pattern}'"
                    )
                    return True
            # Note: A simple exclude pattern like "build" (without / or **) for files
            # will only match a file *named* "build" via the fnmatch rule above.
            # To exclude all contents of a directory "build", the pattern should be
            # "build/" or "build/**". The pruning logic for directories handles these
            # patterns effectively for `os.walk`.

    return False


def _recursive_glob_with_pruning(
    search_root_abs: Path,
    glob_suffix_pattern: str,  # e.g., "*.py" or "data/*.json"
    project_root: Path,
    config_exclude_globs: list[str],
    explicit_exclude_paths: set[Path],
) -> Iterator[Path]:
    """
    Recursively walks a directory, prunes excluded subdirectories, and yields files
    matching the glob_suffix_pattern that are not otherwise excluded.

    Args:
        search_root_abs: Absolute path to the directory to start the search from.
        glob_suffix_pattern: The glob pattern to match files against (relative to directories in the walk).
        project_root: Absolute path of the project root.
        config_exclude_globs: List of exclude glob patterns from config.
        explicit_exclude_paths: Set of absolute file paths to explicitly exclude.

    Yields:
        Absolute Path objects for matching files.
    """
    logger.debug(
        f"Recursive walk starting at '{search_root_abs}' for pattern '.../{glob_suffix_pattern}'"
    )
    for root_str, dir_names, file_names in os.walk(str(search_root_abs), topdown=True):
        current_dir_abs = Path(root_str)

        # Prune directories
        original_dir_count = len(dir_names)
        dir_names[:] = [
            d_name
            for d_name in dir_names
            if not _is_excluded(
                current_dir_abs / d_name,
                project_root,
                config_exclude_globs,
                explicit_exclude_paths,  # Not used for dir pruning but passed for func signature
                is_checking_directory_for_prune=True,
            )
        ]
        if len(dir_names) < original_dir_count:
            logger.debug(
                f"Pruned {original_dir_count - len(dir_names)} subdirectories under {current_dir_abs}"
            )

        # Match files in the current (potentially non-pruned) directory
        for f_name in file_names:
            file_abs = current_dir_abs / f_name

            # Path of file relative to where the glob_suffix_pattern matching should start (search_root_abs)
            try:
                rel_to_search_root = file_abs.relative_to(search_root_abs)
            except ValueError:
                # Should not happen if os.walk starts at search_root_abs and yields descendants
                logger.warning(
                    f"File {file_abs} unexpectedly not relative to search root {search_root_abs}. Skipping."
                )
                continue

            normalized_rel_to_search_root_str = str(rel_to_search_root).replace("\\", "/")

            if fnmatch.fnmatch(normalized_rel_to_search_root_str, glob_suffix_pattern):
                # File matches the include pattern's suffix.
                # Now, perform a final check against global exclude rules for this specific file.
                if not _is_excluded(
                    file_abs,
                    project_root,
                    config_exclude_globs,
                    explicit_exclude_paths,
                    is_checking_directory_for_prune=False,
                ):
                    yield file_abs.resolve()  # Yield resolved path


def discover_files(
    paths: list[Path],
    config: Config,
    default_includes_if_missing: list[str] | None = None,
    explicit_exclude_paths: set[Path] | None = None,
) -> list[Path]:
    """
    Discovers files based on include/exclude patterns from configuration.
    Uses a custom walker for recursive globs (**) to enable directory pruning.

    Args:
    paths: Initial paths (largely ignored, globs operate from project root).
    config: The vibelint configuration object (must have project_root set).
    default_includes_if_missing: Fallback include patterns if 'include_globs' is not in config.
    explicit_exclude_paths: A set of absolute file paths to explicitly exclude.

    Returns:
    A sorted list of unique absolute Path objects for the discovered files.

    Raises:
    ValueError: If config.project_root is None.
    """

    if config.project_root is None:
        raise ValueError("Cannot discover files without a project root defined in Config.")

    project_root = config.project_root.resolve()
    candidate_files: set[Path] = set()
    _explicit_excludes = {p.resolve() for p in (explicit_exclude_paths or set())}

    # Validate and process include_globs configuration
    include_globs_config = config.get("include_globs")
    include_globs_effective = []

    if include_globs_config is None:
        if default_includes_if_missing is not None:
            logger.warning(
                "Configuration key 'include_globs' missing in [tool.vibelint] section "
                f"of pyproject.toml. Using default patterns: {default_includes_if_missing}"
            )
            include_globs_effective = default_includes_if_missing
        else:
            logger.error(
                "Configuration key 'include_globs' missing. No include patterns specified."
            )
    elif not isinstance(include_globs_config, list):
        logger.error(
            f"Config error: 'include_globs' must be a list. Found {type(include_globs_config)}."
        )
    elif not include_globs_config:
        logger.warning("Config: 'include_globs' is empty. No files will be included.")
    else:
        include_globs_effective = include_globs_config

    # Early return if no valid include patterns
    if not include_globs_effective:
        return []

    normalized_includes = [p.replace("\\", "/") for p in include_globs_effective]

    exclude_globs_config = config.get("exclude_globs", [])
    if not isinstance(exclude_globs_config, list):
        logger.error(
            f"Config error: 'exclude_globs' must be a list. Found {type(exclude_globs_config)}. Ignoring."
        )
        exclude_globs_effective = []
    else:
        exclude_globs_effective = exclude_globs_config
    normalized_exclude_globs = [p.replace("\\", "/") for p in exclude_globs_effective]

    logger.debug(f"Starting file discovery from project root: {project_root}")
    logger.debug(f"Effective Include globs: {normalized_includes}")
    logger.debug(f"Exclude globs: {normalized_exclude_globs}")
    logger.debug(f"Explicit excludes: {_explicit_excludes}")

    start_time = time.time()

    for pattern in normalized_includes:
        pattern_start_time = time.time()
        logger.debug(f"Processing include pattern: '{pattern}'")

        if "**" in pattern:
            parts = pattern.split("**", 1)
            base_dir_glob_part = parts[0].rstrip("/")  # "src" or ""
            # glob_suffix is the part after '**/', e.g., "*.py" or "some_dir/*.txt"
            glob_suffix = parts[1].lstrip("/")

            current_search_root_abs = project_root
            if base_dir_glob_part:
                # Handle potential multiple directory components in base_dir_glob_part
                # e.g. pattern "src/app/**/... -> base_dir_glob_part = "src/app"
                current_search_root_abs = (project_root / base_dir_glob_part).resolve()

            if not current_search_root_abs.is_dir():
                logger.debug(
                    f"Skipping include pattern '{pattern}': base '{current_search_root_abs}' not a directory."
                )
                continue

            logger.debug(
                f"Using recursive walker for pattern '{pattern}' starting at '{current_search_root_abs}', suffix '{glob_suffix}'"
            )
            for p_found in _recursive_glob_with_pruning(
                current_search_root_abs,
                glob_suffix,
                project_root,
                normalized_exclude_globs,
                _explicit_excludes,
            ):
                # _recursive_glob_with_pruning already yields resolved, filtered paths
                if p_found.is_file():  # Final check, though walker should only yield files
                    candidate_files.add(p_found)  # p_found is already resolved
        else:
            # Non-recursive glob (no "**")
            logger.debug(f"Using Path.glob for non-recursive pattern: '{pattern}'")
            try:
                for p in project_root.glob(pattern):
                    abs_p = p.resolve()
                    if p.is_symlink():
                        logger.debug(f"    -> Skipping discovered symlink: {p}")
                        continue
                    if p.is_file():
                        if not _is_excluded(
                            abs_p,
                            project_root,
                            normalized_exclude_globs,
                            _explicit_excludes,
                            False,
                        ):
                            candidate_files.add(abs_p)
            except PermissionError as e:
                logger.warning(
                    f"Permission denied for non-recursive glob '{pattern}': {e}. Skipping."
                )
            except (OSError, ValueError) as e:
                logger.error(f"Error during non-recursive glob '{pattern}': {e}", exc_info=True)

        pattern_time = time.time() - pattern_start_time
        logger.debug(f"Pattern '{pattern}' processing took {pattern_time:.4f} seconds.")

    discovery_time = time.time() - start_time
    logger.debug(
        f"Globbing and initial filtering finished in {discovery_time:.4f} seconds. Total candidates: {len(candidate_files)}"
    )

    final_files_set = candidate_files

    # VCS Warning Logic
    vcs_warnings: set[Path] = set()
    if final_files_set:
        for file_path in final_files_set:
            try:
                if any(part in _VCS_DIRS for part in file_path.relative_to(project_root).parts):
                    is_actually_excluded_by_vcs_pattern = False
                    for vcs_dir_name in _VCS_DIRS:
                        if _is_excluded(
                            file_path,
                            project_root,
                            [f"{vcs_dir_name}/", f"{vcs_dir_name}/**"],
                            set(),
                            False,
                        ):
                            is_actually_excluded_by_vcs_pattern = True
                            break
                    if not is_actually_excluded_by_vcs_pattern:
                        vcs_warnings.add(file_path)
            except ValueError:
                pass
            except (OSError, TypeError) as e_vcs:
                logger.debug(f"Error during VCS check for {file_path}: {e_vcs}")

    if vcs_warnings:
        logger.warning(
            f"Found {len(vcs_warnings)} included files within potential VCS directories "
            f"({', '.join(_VCS_DIRS)}). Consider adding patterns like '.git/**' to 'exclude_globs' "
            "in your [tool.vibelint] section if this was unintended."
        )
        try:
            paths_to_log = [
                get_relative_path(p, project_root) for p in sorted(list(vcs_warnings), key=str)[:5]
            ]
            for rel_path_warn in paths_to_log:
                logger.warning(f"  - {rel_path_warn}")
            if len(vcs_warnings) > 5:
                logger.warning(f"  - ... and {len(vcs_warnings) - 5} more.")
        except Exception as e_log:
            logger.warning(f"  (Error logging VCS warning example paths: {e_log})")

    final_count = len(final_files_set)
    if final_count == 0 and include_globs_effective:
        logger.warning("No files found matching include_globs patterns or all were excluded.")

    logger.debug(f"Discovery complete. Returning {final_count} files.")
    return sorted(list(final_files_set))


def discover_files_from_paths(
    custom_paths: list[Path],
    config: Config,
    explicit_exclude_paths: set[Path] | None = None,
) -> list[Path]:
    """
    Discover files from explicitly provided paths (include_globs override).

    This function handles user-provided paths as an override to the configured
    include_globs, while still respecting exclude_globs and sensible defaults
    to avoid processing __pycache__, .git, etc.

    Args:
        custom_paths: List of file or directory paths (include_globs override)
        config: The vibelint configuration object
        explicit_exclude_paths: Additional paths to explicitly exclude

    Returns:
        A sorted list of unique absolute Path objects for Python files
    """
    if config.project_root is None:
        raise ValueError("Cannot discover files without a project root defined in Config.")

    project_root = config.project_root.resolve()
    candidate_files: set[Path] = set()
    _explicit_excludes = {p.resolve() for p in (explicit_exclude_paths or set())}

    # Combine config exclude patterns with defaults
    config_exclude_globs = config.get("exclude_globs", [])
    if not isinstance(config_exclude_globs, list):
        config_exclude_globs = []

    # Always apply default exclude patterns to avoid __pycache__, .git, etc.
    all_exclude_patterns = _DEFAULT_EXCLUDE_PATTERNS + config_exclude_globs

    logger.info(f"Include globs override: processing {len(custom_paths)} custom path(s)")
    logger.debug(f"Using exclude patterns: {all_exclude_patterns}")

    for path in custom_paths:
        abs_path = path.resolve()

        if abs_path.is_file():
            # Single file - check if it's a Python file and not excluded
            if abs_path.suffix == ".py":
                if not _is_excluded(
                    abs_path,
                    project_root,
                    all_exclude_patterns,
                    _explicit_excludes,
                    is_checking_directory_for_prune=False,
                ):
                    candidate_files.add(abs_path)
                else:
                    logger.debug(f"Excluding file {abs_path} due to exclude patterns")

        elif abs_path.is_dir():
            # Directory - recursively find Python files while respecting exclusions
            logger.debug(f"Scanning directory: {abs_path}")

            # Use the existing recursive walker with Python file pattern
            for py_file in _recursive_glob_with_pruning(
                abs_path,
                "*.py",  # Only Python files
                project_root,
                all_exclude_patterns,
                _explicit_excludes,
            ):
                candidate_files.add(py_file)
        else:
            logger.warning(f"Path does not exist or is not a file/directory: {abs_path}")

    sorted_files = sorted(candidate_files)
    logger.info(f"Include globs override result: discovered {len(sorted_files)} Python files")
    return sorted_files
```

---
### File: src/vibelint/distributed_coordinator.py

```python
"""
Distributed Vibelint Coordinator for Microservices Architecture

Coordinates validation tasks across multiple services:
- vibelint: Core validation engine
- kaia-guardrails: Orchestration and security
- Claude Code: Human-in-the-loop workflows

This enables validation at scale across your entire microservices ecosystem.
"""

import asyncio
import json
import time
from dataclasses import asdict, dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import httpx
import tomli


@dataclass
class ServiceInfo:
    """Information about a distributed service."""

    name: str
    path: Path
    role: str  # "orchestrator", "validator", "coordinator"
    priority: int
    health_url: Optional[str] = None
    api_base: Optional[str] = None


@dataclass
class ValidationTask:
    """A validation task that can be distributed across services."""

    task_id: str
    task_type: str  # "single_file", "project_wide", "architecture", "security"
    target_files: List[str]
    requester_service: str
    priority: int
    assigned_services: List[str]
    status: str = "pending"  # "pending", "running", "completed", "failed"
    results: Dict[str, Any] = None
    created_at: float = None
    completed_at: Optional[float] = None


class DistributedVibelintCoordinator:
    """
    Coordinates vibelint validation across microservices architecture.

    Handles:
    - Service discovery and health monitoring
    - Task routing based on service capabilities
    - Result aggregation and conflict resolution
    - Shared state management via vector store
    """

    def __init__(self, config_path: Path):
        self.config_path = config_path
        self.config = self._load_distributed_config()
        self.services: Dict[str, ServiceInfo] = {}
        self.active_tasks: Dict[str, ValidationTask] = {}
        self.task_counter = 0

        # Initialize services
        self._discover_services()

        # Setup shared resources
        self.vector_store_url = (
            self.config.get("shared_resources", {}).get("vector_store", {}).get("url")
        )
        self.coordination_collection = self.config.get("coordination", {}).get(
            "shared_cache_collection"
        )

    def _load_distributed_config(self) -> Dict[str, Any]:
        """Load distributed vibelint configuration."""
        try:
            with open(self.config_path, "rb") as f:
                return tomli.load(f)
        except Exception as e:
            print(f"Failed to load distributed config: {e}")
            return {}

    def _discover_services(self):
        """Discover available services based on configuration."""
        services_config = self.config.get("services", {})
        base_path = self.config_path.parent

        for service_name, service_config in services_config.items():
            service_path = base_path / service_config["path"]
            if service_path.exists():
                self.services[service_name] = ServiceInfo(
                    name=service_name,
                    path=service_path,
                    role=service_config["role"],
                    priority=service_config["priority"],
                )
                print(f"Discovered service: {service_name} at {service_path}")

    async def validate_project_distributed(self, target_files: List[str] = None) -> Dict[str, Any]:
        """
        Run distributed validation across all services.

        This is the main entry point for coordinated validation.
        """
        print("Starting distributed validation...")

        # Create validation tasks based on routing rules
        tasks = self._create_validation_tasks(target_files)

        # Execute tasks across services
        results = await self._execute_tasks_distributed(tasks)

        # Aggregate and resolve conflicts
        final_results = await self._aggregate_results(results)

        # Store results in shared state
        await self._store_validation_results(final_results)

        return final_results

    def _create_validation_tasks(self, target_files: List[str] = None) -> List[ValidationTask]:
        """Create validation tasks based on routing configuration."""
        tasks = []
        routing = self.config.get("validation_routing", {})

        if not target_files:
            # Discover all relevant files in the project
            target_files = self._discover_project_files()

        # Create tasks based on validation types
        for validation_type, assigned_services in routing.items():
            self.task_counter += 1
            task = ValidationTask(
                task_id=f"task_{self.task_counter:04d}",
                task_type=validation_type,
                target_files=target_files,
                requester_service="coordinator",
                priority=1,
                assigned_services=assigned_services,
                created_at=time.time(),
            )
            tasks.append(task)
            self.active_tasks[task.task_id] = task

        return tasks

    def _discover_project_files(self) -> List[str]:
        """Discover all Python files in the project."""
        project_root = self.config_path.parent
        python_files = []

        for service_name, service_info in self.services.items():
            service_files = list(service_info.path.rglob("*.py"))
            python_files.extend([str(f.relative_to(project_root)) for f in service_files])

        return python_files

    async def _execute_tasks_distributed(self, tasks: List[ValidationTask]) -> Dict[str, Any]:
        """Execute validation tasks across distributed services."""
        results = {}

        # Group tasks by assigned services to minimize coordination overhead
        service_tasks = self._group_tasks_by_service(tasks)

        # Execute tasks in parallel across services
        async_tasks = []
        for service_name, service_tasks_list in service_tasks.items():
            async_tasks.append(self._execute_service_tasks(service_name, service_tasks_list))

        # Wait for all services to complete
        service_results = await asyncio.gather(*async_tasks, return_exceptions=True)

        # Combine results from all services
        for i, (service_name, _) in enumerate(service_tasks.items()):
            service_result = service_results[i]
            if isinstance(service_result, Exception):
                print(f"Service {service_name} failed: {service_result}")
                results[service_name] = {"error": str(service_result)}
            else:
                results[service_name] = service_result

        return results

    def _group_tasks_by_service(
        self, tasks: List[ValidationTask]
    ) -> Dict[str, List[ValidationTask]]:
        """Group tasks by the services that need to execute them."""
        service_tasks = {}

        for task in tasks:
            for service_name in task.assigned_services:
                if service_name not in service_tasks:
                    service_tasks[service_name] = []
                service_tasks[service_name].append(task)

        return service_tasks

    async def _execute_service_tasks(
        self, service_name: str, tasks: List[ValidationTask]
    ) -> Dict[str, Any]:
        """Execute tasks for a specific service."""
        service_info = self.services.get(service_name)
        if not service_info:
            return {"error": f"Service {service_name} not found"}

        try:
            if service_name == "vibelint":
                return await self._execute_vibelint_tasks(service_info, tasks)
            elif service_name == "kaia-guardrails":
                return await self._execute_kaia_tasks(service_info, tasks)
            else:
                return {"error": f"Unknown service type: {service_name}"}

        except Exception as e:
            return {"error": f"Failed to execute tasks for {service_name}: {e}"}

    async def _execute_vibelint_tasks(
        self, service_info: ServiceInfo, tasks: List[ValidationTask]
    ) -> Dict[str, Any]:
        """Execute validation tasks using vibelint service."""
        # Import vibelint dynamically to avoid circular dependencies
        import sys

        sys.path.insert(0, str(service_info.path / "src"))

        try:
            from vibelint.config import load_config
            from vibelint.core import VibelintCore

            # Load service-specific configuration
            vibelint_config = load_config(service_info.path)
            core = VibelintCore(vibelint_config)

            results = {}
            for task in tasks:
                print(f"Executing {task.task_type} validation via vibelint...")

                # Execute validation based on task type
                if task.task_type in ["single_file", "project_wide", "architecture"]:
                    task_result = await core.validate_files(task.target_files)
                    results[task.task_id] = {
                        "task_type": task.task_type,
                        "files_validated": len(task.target_files),
                        "violations": task_result.get("violations", []),
                        "metrics": task_result.get("metrics", {}),
                        "service": "vibelint",
                    }
                    task.status = "completed"
                    task.completed_at = time.time()

            return results

        except ImportError as e:
            return {"error": f"Failed to import vibelint: {e}"}

    async def _execute_kaia_tasks(
        self, service_info: ServiceInfo, tasks: List[ValidationTask]
    ) -> Dict[str, Any]:
        """Execute tasks using kaia-guardrails service."""
        # Import kaia-guardrails dynamically
        import sys

        sys.path.insert(0, str(service_info.path / "src"))

        try:
            from kaia_guardrails.memory_system import memory_system
            from kaia_guardrails.orchestrator import KaiaOrchestrator

            orchestrator = KaiaOrchestrator()
            results = {}

            for task in tasks:
                print(f"Executing {task.task_type} analysis via kaia-guardrails...")

                if task.task_type == "architecture":
                    # Architecture analysis with memory integration
                    task_result = await orchestrator.analyze_architecture(task.target_files)
                    results[task.task_id] = {
                        "task_type": task.task_type,
                        "architecture_analysis": task_result,
                        "memory_conflicts": await self._check_memory_conflicts(task),
                        "service": "kaia-guardrails",
                    }

                elif task.task_type == "security":
                    # Security validation
                    task_result = await orchestrator.validate_security(task.target_files)
                    results[task.task_id] = {
                        "task_type": task.task_type,
                        "security_findings": task_result,
                        "service": "kaia-guardrails",
                    }

                task.status = "completed"
                task.completed_at = time.time()

            return results

        except ImportError as e:
            return {"error": f"Failed to import kaia-guardrails: {e}"}

    async def _check_memory_conflicts(self, task: ValidationTask) -> Dict[str, Any]:
        """Check for memory conflicts using kaia-guardrails EBR system."""
        try:
            from kaia_guardrails.memory_conflict_resolver import \
                MemoryConflictResolver

            resolver = MemoryConflictResolver()
            pattern_hash = f"validation_{task.task_type}_{hash(tuple(task.target_files))}"

            conflict = await resolver.detect_memory_conflicts(pattern_hash)
            if conflict:
                return {
                    "has_conflicts": True,
                    "conflict_details": asdict(conflict),
                    "requires_ebr": True,
                }
            else:
                return {"has_conflicts": False}

        except Exception as e:
            return {"error": f"Memory conflict check failed: {e}"}

    async def _aggregate_results(self, service_results: Dict[str, Any]) -> Dict[str, Any]:
        """Aggregate and resolve conflicts between service results."""
        aggregated = {
            "timestamp": datetime.now().isoformat(),
            "services_executed": list(service_results.keys()),
            "total_tasks": len(self.active_tasks),
            "service_results": service_results,
            "conflicts_detected": [],
            "resolution_strategy": "evidence_based_reasoning",
        }

        # Detect conflicts between service results
        conflicts = self._detect_result_conflicts(service_results)
        if conflicts:
            aggregated["conflicts_detected"] = conflicts
            # Use EBR to resolve conflicts if kaia-guardrails is available
            if "kaia-guardrails" in self.services:
                aggregated["conflict_resolution"] = await self._resolve_conflicts_via_ebr(conflicts)

        return aggregated

    def _detect_result_conflicts(self, service_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Detect conflicts between results from different services."""
        conflicts = []

        # Compare validation results for the same files across services
        file_results = {}

        for service_name, results in service_results.items():
            if isinstance(results, dict) and "error" not in results:
                for task_id, task_result in results.items():
                    task_type = task_result.get("task_type")
                    if task_type in file_results:
                        # Potential conflict - same task type from different services
                        conflicts.append(
                            {
                                "task_type": task_type,
                                "conflicting_services": [
                                    file_results[task_type]["service"],
                                    service_name,
                                ],
                                "task_ids": [file_results[task_type]["task_id"], task_id],
                            }
                        )
                    else:
                        file_results[task_type] = {
                            "service": service_name,
                            "task_id": task_id,
                            "result": task_result,
                        }

        return conflicts

    async def _resolve_conflicts_via_ebr(self, conflicts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Resolve conflicts using Evidence-Based Reasoning."""
        try:
            from kaia_guardrails.memory_conflict_resolver import \
                MemoryConflictResolver

            resolver = MemoryConflictResolver()
            resolutions = {}

            for conflict in conflicts:
                # Create EBR context for this conflict
                pattern_hash = f"service_conflict_{hash(json.dumps(conflict, sort_keys=True))}"
                ebr_context = await resolver.prepare_ebr_context_for_llm(conflict)

                resolutions[pattern_hash] = {
                    "conflict": conflict,
                    "ebr_context": ebr_context,
                    "resolution": "trust_most_recent_evidence",  # Default strategy
                }

            return resolutions

        except Exception as e:
            return {"error": f"EBR conflict resolution failed: {e}"}

    async def _store_validation_results(self, results: Dict[str, Any]):
        """Store validation results in shared vector store for future reference."""
        if not self.vector_store_url or not self.coordination_collection:
            print("No vector store configured for result storage")
            return

        try:
            # Store results with timestamp for future EBR analysis
            storage_entry = {
                "type": "distributed_validation_result",
                "timestamp": datetime.now().isoformat(),
                "results": results,
                "services_involved": results.get("services_executed", []),
                "project_hash": hash(str(self.config_path.parent)),
            }

            async with httpx.AsyncClient() as client:
                response = await client.put(
                    f"{self.vector_store_url}/collections/{self.coordination_collection}/points",
                    json={
                        "points": [
                            {
                                "id": f"validation_{int(time.time())}",
                                "payload": storage_entry,
                                "vector": [0.0] * 768,  # Placeholder vector
                            }
                        ]
                    },
                )

                if response.status_code == 200:
                    print("Validation results stored in shared vector store")
                else:
                    print(f"Failed to store results: {response.status_code}")

        except Exception as e:
            print(f"Failed to store validation results: {e}")

    async def health_check_services(self) -> Dict[str, bool]:
        """Check health of all distributed services."""
        health_status = {}

        for service_name, service_info in self.services.items():
            try:
                # Check if service directory exists and has required files
                config_file = service_info.path / "pyproject.toml"
                src_dir = service_info.path / "src"

                is_healthy = config_file.exists() and src_dir.exists()
                health_status[service_name] = is_healthy

                if not is_healthy:
                    print(f"Service {service_name} is unhealthy - missing required files")

            except Exception as e:
                health_status[service_name] = False
                print(f"Health check failed for {service_name}: {e}")

        return health_status


# Example usage for distributed coordination
async def run_distributed_validation():
    """Example of running distributed validation across microservices."""
    config_path = Path(__file__).parent.parent.parent.parent.parent / "vibelint-distributed.toml"

    coordinator = DistributedVibelintCoordinator(config_path)

    # Check service health
    health = await coordinator.health_check_services()
    print(f"Service health: {health}")

    # Run distributed validation
    results = await coordinator.validate_project_distributed()
    print(f"Distributed validation complete: {json.dumps(results, indent=2)}")

    return results


if __name__ == "__main__":
    asyncio.run(run_distributed_validation())
```

---
### File: src/vibelint/embedding_client.py

```python
"""
Embedding Client for Specialized Code and Natural Language Embeddings.

This module provides a unified interface for accessing both local and remote
embedding models, with specialized endpoints for code analysis and natural
language processing.

Usage:
    from vibelint.embedding_client import EmbeddingClient

    client = EmbeddingClient()

    # Code embeddings (optimized for code similarity, patterns, architecture)
    code_embeddings = client.get_code_embeddings(["def function():", "class MyClass:"])

    # Natural language embeddings (optimized for documentation, comments)
    natural_embeddings = client.get_natural_embeddings(["docstring text", "comment text"])

vibelint/src/vibelint/embedding_client.py
"""

import logging
import os
from typing import Any, Dict, List, Optional

import requests

logger = logging.getLogger(__name__)


class EmbeddingClient:
    """
    Unified embedding client supporting both specialized remote endpoints
    and local fallback models.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize the embedding client with configuration.

        Args:
            config: Configuration dictionary with embedding settings
        """
        self.config = config or {}
        self._load_configuration()
        self._initialize_clients()

    def _load_configuration(self):
        """Load configuration from various sources."""
        embedding_config = self.config.get("embeddings", {})

        # Specialized endpoint configuration
        self.code_api_url = embedding_config.get(
            "code_api_url", "https://vanguardone-embedding-auth-worker.mithran-mohanraj.workers.dev"
        )
        self.natural_api_url = embedding_config.get(
            "natural_api_url",
            "https://vanguardtwo-embedding-auth-worker.mithran-mohanraj.workers.dev",
        )

        self.code_model = embedding_config.get("code_model", "text-embedding-ada-002")
        self.natural_model = embedding_config.get("natural_model", "text-embedding-ada-002")

        self.use_specialized = embedding_config.get("use_specialized_embeddings", True)
        self.similarity_threshold = embedding_config.get("similarity_threshold", 0.85)

        # Load API keys from environment
        self.code_api_key = os.getenv("CODE_EMBEDDING_API_KEY")
        self.natural_api_key = os.getenv("NATURAL_EMBEDDING_API_KEY")

        # Local model fallback
        self.local_model_name = embedding_config.get("local_model", "google/embeddinggemma-300m")

    def _initialize_clients(self):
        """Initialize embedding clients."""
        self._local_model = None

        # Check if we can use specialized endpoints
        self._can_use_code_api = bool(self.code_api_key and self.use_specialized)
        self._can_use_natural_api = bool(self.natural_api_key and self.use_specialized)

        # Initialize local model as fallback
        if not (self._can_use_code_api and self._can_use_natural_api):
            self._initialize_local_model()

        logger.info("Embedding client initialized:")
        logger.info(f"  Code API: {'✓' if self._can_use_code_api else '✗'}")
        logger.info(f"  Natural API: {'✓' if self._can_use_natural_api else '✗'}")
        logger.info(f"  Local fallback: {'✓' if self._local_model else '✗'}")

    def _initialize_local_model(self):
        """Initialize local embedding model as fallback."""
        try:
            try:
                from sentence_transformers import SentenceTransformer

                self._local_model = SentenceTransformer(self.local_model_name)
                logger.info(f"Local embedding model loaded: {self.local_model_name}")
            except ImportError:
                logger.warning("sentence-transformers not available, remote endpoints required")
        except Exception as e:
            logger.warning(f"Failed to load local model {self.local_model_name}: {e}")

    def _call_remote_api(
        self, api_url: str, api_key: str, model: str, texts: List[str]
    ) -> List[List[float]]:
        """
        Call remote embedding API.

        Args:
            api_url: API endpoint URL
            api_key: API authentication key
            model: Model name to use
            texts: List of texts to embed

        Returns:
            List of embedding vectors
        """
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}

        payload = {"input": texts, "model": model}

        try:
            response = requests.post(
                f"{api_url}/v1/embeddings", headers=headers, json=payload, timeout=30
            )
            response.raise_for_status()

            data = response.json()
            embeddings = [item["embedding"] for item in data["data"]]

            logger.debug(f"Generated {len(embeddings)} embeddings via {api_url}")
            return embeddings

        except Exception as e:
            logger.warning(f"Remote embedding API failed ({api_url}): {e}")
            raise

    def _get_local_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        Get embeddings using local model.

        Args:
            texts: List of texts to embed

        Returns:
            List of embedding vectors
        """
        if not self._local_model:
            raise RuntimeError("Local embedding model not available")

        embeddings = self._local_model.encode(texts, convert_to_tensor=False)
        return embeddings.tolist()

    def get_code_embeddings(self, code_texts: List[str]) -> List[List[float]]:
        """
        Get embeddings optimized for code analysis.

        Args:
            code_texts: List of code snippets to embed

        Returns:
            List of embedding vectors optimized for code similarity
        """
        if self._can_use_code_api:
            try:
                return self._call_remote_api(
                    self.code_api_url, self.code_api_key, self.code_model, code_texts
                )
            except Exception as e:
                logger.warning(f"Code API failed, falling back to local: {e}")

        # Fallback to local model
        return self._get_local_embeddings(code_texts)

    def get_natural_embeddings(self, natural_texts: List[str]) -> List[List[float]]:
        """
        Get embeddings optimized for natural language analysis.

        Args:
            natural_texts: List of natural language texts to embed

        Returns:
            List of embedding vectors optimized for natural language understanding
        """
        if self._can_use_natural_api:
            try:
                return self._call_remote_api(
                    self.natural_api_url, self.natural_api_key, self.natural_model, natural_texts
                )
            except Exception as e:
                logger.warning(f"Natural API failed, falling back to local: {e}")

        # Fallback to local model
        return self._get_local_embeddings(natural_texts)

    def get_embeddings(self, texts: List[str], content_type: str = "mixed") -> List[List[float]]:
        """
        Get embeddings with automatic routing based on content type.

        Args:
            texts: List of texts to embed
            content_type: "code", "natural", or "mixed"

        Returns:
            List of embedding vectors
        """
        if content_type == "code":
            return self.get_code_embeddings(texts)
        elif content_type == "natural":
            return self.get_natural_embeddings(texts)
        else:
            # For mixed content, use natural language embeddings as default
            return self.get_natural_embeddings(texts)

    def compute_similarity(self, embedding1: List[float], embedding2: List[float]) -> float:
        """
        Compute cosine similarity between two embeddings.

        Args:
            embedding1: First embedding vector
            embedding2: Second embedding vector

        Returns:
            Cosine similarity score (0-1)
        """
        import numpy as np

        # Convert to numpy arrays
        vec1 = np.array(embedding1)
        vec2 = np.array(embedding2)

        # Compute cosine similarity
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)

        if norm1 == 0 or norm2 == 0:
            return 0.0

        similarity = dot_product / (norm1 * norm2)
        return float(similarity)

    def find_similar_pairs(
        self, texts: List[str], content_type: str = "mixed", threshold: Optional[float] = None
    ) -> List[Dict[str, Any]]:
        """
        Find pairs of texts that exceed similarity threshold.

        Args:
            texts: List of texts to analyze
            content_type: Type of content for optimal embedding selection
            threshold: Similarity threshold (uses config default if None)

        Returns:
            List of similar pairs with metadata
        """
        if threshold is None:
            threshold = self.similarity_threshold

        embeddings = self.get_embeddings(texts, content_type)
        similar_pairs = []

        for i in range(len(texts)):
            for j in range(i + 1, len(texts)):
                similarity = self.compute_similarity(embeddings[i], embeddings[j])

                if similarity >= threshold:
                    similar_pairs.append(
                        {
                            "index1": i,
                            "index2": j,
                            "text1": texts[i],
                            "text2": texts[j],
                            "similarity": similarity,
                            "content_type": content_type,
                        }
                    )

        # Sort by similarity descending
        similar_pairs.sort(key=lambda x: x["similarity"], reverse=True)
        return similar_pairs
```

---
### File: src/vibelint/fix.py

```python
"""
Automatic fix functionality for vibelint using deterministic fixes and LLM for docstring generation only.

vibelint/src/vibelint/fix.py
"""

import ast
import logging
from pathlib import Path
from typing import Any, Dict, List, Optional

from vibelint.config import Config
from vibelint.plugin_system import Finding

__all__ = ["FixEngine", "can_fix_finding", "apply_fixes", "regenerate_all_docstrings"]

logger = logging.getLogger(__name__)


class FixEngine:
    """Engine for automatically fixing vibelint issues with deterministic code changes."""

    def __init__(self, config: Config):
        """Initialize fix engine with configuration.

        vibelint/src/vibelint/fix.py
        """
        self.config = config

        # Initialize LLM manager for dual LLM support
        from vibelint.llm import create_llm_manager

        config_dict = config.settings if isinstance(config.settings, dict) else {}
        self.llm_manager = create_llm_manager(config_dict)

    def can_fix_finding(self, finding: Finding) -> bool:
        """Check if a finding can be automatically fixed.

        vibelint/src/vibelint/fix.py
        """
        fixable_rules = {
            "DOCSTRING-MISSING",
            "DOCSTRING-PATH-REFERENCE",
            "EXPORTS-MISSING-ALL",
        }
        return finding.rule_id in fixable_rules

    async def fix_file(self, file_path: Path, findings: list[Finding]) -> bool:
        """Fix all fixable issues in a file deterministically.

        Returns True if any fixes were applied.

        vibelint/src/vibelint/fix.py
        """
        fixable_findings = [f for f in findings if self.can_fix_finding(f)]
        if not fixable_findings:
            return False

        logger.info(f"Fixing {len(fixable_findings)} issues in {file_path}")

        # Read current file content
        try:
            original_content = file_path.read_text(encoding="utf-8")
        except Exception as e:
            logger.error(f"Could not read {file_path}: {e}")
            return False

        # Apply deterministic fixes
        fixed_content = await self._apply_deterministic_fixes(
            file_path, original_content, fixable_findings
        )

        if fixed_content and fixed_content != original_content:
            try:
                # Write fixed content back to file
                file_path.write_text(fixed_content, encoding="utf-8")
                logger.info(f"Applied fixes to {file_path}")
                return True
            except Exception as e:
                logger.error(f"Could not write fixes to {file_path}: {e}")
                return False

        return False

    async def _apply_deterministic_fixes(
        self, file_path: Path, content: str, findings: list[Finding]
    ) -> str:
        """Apply deterministic fixes without LLM file rewriting.

        vibelint/src/vibelint/fix.py
        """
        try:
            # Parse the AST to understand the code structure
            tree = ast.parse(content)
        except SyntaxError as e:
            logger.error(f"Cannot parse {file_path}: {e}")
            return content

        # Track modifications by line number
        lines = content.splitlines()
        modifications = {}

        # Group findings by type for efficient processing
        findings_by_type = {}
        for finding in findings:
            rule_id = finding.rule_id
            if rule_id not in findings_by_type:
                findings_by_type[rule_id] = []
            findings_by_type[rule_id].append(finding)

        # Apply fixes by type
        if "DOCSTRING-MISSING" in findings_by_type:
            await self._fix_missing_docstrings(
                tree, lines, modifications, findings_by_type["DOCSTRING-MISSING"], file_path
            )

        if "DOCSTRING-PATH-REFERENCE" in findings_by_type:
            self._fix_docstring_path_references(
                lines, modifications, findings_by_type["DOCSTRING-PATH-REFERENCE"], file_path
            )

        if "EXPORTS-MISSING-ALL" in findings_by_type:
            self._fix_missing_exports(
                tree, lines, modifications, findings_by_type["EXPORTS-MISSING-ALL"]
            )

        # Apply all modifications to create fixed content
        return self._apply_modifications(lines, modifications)

    async def _fix_missing_docstrings(
        self,
        tree: ast.AST,
        lines: List[str],
        modifications: Dict[int, str],
        findings: List[Finding],
        file_path: Path,
    ) -> None:
        """Add missing docstrings using LLM for content generation only."""

        # Find functions and classes that need docstrings
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
                # Check if this node needs a docstring based on findings
                node_line = node.lineno
                needs_docstring = any(
                    abs(f.line - node_line) <= 2 for f in findings  # Allow some line tolerance
                )

                if needs_docstring and not ast.get_docstring(node):
                    # Generate docstring content using LLM (safe - only returns text)
                    docstring_content = await self._generate_docstring_content(node, file_path)

                    if docstring_content:
                        # Deterministically insert the docstring
                        indent = self._get_indent_for_line(lines, node.lineno)
                        docstring_line = (
                            f'{indent}"""{docstring_content}\n\n{indent}{file_path}\n{indent}"""'
                        )

                        # Insert after the function/class definition line
                        insert_line = node.lineno  # Insert after the def line
                        modifications[insert_line] = docstring_line

    async def _generate_docstring_content(self, node: ast.AST, file_path: Path) -> Optional[str]:
        """Generate only docstring text content using dual LLM system (safe operation)."""
        if not self.llm_manager:
            logger.debug("No LLM manager configured, skipping docstring generation")
            return None

        try:
            from vibelint.llm import LLMRequest

            # Safe prompt - only asks for docstring text, never code
            if isinstance(node, ast.ClassDef):
                prompt = f"Write a brief docstring for a Python class named '{node.name}'. Return only the docstring text without quotes or formatting."
            elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                args = [arg.arg for arg in node.args.args] if hasattr(node, "args") else []
                prompt = f"Write a brief docstring for a Python function named '{node.name}' with parameters {args}. Return only the docstring text without quotes or formatting."
            else:
                logger.debug("Unknown node type for docstring generation")
                return None

            # Use fast LLM for quick docstring generation
            request = LLMRequest(
                content=prompt, task_type="docstring_generation", max_tokens=200, temperature=0.1
            )

            response = await self.llm_manager.process_request(request)

            if response["success"] and response["content"]:
                # Clean the response to ensure it's just text
                content = str(response["content"]).strip()
                # Remove any quotes or markdown that might have been added
                content = content.replace('"""', "").replace("'''", "").replace("`", "")
                return content[:200]  # Limit length

        except Exception as e:
            logger.warning(f"LLM docstring generation failed: {e}, using fallback")

        # Safe fallback
        if isinstance(node, ast.ClassDef):
            return f"{node.name} class implementation."
        elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            return f"{node.name} function implementation."
        return "Implementation."

    def _fix_docstring_path_references(
        self,
        lines: List[str],
        modifications: Dict[int, str],
        findings: List[Finding],
        file_path: Path,
    ) -> None:
        """Add path references to existing docstrings based on configuration."""
        # Get docstring configuration
        config_dict = self.config.settings if isinstance(self.config.settings, dict) else {}
        docstring_config = config_dict.get("docstring", {})
        require_path_references = docstring_config.get("require_path_references", False)

        # Skip fix if path references are not required
        if not require_path_references:
            return

        # Get path format configuration
        path_format = docstring_config.get("path_reference_format", "relative")
        expected_path = self._get_expected_path_for_fix(file_path, path_format)

        for finding in findings:
            line_idx = finding.line - 1  # Convert to 0-based index
            if 0 <= line_idx < len(lines):
                line = lines[line_idx]
                # If this is a docstring line, ensure it has path reference
                if '"""' in line or "'''" in line:
                    # Add path reference if not already present
                    if expected_path not in line:
                        # Modify the docstring to include path
                        indent = self._get_indent_for_line(lines, finding.line)
                        if line.strip().endswith('"""') or line.strip().endswith("'''"):
                            # Single line docstring - expand it
                            quote = '"""' if '"""' in line else "'''"
                            content = line.strip().replace(quote, "").strip()
                            new_docstring = f"{indent}{quote}{content}\n\n{indent}{expected_path}\n{indent}{quote}"
                            modifications[finding.line - 1] = new_docstring

    def _get_expected_path_for_fix(self, file_path: Path, path_format: str) -> str:
        """Get expected path reference for fix based on format configuration."""
        if path_format == "absolute":
            return str(file_path)
        elif path_format == "module_path":
            # Convert to Python module path (e.g., vibelint.validators.docstring)
            parts = file_path.parts
            if "src" in parts:
                src_idx = parts.index("src")
                module_parts = parts[src_idx + 1 :]
            else:
                module_parts = parts

            # Remove .py extension and convert to module path
            if module_parts and module_parts[-1].endswith(".py"):
                module_parts = module_parts[:-1] + (module_parts[-1][:-3],)

            return ".".join(module_parts)
        else:  # relative format (default)
            # Get relative path, removing project root and src/ prefix
            relative_path = str(file_path)
            try:
                # Try to find project root by looking for common markers
                current = file_path.parent
                while current.parent != current:
                    if any(
                        (current / marker).exists()
                        for marker in ["pyproject.toml", "setup.py", ".git"]
                    ):
                        relative_path = str(file_path.relative_to(current))
                        break
                    current = current.parent
            except ValueError:
                pass

            # Remove src/ prefix if present
            if relative_path.startswith("src/"):
                relative_path = relative_path[4:]

            return relative_path

    def _fix_missing_exports(
        self,
        tree: ast.AST,
        lines: List[str],
        modifications: Dict[int, str],
        findings: List[Finding],
    ) -> None:
        """Add missing __all__ exports."""
        # Find all public functions and classes
        public_names = []
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
                if not node.name.startswith("_"):  # Public items
                    public_names.append(node.name)

        if public_names:
            # Check if __all__ already exists
            has_all = any(
                isinstance(node, ast.Assign)
                and any(
                    isinstance(target, ast.Name) and target.id == "__all__"
                    for target in node.targets
                )
                for node in ast.walk(tree)
            )

            if not has_all:
                # Add __all__ at the top of the file after imports
                exports_line = f"__all__ = {public_names!r}"

                # Find a good place to insert (after imports)
                insert_line = 0
                for i, line in enumerate(lines):
                    if line.strip().startswith("import ") or line.strip().startswith("from "):
                        insert_line = i + 1
                    elif line.strip() and not line.strip().startswith("#"):
                        break

                modifications[insert_line] = exports_line

    def _get_indent_for_line(self, lines: List[str], line_number: int) -> str:
        """Get the indentation for a given line number."""
        if 1 <= line_number <= len(lines):
            line = lines[line_number - 1]
            return line[: len(line) - len(line.lstrip())]
        return "    "  # Default 4-space indent

    def _apply_modifications(self, lines: List[str], modifications: Dict[int, str]) -> str:
        """Apply all modifications to the lines and return the fixed content."""
        # Sort modifications by line number in reverse order to avoid index shifting
        sorted_modifications = sorted(modifications.items(), reverse=True)

        result_lines = lines[:]
        for line_num, new_content in sorted_modifications:
            if line_num < len(result_lines):
                result_lines[line_num] = new_content
            else:
                # Insert at end
                result_lines.append(new_content)

        return "\n".join(result_lines)


# Convenience functions for the CLI
async def apply_fixes(config: Config, file_findings: dict[Path, list[Finding]]) -> int:
    """Apply fixes to all files with fixable findings.

    vibelint/src/vibelint/fix.py
    """
    engine = FixEngine(config)
    fixed_count = 0

    for file_path, findings in file_findings.items():
        if await engine.fix_file(file_path, findings):
            fixed_count += 1

    return fixed_count


def can_fix_finding(finding: Finding) -> bool:
    """Check if a finding can be automatically fixed.

    vibelint/src/vibelint/fix.py
    """
    return finding.rule_id in {
        "DOCSTRING-MISSING",
        "DOCSTRING-PATH-REFERENCE",
        "EXPORTS-MISSING-ALL",
    }


async def regenerate_all_docstrings(config: Config, file_paths: List[Path]) -> int:
    """Regenerate ALL docstrings in the specified files using LLM.

    Unlike apply_fixes which only adds missing docstrings, this function
    regenerates existing docstrings as well for consistency and improved quality.

    Returns the number of files successfully processed.

    vibelint/src/vibelint/fix.py
    """
    engine = FixEngine(config)
    processed_count = 0

    if not engine.llm_config.get("api_base_url"):
        logger.error("No LLM API configured. Cannot regenerate docstrings.")
        return 0

    for file_path in file_paths:
        try:
            if await _regenerate_docstrings_in_file(engine, file_path):
                processed_count += 1
                logger.info(f"Regenerated docstrings in {file_path}")
            else:
                logger.debug(f"No docstrings to regenerate in {file_path}")
        except Exception as e:
            logger.error(f"Failed to regenerate docstrings in {file_path}: {e}")

    return processed_count


async def preview_docstring_changes(config: Config, file_paths: List[Path]) -> Dict[str, Any]:
    """Preview what docstring changes would be made without modifying files.

    Returns a dictionary containing:
    - files_analyzed: list of files that would be changed
    - total_changes: total number of docstring changes
    - preview_samples: dict of file -> list of preview changes

    vibelint/src/vibelint/fix.py
    """
    engine = FixEngine(config)
    preview_results = {
        "files_analyzed": [],
        "total_changes": 0,
        "preview_samples": {},
        "errors": [],
    }

    if not engine.llm_config.get("api_base_url"):
        preview_results["errors"].append("No LLM API configured. Cannot preview docstring changes.")
        return preview_results

    for file_path in file_paths:
        try:
            file_preview = await _preview_docstrings_in_file(engine, file_path)
            if file_preview["changes"]:
                preview_results["files_analyzed"].append(str(file_path))
                preview_results["total_changes"] += len(file_preview["changes"])
                preview_results["preview_samples"][str(file_path)] = file_preview["changes"]
        except Exception as e:
            preview_results["errors"].append(f"Failed to preview {file_path}: {e}")

    return preview_results


async def _preview_docstrings_in_file(engine: FixEngine, file_path: Path) -> Dict[str, Any]:
    """Preview docstring changes for a single file without modifying it.

    Returns dict with 'changes' list containing preview information.
    """
    file_preview = {"changes": []}

    try:
        content = file_path.read_text(encoding="utf-8")
        lines = content.splitlines()

        # Parse the file to find all functions and classes
        tree = ast.parse(content)

        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
                change_preview = await _preview_node_docstring(engine, node, lines, file_path)
                if change_preview:
                    file_preview["changes"].append(change_preview)

    except (OSError, UnicodeDecodeError, SyntaxError) as e:
        logger.error(f"Error previewing {file_path}: {e}")

    return file_preview


async def _preview_node_docstring(
    engine: FixEngine,
    node: ast.AST,
    lines: List[str],
    file_path: Path,
) -> Optional[Dict[str, Any]]:
    """Preview what docstring change would be made for a specific AST node.

    Returns preview dict with change information, or None if no change.
    """
    # SAFETY CHECK: Only process functions and classes that we can safely identify
    if not isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
        return None

    # Get current docstring if it exists
    current_docstring = ast.get_docstring(node)

    # Generate new docstring content (this calls the LLM)
    new_docstring_content = await engine._generate_docstring_content(node, file_path)
    if not new_docstring_content:
        return None

    # SAFETY VALIDATION: Check that generated content is reasonable
    if not _validate_docstring_content(new_docstring_content):
        return None

    # Determine what type of change this would be
    change_type = "add" if not current_docstring else "modify"
    node_type = "function" if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)) else "class"

    return {
        "node_name": node.name,
        "node_type": node_type,
        "line_number": node.lineno,
        "change_type": change_type,
        "current_docstring": (
            current_docstring[:100] + "..."
            if current_docstring and len(current_docstring) > 100
            else current_docstring
        ),
        "new_docstring": (
            new_docstring_content[:100] + "..."
            if len(new_docstring_content) > 100
            else new_docstring_content
        ),
    }


async def _regenerate_docstrings_in_file(engine: FixEngine, file_path: Path) -> bool:
    """Regenerate all docstrings in a single file.

    Returns True if any docstrings were regenerated.

    vibelint/src/vibelint/fix.py
    """
    try:
        content = file_path.read_text(encoding="utf-8")
        lines = content.splitlines()

        # Parse the file to find all functions and classes
        tree = ast.parse(content)
        modifications = {}

        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
                await _regenerate_node_docstring(engine, node, lines, modifications, file_path)

        if modifications:
            # Apply modifications
            result_content = _apply_line_modifications(lines, modifications)
            file_path.write_text(result_content, encoding="utf-8")
            return True

        return False

    except (OSError, UnicodeDecodeError, SyntaxError) as e:
        logger.error(f"Error processing {file_path}: {e}")
        return False


async def _regenerate_node_docstring(
    engine: FixEngine,
    node: ast.AST,
    lines: List[str],
    modifications: Dict[int, str],
    file_path: Path,
) -> None:
    """Regenerate docstring for a specific AST node with strict safety validation.

    SAFETY CRITICAL: This function must NEVER modify any Python code, only docstring content.

    vibelint/src/vibelint/fix.py
    """
    # SAFETY CHECK: Only process functions and classes that we can safely identify
    if not isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
        logger.warning(f"Skipping unsafe node type {type(node)} for safety")
        return

    # Get current docstring if it exists
    current_docstring = ast.get_docstring(node)

    # SAFETY CHECK: Validate LLM-generated content before using it
    new_docstring_content = await engine._generate_docstring_content(node, file_path)
    if not new_docstring_content:
        return

    # SAFETY VALIDATION: Check that generated content is reasonable
    if not _validate_docstring_content(new_docstring_content):
        logger.warning(f"Generated docstring failed safety validation for {node.name}")
        return

    # Determine indentation safely
    node_line = node.lineno - 1
    if node_line < len(lines):
        indent = len(lines[node_line]) - len(lines[node_line].lstrip())
        indent_str = " " * (indent + 4)  # Add 4 spaces for function/class body
    else:
        indent_str = "    "  # Default indentation

    # Format the new docstring with path reference and safety warning
    new_docstring = (
        f'{indent_str}"""{new_docstring_content}\n\n'
        f"{indent_str}[WARNING]  CRITICAL WARNING: This docstring was auto-generated by LLM and MUST be reviewed.\n"
        f"{indent_str}Inaccurate documentation can cause security vulnerabilities, system failures,\n"
        f"{indent_str}and data corruption. Verify all parameters, return types, and behavior descriptions.\n\n"
        f'{indent_str}{file_path}\n{indent_str}"""'
    )

    if current_docstring:
        # ULTRA SAFE: Find docstring using multiple validation methods
        docstring_lines = _find_docstring_lines_safely(node, lines)

        if docstring_lines:
            # SAFETY CHECK: Verify we're only modifying docstring lines
            for line_num in docstring_lines:
                if line_num < len(lines):
                    line_content = lines[line_num]
                    # CRITICAL: Only modify lines that are clearly part of docstring
                    if not _is_safe_docstring_line(line_content):
                        logger.error(
                            f"SAFETY VIOLATION: Attempted to modify non-docstring line {line_num}: {line_content}"
                        )
                        return  # Abort entire operation for safety

            # Safe to proceed - modify only the first docstring line, remove others
            for i, line_num in enumerate(docstring_lines):
                if i == 0:
                    modifications[line_num] = new_docstring
                else:
                    modifications[line_num] = ""  # Remove continuation lines
        else:
            logger.warning(f"Could not safely locate docstring for {node.name}")
    else:
        # Add new docstring after function/class definition
        insert_line = node.lineno  # Line after def/class
        modifications[insert_line] = new_docstring


def _validate_docstring_content(content: str) -> bool:
    """Validate that LLM-generated docstring content is safe and reasonable.

    vibelint/src/vibelint/fix.py
    """
    if not content or not isinstance(content, str):
        return False

    # Check for dangerous content
    dangerous_patterns = [
        "import ",
        "exec(",
        "eval(",
        "__import__",
        "subprocess",
        "os.system",
        "shell=True",
        "DELETE",
        "DROP TABLE",
        "rm -rf",
    ]

    content_lower = content.lower()
    for pattern in dangerous_patterns:
        if pattern.lower() in content_lower:
            logger.error(f"SAFETY: Dangerous pattern '{pattern}' found in generated docstring")
            return False

    # Check reasonable length (docstrings shouldn't be huge)
    if len(content) > 2000:
        logger.warning("Generated docstring is suspiciously long")
        return False

    return True


def _find_docstring_lines_safely(node: ast.AST, lines: List[str]) -> List[int]:
    """Safely find the exact line numbers of a docstring using multiple validation methods.

    vibelint/src/vibelint/fix.py
    """
    # Method 1: Use AST to find docstring node
    docstring_node = None
    for child in ast.iter_child_nodes(node):
        if isinstance(child, ast.Expr) and isinstance(child.value, ast.Constant):
            if isinstance(child.value.value, str):
                docstring_node = child
                break

    if not docstring_node:
        return []

    # Get line range from AST
    start_line = docstring_node.lineno - 1  # Convert to 0-based
    end_line = docstring_node.end_lineno - 1 if docstring_node.end_lineno else start_line

    # Method 2: Verify by examining actual line content
    docstring_lines = []
    for line_num in range(start_line, end_line + 1):
        if line_num < len(lines):
            if _is_safe_docstring_line(lines[line_num]):
                docstring_lines.append(line_num)
            else:
                # If any line in the range is not a docstring line, abort for safety
                logger.error(f"SAFETY: Line {line_num} in docstring range is not a docstring line")
                return []

    return docstring_lines


def _is_safe_docstring_line(line: str) -> bool:
    """Check if a line is definitely part of a docstring and safe to modify.

    vibelint/src/vibelint/fix.py
    """
    stripped = line.strip()

    # Must contain quotes or be empty/whitespace (for multi-line docstrings)
    if not stripped:
        return True  # Empty line within docstring

    # Must contain docstring quotes
    if '"""' in stripped or "'''" in stripped:
        return True

    # If it doesn't start with quotes, it might be docstring content
    # But we need to be very careful - check it doesn't look like code
    if any(
        pattern in stripped
        for pattern in ["def ", "class ", "import ", "=", "return ", "if ", "for ", "while "]
    ):
        return False

    # If it's indented and looks like text, probably docstring content
    if line.startswith("    ") and not stripped.startswith(("#", "//")):
        return True

    # Default to false for safety
    return False


def _apply_line_modifications(lines: List[str], modifications: Dict[int, str]) -> str:
    """Apply line modifications to content deterministically.

    vibelint/src/vibelint/fix.py
    """
    result_lines = lines[:]

    # Sort modifications by line number in reverse order to avoid index shifting
    sorted_modifications = sorted(modifications.items(), reverse=True)

    for line_num, new_content in sorted_modifications:
        if line_num < len(result_lines):
            if new_content == "":
                # Remove line
                result_lines.pop(line_num)
            else:
                result_lines[line_num] = new_content
        elif new_content:
            # Insert at end
            result_lines.append(new_content)

    return "\n".join(result_lines)
```

---
### File: src/vibelint/justification.py

```python
"""
Justification Engine for vibelint.

Core justification workflow that uses static analysis and minimal LLM calls
to justify code decisions and identify redundancies.

This is the foundation of vibelint's software quality approach.
"""

import ast
import json
import logging
import time
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Set

from vibelint.llm import LLMRequest, LLMRole, create_llm_manager

logger = logging.getLogger(__name__)

__all__ = ["JustificationEngine", "JustificationResult", "CodeJustification"]


@dataclass
class CodeJustification:
    """A single justification for a piece of code."""

    file_path: str
    element_type: str  # 'file', 'method', 'class'
    element_name: str
    line_number: int
    justification: str
    analysis_method: str  # 'static_analysis', 'llm_fast', 'llm_orchestrator'
    has_documentation: bool
    dependencies: List[str]
    complexity_score: int


@dataclass
class JustificationResult:
    """Result of justification analysis."""

    file_path: str
    justifications: List[CodeJustification]
    redundancies_found: List[str]
    recommendations: List[str]
    analysis_summary: Dict[str, Any]  # Summary of analysis methods used


class JustificationEngine:
    """
    Core justification engine using static analysis and minimal LLM usage.

    Philosophy:
    1. Static analysis for structure and dependencies
    2. Fast LLM for simple yes/no decisions on method similarity
    3. Orchestrator LLM for complex cross-file analysis only when needed
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.llm_manager = create_llm_manager(config or {})

        # Cache for static analysis results
        self._import_cache: Dict[str, Set[str]] = {}
        self._method_cache: Dict[str, List[Dict[str, Any]]] = {}

        # Logging infrastructure
        self._session_id = f"justification_{int(time.time())}"
        self._llm_call_logs: List[Dict[str, Any]] = []
        self._create_log_directory()

    def justify_file(self, file_path: Path, content: str) -> JustificationResult:
        """Justify a single file's existence and structure."""

        if not file_path.suffix == ".py":
            return self._justify_non_python_file(file_path)

        try:
            tree = ast.parse(content)
        except SyntaxError as e:
            logger.warning(f"Syntax error in {file_path}: {e}")
            return JustificationResult(
                file_path=str(file_path),
                justifications=[],
                redundancies_found=[],
                recommendations=[f"Fix syntax error: {e}"],
                quality_score=0.0
            )

        # Static analysis first
        imports = self._extract_imports(tree)
        methods = self._extract_methods(tree)
        classes = self._extract_classes(tree)

        # Cache results
        self._import_cache[str(file_path)] = imports
        self._method_cache[str(file_path)] = methods

        # Generate justifications
        justifications = []

        # File-level justification
        file_justification = self._analyze_file_purpose(file_path, tree, content)
        justifications.append(file_justification)

        # Method-level justifications
        for method_info in methods:
            if method_info["complexity"] >= 2:  # Only justify non-trivial methods
                method_justification = self._analyze_method(file_path, method_info, tree)
                justifications.append(method_justification)

        # Class-level justifications
        for class_info in classes:
            class_justification = self._analyze_class(file_path, class_info, tree)
            justifications.append(class_justification)

        # Find redundancies (static analysis only)
        redundancies = self._find_local_redundancies(methods, classes)

        # Generate recommendations
        recommendations = self._generate_recommendations(justifications, redundancies)

        # Create analysis summary
        analysis_summary = {
            "total_elements": len(justifications),
            "documented_elements": len([j for j in justifications if j.has_documentation]),
            "static_analysis_count": len([j for j in justifications if j.analysis_method == "static_analysis"]),
            "llm_analysis_count": len([j for j in justifications if j.analysis_method.startswith("llm_")]),
            "avg_complexity": sum(j.complexity_score for j in justifications) / len(justifications) if justifications else 0,
            "file_size_chars": len(content)
        }

        return JustificationResult(
            file_path=str(file_path),
            justifications=justifications,
            redundancies_found=redundancies,
            recommendations=recommendations,
            analysis_summary=analysis_summary
        )

    def justify_code_similarity(self, code1: str, code2: str,
                                element1_name: str, element2_name: str) -> Dict[str, Any]:
        """Get real similarity score using code embeddings."""

        try:
            # Try to use embedding analysis if available
            from vibelint.embedding_client import EmbeddingClient

            embedding_client = EmbeddingClient()

            # Get embeddings for both code snippets
            embedding1 = embedding_client.get_embedding(code1)
            embedding2 = embedding_client.get_embedding(code2)

            # Calculate cosine similarity
            import numpy as np

            # Normalize vectors
            norm1 = np.linalg.norm(embedding1)
            norm2 = np.linalg.norm(embedding2)

            if norm1 == 0 or norm2 == 0:
                similarity_score = 0.0
            else:
                similarity_score = np.dot(embedding1, embedding2) / (norm1 * norm2)

            # Log the similarity analysis
            self._log_llm_call({
                "operation": "code_similarity",
                "elements": [element1_name, element2_name],
                "method": "embedding_cosine_similarity",
                "similarity_score": float(similarity_score),
                "code1_length": len(code1),
                "code2_length": len(code2),
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            })

            return {
                "similar": similarity_score > 0.8,  # Threshold for similarity
                "similarity_score": float(similarity_score),
                "method": "embedding_analysis",
                "reasoning": f"Cosine similarity: {similarity_score:.3f}"
            }

        except ImportError:
            logger.warning("Embedding client not available, falling back to LLM comparison")
            return self._llm_method_comparison(method1_path, method1_name, method2_path, method2_name)
        except Exception as e:
            logger.error(f"Embedding similarity failed: {e}, falling back to LLM")
            return self._llm_method_comparison(method1_path, method1_name, method2_path, method2_name)

    def _llm_method_comparison(self, method1_path: str, method1_name: str,
                              method2_path: str, method2_name: str) -> Dict[str, Any]:
        """Use fast LLM to compare two methods for similarity (yes/no decision)."""

        if not self.llm_manager or not self.llm_manager.is_llm_available(LLMRole.FAST):
            logger.warning("Fast LLM not available for method comparison")
            return {"similar": False, "confidence": 0.0, "reasoning": "LLM unavailable"}

        # Get method content from cache
        method1_info = self._get_method_from_cache(method1_path, method1_name)
        method2_info = self._get_method_from_cache(method2_path, method2_name)

        if not method1_info or not method2_info:
            return {"similar": False, "confidence": 0.0, "reasoning": "Method not found"}

        # Create focused prompt for fast LLM (750 token limit)
        prompt = f"""Compare these two methods for functional similarity:

Method 1: {method1_name}
{method1_info.get('signature', 'Unknown signature')}
Purpose: {method1_info.get('purpose', 'No docstring')}

Method 2: {method2_name}
{method2_info.get('signature', 'Unknown signature')}
Purpose: {method2_info.get('purpose', 'No docstring')}

Answer: {{"similar": true/false, "confidence": 0.0-1.0, "reason": "brief explanation"}}"""

        request = LLMRequest(
            content=prompt,
            task_type="method_comparison",
            max_tokens=100,  # Short response
            temperature=0.1   # Deterministic
        )

        try:
            start_time = time.time()
            response = self.llm_manager.process_request(request)
            duration = time.time() - start_time

            # Log the LLM interaction
            self._log_llm_call({
                "operation": "method_comparison",
                "methods": [f"{method1_path}:{method1_name}", f"{method2_path}:{method2_name}"],
                "prompt": prompt,
                "response_raw": response.content,
                "llm_role": "fast",
                "duration_seconds": duration,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            })

            # Parse structured JSON response
            result = json.loads(response.content)
            return result
        except Exception as e:
            logger.error(f"Fast LLM method comparison failed: {e}")
            # Log the failure
            self._log_llm_call({
                "operation": "method_comparison",
                "methods": [f"{method1_path}:{method1_name}", f"{method2_path}:{method2_name}"],
                "prompt": prompt,
                "error": str(e),
                "llm_role": "fast",
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            })
            return {"similar": False, "confidence": 0.0, "reasoning": f"Error: {e}"}

    def _justify_non_python_file(self, file_path: Path) -> JustificationResult:
        """Justify non-Python files using file system analysis."""

        purpose = self._infer_file_purpose_from_path(file_path)

        justification = CodeJustification(
            file_path=str(file_path),
            element_type="file",
            element_name=file_path.name,
            line_number=1,
            justification=purpose,
            analysis_method="static_analysis",
            has_documentation=False,  # Non-Python files don't have docstrings
            dependencies=[],
            complexity_score=1
        )

        return JustificationResult(
            file_path=str(file_path),
            justifications=[justification],
            redundancies_found=[],
            recommendations=[],
            quality_score=0.8
        )

    def _extract_imports(self, tree: ast.AST) -> Set[str]:
        """Extract all imports from AST (static analysis)."""
        imports = set()

        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name)
            elif isinstance(node, ast.ImportFrom) and node.module:
                imports.add(node.module)

        return imports

    def _extract_methods(self, tree: ast.AST) -> List[Dict[str, Any]]:
        """Extract method information from AST (static analysis)."""
        methods = []

        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                method_info = {
                    "name": node.name,
                    "line_number": node.lineno,
                    "signature": self._extract_signature(node),
                    "purpose": ast.get_docstring(node) or "No docstring",
                    "complexity": self._calculate_complexity(node),
                    "is_private": node.name.startswith("_"),
                    "dependencies": self._extract_method_dependencies(node)
                }
                methods.append(method_info)

        return methods

    def _extract_classes(self, tree: ast.AST) -> List[Dict[str, Any]]:
        """Extract class information from AST (static analysis)."""
        classes = []

        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                class_info = {
                    "name": node.name,
                    "line_number": node.lineno,
                    "purpose": ast.get_docstring(node) or "No docstring",
                    "method_count": len([n for n in node.body if isinstance(n, ast.FunctionDef)]),
                    "dependencies": self._extract_class_dependencies(node)
                }
                classes.append(class_info)

        return classes

    def _justify_file_purpose(self, file_path: Path, tree: ast.AST, content: str) -> CodeJustification:
        """Justify file's existence using static analysis."""

        module_doc = ast.get_docstring(tree)

        # Analyze structure
        functions = [n for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]
        classes = [n for n in ast.walk(tree) if isinstance(n, ast.ClassDef)]

        # Infer purpose from filename and structure
        purpose = self._infer_file_purpose(file_path, functions, classes, module_doc)

        complexity = len(functions) + len(classes) * 2
        confidence = 0.9 if module_doc else 0.6

        return CodeJustification(
            file_path=str(file_path),
            element_type="file",
            element_name=file_path.name,
            line_number=1,
            justification=purpose,
            confidence=confidence,
            dependencies=list(self._import_cache.get(str(file_path), set())),
            complexity_score=complexity
        )

    def _justify_method(self, file_path: Path, method_info: Dict[str, Any], tree: ast.AST) -> CodeJustification:
        """Justify method's existence using static analysis."""

        purpose = method_info["purpose"]
        if purpose == "No docstring":
            # Infer from method name and structure
            purpose = f"Method {method_info['name']} - inferred from implementation"

        return CodeJustification(
            file_path=str(file_path),
            element_type="method",
            element_name=method_info["name"],
            line_number=method_info["line_number"],
            justification=purpose,
            confidence=0.8 if method_info["purpose"] != "No docstring" else 0.4,
            dependencies=method_info["dependencies"],
            complexity_score=method_info["complexity"]
        )

    def _justify_class(self, file_path: Path, class_info: Dict[str, Any], tree: ast.AST) -> CodeJustification:
        """Justify class's existence using static analysis."""

        purpose = class_info["purpose"]
        if purpose == "No docstring":
            purpose = f"Class {class_info['name']} with {class_info['method_count']} methods"

        return CodeJustification(
            file_path=str(file_path),
            element_type="class",
            element_name=class_info["name"],
            line_number=class_info["line_number"],
            justification=purpose,
            confidence=0.8 if class_info["purpose"] != "No docstring" else 0.5,
            dependencies=class_info["dependencies"],
            complexity_score=class_info["method_count"]
        )

    def _find_local_redundancies(self, methods: List[Dict[str, Any]],
                                classes: List[Dict[str, Any]]) -> List[str]:
        """Find potential redundancies within a single file (static analysis)."""
        redundancies = []

        # Check for similar method names
        method_names = [m["name"] for m in methods]
        for i, name1 in enumerate(method_names):
            for j, name2 in enumerate(method_names[i+1:], i+1):
                if self._names_similar(name1, name2):
                    redundancies.append(f"Similar method names: {name1}, {name2}")

        # Check for classes with single methods (possible over-engineering)
        for class_info in classes:
            if class_info["method_count"] == 1:
                redundancies.append(f"Single-method class: {class_info['name']}")

        return redundancies

    def _generate_recommendations(self, justifications: List[CodeJustification],
                                 redundancies: List[str]) -> List[str]:
        """Generate actionable recommendations."""
        recommendations = []

        # Check documentation coverage
        undocumented = [j for j in justifications if j.confidence < 0.5]
        if undocumented:
            recommendations.append(f"Add documentation to {len(undocumented)} elements")

        # Check complexity
        complex_elements = [j for j in justifications if j.complexity_score > 10]
        if complex_elements:
            recommendations.append(f"Consider breaking down {len(complex_elements)} complex elements")

        # Add redundancy recommendations
        if redundancies:
            recommendations.append("Review potential redundancies for consolidation")

        return recommendations

    def _calculate_quality_score(self, justifications: List[CodeJustification]) -> float:
        """Calculate overall quality score for the file."""
        if not justifications:
            return 0.0

        # Average confidence weighted by complexity
        total_weight = 0
        weighted_confidence = 0

        for just in justifications:
            weight = max(just.complexity_score, 1)
            weighted_confidence += just.confidence * weight
            total_weight += weight

        return weighted_confidence / total_weight if total_weight > 0 else 0.0

    # Helper methods for static analysis
    def _extract_signature(self, node: ast.FunctionDef) -> str:
        """Extract method signature."""
        args = [arg.arg for arg in node.args.args]
        return f"{node.name}({', '.join(args)})"

    def _calculate_complexity(self, node: ast.FunctionDef) -> int:
        """Calculate cyclomatic complexity."""
        complexity = 1
        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.ExceptHandler)):
                complexity += 1
        return complexity

    def _extract_method_dependencies(self, node: ast.FunctionDef) -> List[str]:
        """Extract what the method depends on."""
        deps = []
        for child in ast.walk(node):
            if isinstance(child, ast.Call) and isinstance(child.func, ast.Name):
                deps.append(child.func.id)
        return list(set(deps))

    def _extract_class_dependencies(self, node: ast.ClassDef) -> List[str]:
        """Extract what the class depends on."""
        deps = []
        for base in node.bases:
            if isinstance(base, ast.Name):
                deps.append(base.id)
        return deps

    def _infer_file_purpose(self, file_path: Path, functions: List[ast.FunctionDef],
                           classes: List[ast.ClassDef], module_doc: Optional[str]) -> str:
        """Infer file purpose from static analysis."""

        if module_doc:
            return f"Documented module: {module_doc.split('.')[0]}"

        filename = file_path.stem

        if filename.endswith("_test") or filename.startswith("test_"):
            return "Test module"
        elif filename == "__init__":
            return "Package initialization"
        elif filename in ["main", "__main__"]:
            return "Application entry point"
        elif len(classes) > len(functions):
            return f"Class definitions module ({len(classes)} classes)"
        elif len(functions) > 0:
            return f"Function definitions module ({len(functions)} functions)"
        else:
            return "Configuration or data module"

    def _infer_file_purpose_from_path(self, file_path: Path) -> str:
        """Infer non-Python file purpose from path and extension."""

        if file_path.name in ["README.md", "CHANGELOG.md", "LICENSE"]:
            return "Project documentation"
        elif file_path.suffix in [".toml", ".yaml", ".yml", ".json"]:
            return "Configuration file"
        elif file_path.suffix in [".sh", ".bat"]:
            return "Automation script"
        else:
            return f"Support file ({file_path.suffix})"

    def _names_similar(self, name1: str, name2: str) -> bool:
        """Check if two names are suspiciously similar."""
        # Simple heuristic: similar if they share >80% of characters
        if len(name1) < 3 or len(name2) < 3:
            return False

        common_chars = len(set(name1.lower()) & set(name2.lower()))
        max_chars = max(len(set(name1.lower())), len(set(name2.lower())))

        return common_chars / max_chars > 0.8

    def _get_method_from_cache(self, file_path: str, method_name: str) -> Optional[Dict[str, Any]]:
        """Get method info from cache."""
        methods = self._method_cache.get(file_path, [])
        for method in methods:
            if method["name"] == method_name:
                return method
        return None

    def _create_log_directory(self):
        """Create logging directory for justification outputs."""
        # Look for existing .vibelint-reports directory
        current_path = Path.cwd()
        while current_path.parent != current_path:
            reports_dir = current_path / ".vibelint-reports"
            if reports_dir.exists():
                self.log_dir = reports_dir / "justification"
                break
            current_path = current_path.parent
        else:
            # Fallback: create in current directory
            self.log_dir = Path.cwd() / ".vibelint-reports" / "justification"

        self.log_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"Justification logs will be saved to: {self.log_dir}")

    def _log_llm_call(self, call_data: Dict[str, Any]):
        """Log an LLM call with full details."""
        self._llm_call_logs.append(call_data)

    def save_session_logs(self, file_path: str, result: JustificationResult):
        """Save comprehensive session logs including all LLM calls and final justification."""

        # Create session log with all details
        session_log = {
            "session_id": self._session_id,
            "file_analyzed": file_path,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "llm_calls": self._llm_call_logs,
            "final_justification": {
                "justifications": [asdict(j) for j in result.justifications],
                "redundancies_found": result.redundancies_found,
                "recommendations": result.recommendations,
                "quality_score": result.quality_score
            },
            "summary": {
                "total_llm_calls": len(self._llm_call_logs),
                "elements_analyzed": len(result.justifications),
                "static_analysis_only": len(self._llm_call_logs) == 0
            }
        }

        # Save detailed session log
        session_file = self.log_dir / f"{self._session_id}_detailed.json"
        with open(session_file, 'w', encoding='utf-8') as f:
            json.dump(session_log, f, indent=2, ensure_ascii=False)

        # Save human-readable summary
        summary_file = self.log_dir / f"{self._session_id}_summary.md"
        self._save_readable_summary(summary_file, session_log)

        logger.info(f"Justification session logs saved:")
        logger.info(f"  Detailed: {session_file}")
        logger.info(f"  Summary: {summary_file}")

        return session_file, summary_file

    def _save_readable_summary(self, summary_file: Path, session_log: Dict[str, Any]):
        """Save human-readable summary of justification analysis."""

        content = f"""# Justification Analysis Summary

**Session:** {session_log['session_id']}
**File:** {session_log['file_analyzed']}
**Timestamp:** {session_log['timestamp']}
**Quality Score:** {session_log['final_justification']['quality_score']:.1%}

## Analysis Summary

- **Elements Analyzed:** {session_log['summary']['elements_analyzed']}
- **LLM Calls Made:** {session_log['summary']['total_llm_calls']}
- **Analysis Type:** {'Static Analysis Only' if session_log['summary']['static_analysis_only'] else 'Static + LLM Analysis'}

## Code Justifications

"""

        for just_data in session_log['final_justification']['justifications']:
            confidence_emoji = "✅" if just_data['confidence'] > 0.7 else "⚠️" if just_data['confidence'] > 0.4 else "❌"
            content += f"""### {just_data['element_name']} ({just_data['element_type']})

{confidence_emoji} **Confidence:** {just_data['confidence']:.1%}
**Line:** {just_data['line_number']}
**Complexity:** {just_data['complexity_score']}

**Justification:** {just_data['justification']}

"""

        if session_log['final_justification']['redundancies_found']:
            content += "## ⚠️ Potential Redundancies\n\n"
            for redundancy in session_log['final_justification']['redundancies_found']:
                content += f"- {redundancy}\n"
            content += "\n"

        if session_log['final_justification']['recommendations']:
            content += "## 💡 Recommendations\n\n"
            for rec in session_log['final_justification']['recommendations']:
                content += f"- {rec}\n"
            content += "\n"

        if session_log['llm_calls']:
            content += "## 🤖 LLM Call Details\n\n"
            for i, call in enumerate(session_log['llm_calls'], 1):
                content += f"""### Call {i}: {call['operation']}

**LLM Role:** {call['llm_role']}
**Duration:** {call.get('duration_seconds', 'N/A')}s
**Timestamp:** {call['timestamp']}

**Prompt:**
```
{call['prompt'][:500]}{'...' if len(call['prompt']) > 500 else ''}
```

**Response:**
```
{call.get('response_raw', call.get('error', 'No response'))[:500]}{'...' if len(call.get('response_raw', '')) > 500 else ''}
```

---

"""

        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(content)
```

---
### File: src/vibelint/justification_v2.py

```python
"""
Honest Justification Engine for vibelint.

This engine provides factual analysis of code without making up confidence scores
or pretending to have capabilities it doesn't have. Uses real ML techniques
where available (embeddings) and is transparent about limitations.
"""

import ast
import json
import logging
import time
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Set

logger = logging.getLogger(__name__)

__all__ = ["JustificationEngineV2", "CodeAnalysis", "AnalysisResult"]


@dataclass
class CodeAnalysis:
    """Factual analysis of a code element."""

    file_path: str
    element_type: str  # 'file', 'method', 'class'
    element_name: str
    line_number: int
    description: str  # What we can factually observe
    analysis_method: str  # 'static_analysis', 'embedding_similarity', 'llm_comparison'
    has_documentation: bool
    dependencies: List[str]
    complexity_metrics: Dict[str, int]
    llm_reasoning: Optional[str] = None  # Only when LLM was actually used


@dataclass
class AnalysisResult:
    """Result of code analysis."""

    target_path: str
    analyses: List[CodeAnalysis]
    structural_issues: List[str]
    recommendations: List[str]
    analysis_summary: Dict[str, Any]
    llm_calls_made: List[Dict[str, Any]]  # Full log of any LLM interactions


class JustificationEngineV2:
    """
    Honest code analysis engine.

    Philosophy:
    1. Static analysis for structure, complexity, dependencies
    2. Embedding models for real similarity scores (when available)
    3. LLMs only for targeted comparison tasks (fast model for yes/no)
    4. No fake confidence scores or made-up metrics
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}

        # Try to initialize embedding client for real similarity
        self.embedding_client = None
        try:
            from vibelint.embedding_client import EmbeddingClient
            self.embedding_client = EmbeddingClient()
            logger.info("Embedding client available for similarity analysis")
        except ImportError:
            logger.info("Embedding client not available - similarity analysis limited")

        # Try to initialize LLM for targeted comparisons
        self.llm_manager = None
        try:
            from vibelint.llm import create_llm_manager
            self.llm_manager = create_llm_manager(config or {})
            if self.llm_manager:
                logger.info("LLM manager available for targeted comparisons")
        except ImportError:
            logger.info("LLM manager not available")

        # Logging infrastructure
        self._session_id = f"analysis_{int(time.time())}"
        self._llm_call_logs: List[Dict[str, Any]] = []
        self._create_log_directory()

    def analyze_file(self, file_path: Path, content: str) -> AnalysisResult:
        """Perform honest analysis of a single file."""

        if not file_path.suffix == ".py":
            return self._analyze_non_python_file(file_path)

        try:
            tree = ast.parse(content)
        except SyntaxError as e:
            logger.warning(f"Syntax error in {file_path}: {e}")
            return AnalysisResult(
                target_path=str(file_path),
                analyses=[],
                structural_issues=[f"Syntax error: {e}"],
                recommendations=["Fix syntax errors before analysis"],
                analysis_summary={"status": "syntax_error"},
                llm_calls_made=[]
            )

        analyses = []

        # Static analysis of structure
        imports = self._extract_imports(tree)
        methods = self._extract_methods(tree)
        classes = self._extract_classes(tree)

        # File-level analysis
        file_analysis = self._analyze_file_structure(file_path, tree, content, imports)
        analyses.append(file_analysis)

        # Method analyses
        for method_info in methods:
            method_analysis = self._analyze_method_structure(file_path, method_info)
            analyses.append(method_analysis)

        # Class analyses
        for class_info in classes:
            class_analysis = self._analyze_class_structure(file_path, class_info)
            analyses.append(class_analysis)

        # Find structural issues
        structural_issues = self._find_structural_issues(methods, classes, imports)

        # Generate factual recommendations
        recommendations = self._generate_recommendations(analyses, structural_issues)

        # Create analysis summary
        analysis_summary = {
            "total_elements": len(analyses),
            "documented_elements": len([a for a in analyses if a.has_documentation]),
            "methods_count": len(methods),
            "classes_count": len(classes),
            "imports_count": len(imports),
            "file_size_chars": len(content),
            "avg_complexity": sum(sum(a.complexity_metrics.values()) for a in analyses) / len(analyses) if analyses else 0,
            "analysis_capabilities": {
                "static_analysis": True,
                "embedding_similarity": self.embedding_client is not None,
                "llm_comparison": self.llm_manager is not None
            }
        }

        return AnalysisResult(
            target_path=str(file_path),
            analyses=analyses,
            structural_issues=structural_issues,
            recommendations=recommendations,
            analysis_summary=analysis_summary,
            llm_calls_made=self._llm_call_logs.copy()
        )

    def analyze_directory(self, directory_path: Path) -> Dict[str, AnalysisResult]:
        """Analyze all Python files in a directory with cross-file redundancy detection."""
        results = {}
        all_methods = []  # Collect all methods for similarity analysis

        python_files = list(directory_path.rglob("*.py"))
        logger.info(f"Analyzing {len(python_files)} Python files in {directory_path}")

        # First pass: analyze individual files
        for py_file in python_files:
            try:
                content = py_file.read_text(encoding='utf-8')
                result = self.analyze_file(py_file, content)
                results[str(py_file)] = result

                # Collect methods for cross-file analysis
                for analysis in result.analyses:
                    if analysis.element_type == "method":
                        all_methods.append({
                            "file_path": str(py_file),
                            "name": analysis.element_name,
                            "line": analysis.line_number,
                            "analysis": analysis
                        })

            except Exception as e:
                logger.error(f"Failed to analyze {py_file}: {e}")
                results[str(py_file)] = AnalysisResult(
                    target_path=str(py_file),
                    analyses=[],
                    structural_issues=[f"Analysis failed: {e}"],
                    recommendations=[],
                    analysis_summary={"status": "error"},
                    llm_calls_made=[]
                )

        # Second pass: cross-file redundancy detection
        if len(all_methods) > 1:
            logger.info(f"Running redundancy detection on {len(all_methods)} methods")
            redundancies = self._find_cross_file_redundancies(all_methods)

            # Add redundancy findings to each file's results
            for redundancy in redundancies:
                file1 = redundancy["method1"]["file_path"]
                file2 = redundancy["method2"]["file_path"]

                if file1 in results:
                    results[file1].structural_issues.append(
                        f"Potential redundancy: {redundancy['method1']['name']} similar to "
                        f"{redundancy['method2']['name']} in {Path(file2).name} "
                        f"(similarity: {redundancy['similarity_score']:.3f})"
                    )

        return results

    def calculate_code_similarity(self, code1: str, code2: str,
                                 name1: str, name2: str) -> Dict[str, Any]:
        """Calculate real similarity using embeddings if available."""

        if not self.embedding_client:
            return {
                "similarity_available": False,
                "reason": "No embedding model available"
            }

        try:
            # Get embeddings
            embedding1 = self.embedding_client.get_embedding(code1)
            embedding2 = self.embedding_client.get_embedding(code2)

            # Calculate cosine similarity
            import numpy as np

            norm1 = np.linalg.norm(embedding1)
            norm2 = np.linalg.norm(embedding2)

            if norm1 == 0 or norm2 == 0:
                similarity_score = 0.0
            else:
                similarity_score = np.dot(embedding1, embedding2) / (norm1 * norm2)

            # Log the analysis
            self._llm_call_logs.append({
                "operation": "embedding_similarity",
                "elements": [name1, name2],
                "similarity_score": float(similarity_score),
                "method": "cosine_similarity",
                "embedding_model": getattr(self.embedding_client, 'model_name', 'unknown'),
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            })

            return {
                "similarity_available": True,
                "similarity_score": float(similarity_score),
                "method": "embedding_cosine_similarity",
                "model_used": getattr(self.embedding_client, 'model_name', 'unknown'),
                "interpretation": "Range: -1 (opposite) to 1 (identical)"
            }

        except Exception as e:
            logger.error(f"Embedding similarity calculation failed: {e}")
            return {
                "similarity_available": False,
                "reason": f"Embedding calculation failed: {e}"
            }

    def _analyze_file_structure(self, file_path: Path, tree: ast.AST,
                               content: str, imports: Set[str]) -> CodeAnalysis:
        """Analyze file structure factually."""

        module_doc = ast.get_docstring(tree)
        functions = [n for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]
        classes = [n for n in ast.walk(tree) if isinstance(n, ast.ClassDef)]

        # Factual description based on what we observe
        if module_doc:
            description = f"Module with docstring. Contains {len(functions)} functions, {len(classes)} classes"
        else:
            description = f"Module without docstring. Contains {len(functions)} functions, {len(classes)} classes"

        return CodeAnalysis(
            file_path=str(file_path),
            element_type="file",
            element_name=file_path.name,
            line_number=1,
            description=description,
            analysis_method="static_analysis",
            has_documentation=module_doc is not None,
            dependencies=list(imports),
            complexity_metrics={
                "function_count": len(functions),
                "class_count": len(classes),
                "import_count": len(imports),
                "line_count": len(content.splitlines())
            }
        )

    def _analyze_method_structure(self, file_path: Path, method_info: Dict[str, Any]) -> CodeAnalysis:
        """Analyze method structure with multiple techniques."""

        has_doc = method_info["purpose"] != "No docstring"
        complexity = method_info["complexity"]

        # Enhanced description using multiple analysis methods
        llm_description = None
        analysis_method = "static_analysis"

        # Try to get LLM description for complex or undocumented methods
        if self.llm_manager and (complexity > 5 or not has_doc):
            llm_description = self._get_llm_method_description(method_info, file_path)
            if llm_description:
                analysis_method = "static_analysis+llm"

        # Build comprehensive description
        if llm_description:
            description = f"{llm_description} (complexity: {complexity})"
        elif has_doc:
            description = f"Method with docstring. Cyclomatic complexity: {complexity}"
        else:
            description = f"Method without docstring. Cyclomatic complexity: {complexity}"

        # Check for potential dead code
        usage_analysis = self._analyze_method_usage(method_info, file_path)

        return CodeAnalysis(
            file_path=str(file_path),
            element_type="method",
            element_name=method_info["name"],
            line_number=method_info["line_number"],
            description=description,
            analysis_method=analysis_method,
            has_documentation=has_doc,
            dependencies=method_info["dependencies"],
            complexity_metrics={
                "cyclomatic_complexity": complexity,
                "parameter_count": len(method_info.get("parameters", [])),
                "is_private": method_info["is_private"],
                "usage_score": usage_analysis["usage_score"],
                "potentially_dead": usage_analysis["potentially_dead"]
            },
            llm_reasoning=llm_description
        )

    def _analyze_class_structure(self, file_path: Path, class_info: Dict[str, Any]) -> CodeAnalysis:
        """Analyze class structure factually."""

        has_doc = class_info["purpose"] != "No docstring"
        method_count = class_info["method_count"]

        if has_doc:
            description = f"Class with docstring. Contains {method_count} methods"
        else:
            description = f"Class without docstring. Contains {method_count} methods"

        return CodeAnalysis(
            file_path=str(file_path),
            element_type="class",
            element_name=class_info["name"],
            line_number=class_info["line_number"],
            description=description,
            analysis_method="static_analysis",
            has_documentation=has_doc,
            dependencies=class_info["dependencies"],
            complexity_metrics={
                "method_count": method_count,
                "inheritance_depth": len(class_info["dependencies"])
            }
        )

    def _extract_imports(self, tree: ast.AST) -> Set[str]:
        """Extract all imports from AST."""
        imports = set()
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name)
            elif isinstance(node, ast.ImportFrom) and node.module:
                imports.add(node.module)
        return imports

    def _extract_methods(self, tree: ast.AST) -> List[Dict[str, Any]]:
        """Extract method information from AST."""
        methods = []
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                complexity = self._calculate_cyclomatic_complexity(node)
                method_info = {
                    "name": node.name,
                    "line_number": node.lineno,
                    "purpose": ast.get_docstring(node) or "No docstring",
                    "complexity": complexity,
                    "is_private": node.name.startswith("_"),
                    "dependencies": self._extract_method_dependencies(node),
                    "parameters": [arg.arg for arg in node.args.args]
                }
                methods.append(method_info)
        return methods

    def _extract_classes(self, tree: ast.AST) -> List[Dict[str, Any]]:
        """Extract class information from AST."""
        classes = []
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                class_info = {
                    "name": node.name,
                    "line_number": node.lineno,
                    "purpose": ast.get_docstring(node) or "No docstring",
                    "method_count": len([n for n in node.body if isinstance(n, ast.FunctionDef)]),
                    "dependencies": [base.id for base in node.bases if isinstance(base, ast.Name)]
                }
                classes.append(class_info)
        return classes

    def _calculate_cyclomatic_complexity(self, node: ast.FunctionDef) -> int:
        """Calculate actual cyclomatic complexity."""
        complexity = 1  # Base complexity
        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.ExceptHandler,
                                ast.With, ast.AsyncWith, ast.Assert)):
                complexity += 1
            elif isinstance(child, ast.BoolOp):
                complexity += len(child.values) - 1
        return complexity

    def _extract_method_dependencies(self, node: ast.FunctionDef) -> List[str]:
        """Extract function calls within method."""
        deps = []
        for child in ast.walk(node):
            if isinstance(child, ast.Call) and isinstance(child.func, ast.Name):
                deps.append(child.func.id)
        return list(set(deps))

    def _find_structural_issues(self, methods: List[Dict[str, Any]],
                               classes: List[Dict[str, Any]], imports: Set[str]) -> List[str]:
        """Find factual structural issues."""
        issues = []

        # High complexity methods
        high_complexity = [m for m in methods if m["complexity"] > 10]
        if high_complexity:
            issues.append(f"{len(high_complexity)} methods have high complexity (>10)")

        # Undocumented public methods
        undocumented_public = [m for m in methods
                              if not m["is_private"] and m["purpose"] == "No docstring"]
        if undocumented_public:
            issues.append(f"{len(undocumented_public)} public methods lack documentation")

        # Classes with many methods
        large_classes = [c for c in classes if c["method_count"] > 20]
        if large_classes:
            issues.append(f"{len(large_classes)} classes have >20 methods")

        # Too many imports
        if len(imports) > 20:
            issues.append(f"High import count: {len(imports)} imports")

        return issues

    def _generate_recommendations(self, analyses: List[CodeAnalysis],
                                 issues: List[str]) -> List[str]:
        """Generate factual recommendations."""
        recommendations = []

        undocumented = len([a for a in analyses if not a.has_documentation])
        if undocumented > 0:
            recommendations.append(f"Add documentation to {undocumented} elements")

        if issues:
            recommendations.append("Address structural issues found")

        return recommendations

    def _analyze_non_python_file(self, file_path: Path) -> AnalysisResult:
        """Analyze non-Python files."""
        file_type = file_path.suffix
        size = file_path.stat().st_size if file_path.exists() else 0

        analysis = CodeAnalysis(
            file_path=str(file_path),
            element_type="file",
            element_name=file_path.name,
            line_number=1,
            description=f"{file_type} file, {size} bytes",
            analysis_method="filesystem_analysis",
            has_documentation=False,
            dependencies=[],
            complexity_metrics={"file_size_bytes": size}
        )

        return AnalysisResult(
            target_path=str(file_path),
            analyses=[analysis],
            structural_issues=[],
            recommendations=[],
            analysis_summary={"file_type": file_type, "analysis_type": "filesystem"},
            llm_calls_made=[]
        )

    def _create_log_directory(self):
        """Create logging directory."""
        current_path = Path.cwd()
        while current_path.parent != current_path:
            reports_dir = current_path / ".vibelint-reports"
            if reports_dir.exists():
                self.log_dir = reports_dir / "justification_v2"
                break
            current_path = current_path.parent
        else:
            self.log_dir = Path.cwd() / ".vibelint-reports" / "justification_v2"

        self.log_dir.mkdir(parents=True, exist_ok=True)

    def save_analysis_logs(self, target_path: str, result: AnalysisResult):
        """Save honest analysis logs."""

        session_log = {
            "session_id": self._session_id,
            "target_analyzed": target_path,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "llm_calls": self._llm_call_logs,
            "analysis_result": {
                "analyses": [asdict(a) for a in result.analyses],
                "structural_issues": result.structural_issues,
                "recommendations": result.recommendations,
                "analysis_summary": result.analysis_summary
            },
            "capabilities_used": {
                "static_analysis": True,
                "embedding_analysis": len([call for call in self._llm_call_logs
                                         if call.get("operation") == "embedding_similarity"]) > 0,
                "llm_comparison": len([call for call in self._llm_call_logs
                                     if call.get("operation") == "llm_comparison"]) > 0
            }
        }

        # Save detailed log
        session_file = self.log_dir / f"{self._session_id}_detailed.json"
        with open(session_file, 'w', encoding='utf-8') as f:
            json.dump(session_log, f, indent=2, ensure_ascii=False)

        # Save human-readable summary
        summary_file = self.log_dir / f"{self._session_id}_summary.md"
        self._save_readable_summary(summary_file, session_log)

        return session_file, summary_file

    def _save_readable_summary(self, summary_file: Path, session_log: Dict[str, Any]):
        """Save honest readable summary."""

        analysis_result = session_log["analysis_result"]

        content = f"""# Code Analysis Report (Honest Edition)

**Session:** {session_log['session_id']}
**Target:** {session_log['target_analyzed']}
**Timestamp:** {session_log['timestamp']}

## Analysis Summary

- **Elements Analyzed:** {analysis_result['analysis_summary'].get('total_elements', 0)}
- **Documentation Coverage:** {analysis_result['analysis_summary'].get('documented_elements', 0)} elements have documentation
- **Capabilities Used:** Static Analysis: ✅ | Embeddings: {'✅' if session_log['capabilities_used']['embedding_analysis'] else '❌'} | LLM: {'✅' if session_log['capabilities_used']['llm_comparison'] else '❌'}

## Factual Observations

"""

        for analysis_data in analysis_result['analyses']:
            doc_status = "📝" if analysis_data['has_documentation'] else "📄"
            content += f"""### {analysis_data['element_name']} ({analysis_data['element_type']})

{doc_status} **Method:** {analysis_data['analysis_method']}
**Line:** {analysis_data['line_number']}
**Complexity Metrics:** {analysis_data['complexity_metrics']}

**Description:** {analysis_data['description']}

"""

        if analysis_result['structural_issues']:
            content += "## Structural Issues Found\n\n"
            for issue in analysis_result['structural_issues']:
                content += f"- {issue}\n"
            content += "\n"

        if analysis_result['recommendations']:
            content += "## Recommendations\n\n"
            for rec in analysis_result['recommendations']:
                content += f"- {rec}\n"
            content += "\n"

        if session_log['llm_calls']:
            content += "## ML/LLM Analysis Details\n\n"
            for i, call in enumerate(session_log['llm_calls'], 1):
                content += f"""### Analysis {i}: {call['operation']}

**Method:** {call.get('method', 'unknown')}
**Timestamp:** {call['timestamp']}

"""
                if 'similarity_score' in call:
                    content += f"**Similarity Score:** {call['similarity_score']:.3f}\n"
                if 'model_used' in call:
                    content += f"**Model Used:** {call['model_used']}\n"

                content += "\n---\n\n"

        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(content)

    def _find_cross_file_redundancies(self, all_methods: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Find redundant methods across files using embedding similarity."""
        redundancies = []

        if not self.embedding_client:
            logger.info("No embedding client - skipping similarity analysis")
            return redundancies

        logger.info(f"Comparing {len(all_methods)} methods for similarity")

        # Compare each method with every other method
        for i, method1 in enumerate(all_methods):
            for j, method2 in enumerate(all_methods[i+1:], i+1):
                if method1["file_path"] == method2["file_path"]:
                    continue  # Skip same-file comparisons

                # Get method source code (approximation)
                try:
                    # Read the files and extract method content
                    file1_content = Path(method1["file_path"]).read_text()
                    file2_content = Path(method2["file_path"]).read_text()

                    method1_code = self._extract_method_source(file1_content, method1["name"])
                    method2_code = self._extract_method_source(file2_content, method2["name"])

                    if method1_code and method2_code:
                        similarity = self.calculate_code_similarity(
                            method1_code, method2_code,
                            f"{Path(method1['file_path']).name}:{method1['name']}",
                            f"{Path(method2['file_path']).name}:{method2['name']}"
                        )

                        if similarity.get("similarity_available") and similarity["similarity_score"] > 0.7:
                            redundancies.append({
                                "method1": method1,
                                "method2": method2,
                                "similarity_score": similarity["similarity_score"],
                                "analysis_method": similarity["method"]
                            })

                except Exception as e:
                    logger.debug(f"Failed to compare {method1['name']} vs {method2['name']}: {e}")

        return redundancies

    def _extract_method_source(self, file_content: str, method_name: str) -> Optional[str]:
        """Extract source code for a specific method."""
        try:
            tree = ast.parse(file_content)
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef) and node.name == method_name:
                    lines = file_content.splitlines()
                    start_line = node.lineno - 1
                    # Find end line by looking for next function or class at same indentation
                    end_line = len(lines)
                    base_indent = len(lines[start_line]) - len(lines[start_line].lstrip())

                    for i in range(start_line + 1, len(lines)):
                        line = lines[i]
                        if line.strip() and (len(line) - len(line.lstrip())) <= base_indent:
                            if line.strip().startswith(('def ', 'class ', '@')):
                                end_line = i
                                break

                    return '\n'.join(lines[start_line:end_line])

        except Exception as e:
            logger.debug(f"Failed to extract method {method_name}: {e}")

        return None

    def generate_project_tree_analysis(self, directory_path: Path) -> str:
        """Generate a comprehensive project tree with one-line justifications."""
        results = self.analyze_directory(directory_path)

        tree_content = f"""# Project Analysis Tree
**Directory:** {directory_path}
**Timestamp:** {time.strftime('%Y-%m-%d %H:%M:%S')}

## Project Structure with Justifications

"""

        # Build file tree with justifications
        for file_path, result in sorted(results.items()):
            relative_path = Path(file_path).relative_to(directory_path)
            indent = "  " * (len(relative_path.parts) - 1)

            if result.analyses:
                file_analysis = next((a for a in result.analyses if a.element_type == "file"), None)
                if file_analysis:
                    justification = file_analysis.description
                else:
                    justification = "Analysis failed"
            else:
                justification = "Empty or error"

            # Format: indented_path - justification (complexity: X)
            complexity = result.analysis_summary.get('avg_complexity', 0)
            doc_ratio = (result.analysis_summary.get('documented_elements', 0) /
                        max(result.analysis_summary.get('total_elements', 1), 1))

            tree_content += f"{indent}📄 **{relative_path.name}** - {justification} "
            tree_content += f"(complexity: {complexity:.1f}, docs: {doc_ratio:.1%})\n"

            # Add issues if any
            if result.structural_issues:
                for issue in result.structural_issues[:2]:  # Show first 2 issues
                    tree_content += f"{indent}  ⚠️ {issue}\n"

        return tree_content

    def _get_llm_method_description(self, method_info: Dict[str, Any], file_path: Path) -> Optional[str]:
        """Get LLM-generated description of method purpose."""
        if not self.llm_manager:
            return None

        try:
            # Extract method source code
            file_content = file_path.read_text()
            method_source = self._extract_method_source(file_content, method_info["name"])

            if not method_source:
                return None

            # Use fast LLM for description (within 750 token limit)
            prompt = f"""Analyze this Python method and provide a one-sentence description of what it does:

```python
{method_source[:2000]}  # Truncate for token limit
```

Respond with ONLY a single sentence description, no preamble."""

            request = {
                "prompt": prompt,
                "max_tokens": 100,
                "temperature": 0.1
            }

            response = self.llm_manager.call_fast_llm(request)

            if response and response.get("success"):
                description = response["content"].strip()

                # Log the LLM call
                self._llm_call_logs.append({
                    "operation": "method_description",
                    "method_name": method_info["name"],
                    "file_path": str(file_path),
                    "prompt_tokens": len(prompt.split()),
                    "response": description,
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                })

                return description

        except Exception as e:
            logger.debug(f"LLM description failed for {method_info['name']}: {e}")

        return None

    def _analyze_method_usage(self, method_info: Dict[str, Any], file_path: Path) -> Dict[str, Any]:
        """Analyze method usage patterns to detect potential dead code."""
        method_name = method_info["name"]
        is_private = method_info["is_private"]

        # Quick static analysis for usage
        usage_score = 0
        potentially_dead = False

        try:
            # Check if method is called within the same file
            file_content = file_path.read_text()

            # Count direct calls
            call_count = file_content.count(f"{method_name}(")
            call_count += file_content.count(f".{method_name}(")

            # Special methods (dunder methods) are likely used
            if method_name.startswith("__") and method_name.endswith("__"):
                usage_score = 0.9
            # Public methods are more likely to be used
            elif not is_private:
                usage_score = 0.7 if call_count > 0 else 0.3
            # Private methods with no calls are suspicious
            else:
                usage_score = 0.5 if call_count > 0 else 0.1
                potentially_dead = call_count == 0

            # TODO: Enhanced analysis could check:
            # - Cross-file references (grep across project)
            # - Test coverage data
            # - Runtime call traces

        except Exception as e:
            logger.debug(f"Usage analysis failed for {method_name}: {e}")
            usage_score = 0.5  # Unknown usage

        return {
            "usage_score": usage_score,
            "potentially_dead": potentially_dead,
            "analysis_method": "static_call_count"
        }

    def _analyze_file_purpose_with_llm(self, file_path: Path, content: str) -> Optional[str]:
        """Get LLM-generated file purpose description."""
        if not self.llm_manager:
            return None

        try:
            # Get file structure summary
            tree = ast.parse(content)
            functions = [n.name for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)][:5]
            classes = [n.name for n in ast.walk(tree) if isinstance(n, ast.ClassDef)][:3]

            # Build context for LLM
            context = f"""File: {file_path.name}
Functions: {', '.join(functions) if functions else 'None'}
Classes: {', '.join(classes) if classes else 'None'}
"""

            # Add docstring if available
            module_doc = ast.get_docstring(tree)
            if module_doc:
                context += f"Docstring: {module_doc[:200]}..."

            prompt = f"""Analyze this Python file and describe its main purpose in one sentence:

{context}

Respond with ONLY a single sentence description of the file's purpose."""

            request = {
                "prompt": prompt,
                "max_tokens": 50,
                "temperature": 0.1
            }

            response = self.llm_manager.call_fast_llm(request)

            if response and response.get("success"):
                description = response["content"].strip()

                # Log the LLM call
                self._llm_call_logs.append({
                    "operation": "file_description",
                    "file_path": str(file_path),
                    "response": description,
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                })

                return description

        except Exception as e:
            logger.debug(f"LLM file description failed for {file_path}: {e}")

        return None
```

---
### File: src/vibelint/llm/__init__.py

```python
"""
LLM subsystem for vibelint.

Provides orchestrated LLM capabilities with fast/orchestrator role separation
and configuration management.

vibelint/src/vibelint/llm/__init__.py
"""

from .llm_config import get_llm_config
from .llm_orchestrator import (
    LLMOrchestrator,
    LLMRequest,
    LLMResponse,
    LLMRole,
    LLMBackend,
    create_llm_orchestrator
)
from .manager import create_llm_manager

__all__ = [
    # Configuration
    "get_llm_config",
    # Core orchestrator
    "LLMOrchestrator",
    "LLMRequest",
    "LLMResponse",
    "LLMRole",
    "LLMBackend",
    "create_llm_orchestrator",
    # Manager
    "create_llm_manager"
]
```

---
### File: src/vibelint/llm/llm_config.py

```python
"""
Vibelint Configuration Management

Handles configuration loading for vibelint with support for multiple config sources
and environment overrides.

Configuration priority order:
1. Environment variables
2. dev.pyproject.toml (development overrides)
3. pyproject.toml (production config) 
4. Default values

This is the authoritative config implementation - kaia imports from here.
"""

import os
from pathlib import Path
from typing import Any, Dict, Optional

# Handle TOML library imports - support Python 3.10 (tomli) and 3.11+ (tomllib)
try:
    import tomllib  # Python 3.11+
except ImportError:
    import tomli as tomllib  # Python 3.10 fallback

from dotenv import load_dotenv

# Configuration constants
DEFAULT_FAST_TEMPERATURE = 0.1
DEFAULT_FAST_MAX_TOKENS = 2048
DEFAULT_ORCHESTRATOR_TEMPERATURE = 0.2
DEFAULT_ORCHESTRATOR_MAX_TOKENS = 8192
DEFAULT_CONTEXT_THRESHOLD = 3000


def load_env_files(project_root: Optional[Path] = None):
    """Load environment variables from multiple possible locations."""
    if project_root is None:
        # Auto-detect from vibelint location
        project_root = Path(__file__).parent.parent.parent.parent.parent

    env_paths = [
        Path.cwd() / ".env",  # Current directory
        project_root / ".env",  # Project root
        project_root / "tools" / "vibelint" / ".env",  # vibelint directory
        Path.home() / ".vibelint.env",  # User home directory
    ]
    for env_path in env_paths:
        if env_path.exists():
            load_dotenv(env_path)


def load_toml_config(config_path: Path) -> Dict[str, Any]:
    """Load configuration from TOML file."""
    if not config_path.exists():
        return {}

    try:
        with open(config_path, "rb") as f:
            return tomllib.load(f)
    except Exception as e:
        print(f"Warning: Failed to load {config_path}: {e}")
        return {}


def find_project_root() -> Path:
    """Find the project root (killeraiagent) from vibelint's location."""
    current = Path(__file__).parent

    # Look for the main killeraiagent project, not vibelint's own project
    while current.parent != current:
        # Check for killeraiagent project indicators
        pyproject = current / "pyproject.toml"
        if pyproject.exists():
            try:
                with open(pyproject, "rb") as f:
                    config = tomllib.load(f)
                # Look for killeraiagent project name or our specific LLM config
                if (
                    config.get("project", {}).get("name") == "killeraiagent"
                    or ("tool" in config
                        and "vibelint" in config["tool"]
                        and "llm" in config["tool"]["vibelint"]
                        and config["tool"]["vibelint"]["llm"].get("fast_api_url"))
                ):
                    return current
            except Exception:
                pass
        current = current.parent

    # Fallback to 5 levels up from this file (should reach killeraiagent root)
    return Path(__file__).parent.parent.parent.parent.parent


def get_vibelint_config() -> Dict[str, Any]:
    """
    Load vibelint configuration with proper priority order.

    Priority:
    1. Environment variables
    2. dev.pyproject.toml (development overrides)
    3. pyproject.toml (production config) 
    4. Default values
    """
    project_root = find_project_root()

    # Load environment files first
    load_env_files(project_root)

    # Load base configuration from project root
    base_config = load_toml_config(project_root / "pyproject.toml")

    # Load development overrides from project root
    dev_config = load_toml_config(project_root / "dev.pyproject.toml")

    # Load vibelint-specific config
    vibelint_config = load_toml_config(Path(__file__).parent.parent.parent / "pyproject.toml")

    # Merge configurations (dev overrides base)
    merged_config: Dict[str, Any] = {}

    # Start with vibelint's own config
    if "tool" in vibelint_config and "vibelint" in vibelint_config["tool"]:
        merged_config = vibelint_config["tool"]["vibelint"].copy()

    # Apply base config from project root
    if "tool" in base_config and "vibelint" in base_config["tool"]:
        merged_config.update(base_config["tool"]["vibelint"])

    # Apply dev overrides from project root
    if "tool" in dev_config and "vibelint" in dev_config["tool"]:
        merged_config.update(dev_config["tool"]["vibelint"])

    return merged_config


def get_llm_config() -> Dict[str, Any]:
    """
    Get LLM configuration for vibelint.

    Returns:
    LLM configuration dict with environment variable overrides
    """
    config = get_vibelint_config()
    llm_config = config.get("llm", {}).copy()

    # Apply environment variable overrides
    env_overrides = {
        # Fast LLM overrides
        "fast_api_url": os.getenv("VIBELINT_FAST_LLM_API_URL") or os.getenv("FAST_LLM_API_URL"),
        "fast_model": os.getenv("VIBELINT_FAST_LLM_MODEL") or os.getenv("FAST_LLM_MODEL"),
        "fast_backend": os.getenv("VIBELINT_FAST_LLM_BACKEND") or os.getenv("FAST_LLM_BACKEND"),
        "fast_api_key": os.getenv("VIBELINT_FAST_LLM_API_KEY") or os.getenv("FAST_LLM_API_KEY"),
        "fast_temperature": _get_env_float("VIBELINT_FAST_LLM_TEMPERATURE") or _get_env_float("FAST_LLM_TEMPERATURE"),
        "fast_max_tokens": _get_env_int("VIBELINT_FAST_LLM_MAX_TOKENS") or _get_env_int("FAST_LLM_MAX_TOKENS"),

        # Orchestrator LLM overrides
        "orchestrator_api_url": os.getenv("VIBELINT_ORCHESTRATOR_LLM_API_URL") or os.getenv("ORCHESTRATOR_LLM_API_URL"),
        "orchestrator_model": os.getenv("VIBELINT_ORCHESTRATOR_LLM_MODEL") or os.getenv("ORCHESTRATOR_LLM_MODEL"),
        "orchestrator_backend": os.getenv("VIBELINT_ORCHESTRATOR_LLM_BACKEND") or os.getenv("ORCHESTRATOR_LLM_BACKEND"),
        "orchestrator_api_key": os.getenv("VIBELINT_ORCHESTRATOR_LLM_API_KEY") or os.getenv("ORCHESTRATOR_LLM_API_KEY"),
        "orchestrator_temperature": _get_env_float("VIBELINT_ORCHESTRATOR_LLM_TEMPERATURE") or _get_env_float("ORCHESTRATOR_LLM_TEMPERATURE"),
        "orchestrator_max_tokens": _get_env_int("VIBELINT_ORCHESTRATOR_LLM_MAX_TOKENS") or _get_env_int("ORCHESTRATOR_LLM_MAX_TOKENS"),

        # Routing configuration
        "context_threshold": _get_env_int("VIBELINT_LLM_CONTEXT_THRESHOLD") or _get_env_int("LLM_CONTEXT_THRESHOLD"),
    }

    # Apply non-None overrides
    for key, value in env_overrides.items():
        if value is not None:
            llm_config[key] = value

    # Apply defaults for missing values
    defaults = {
        "fast_temperature": DEFAULT_FAST_TEMPERATURE,
        "fast_max_tokens": DEFAULT_FAST_MAX_TOKENS,
        "fast_backend": "vllm",
        "orchestrator_temperature": DEFAULT_ORCHESTRATOR_TEMPERATURE,
        "orchestrator_max_tokens": DEFAULT_ORCHESTRATOR_MAX_TOKENS,
        "orchestrator_backend": "llamacpp",
        "context_threshold": DEFAULT_CONTEXT_THRESHOLD,
    }

    for key, default_value in defaults.items():
        if key not in llm_config:
            llm_config[key] = default_value

    return llm_config


def _get_env_float(key: str) -> Optional[float]:
    """Get float value from environment variable."""
    value = os.getenv(key)
    if value is not None:
        try:
            return float(value)
        except ValueError:
            return None
    return None


def _get_env_int(key: str) -> Optional[int]:
    """Get integer value from environment variable."""
    value = os.getenv(key)
    if value is not None:
        try:
            return int(value)
        except ValueError:
            return None
    return None


def _get_env_bool(key: str) -> Optional[bool]:
    """Get boolean value from environment variable."""
    value = os.getenv(key)
    if value is not None:
        return value.lower() in ("true", "1", "yes", "on")
    return None


# Factory function for creating orchestrator
def create_llm_orchestrator_from_config():
    """Create LLM orchestrator from vibelint configuration."""
    from .llm_orchestrator import LLMOrchestrator

    config = {"llm": get_llm_config()}
    return LLMOrchestrator.from_config(config)


# Export main functions
__all__ = [
    "get_vibelint_config",
    "get_llm_config",
    "load_env_files",
    "find_project_root",
    "create_llm_orchestrator_from_config",
]
```

---
### File: src/vibelint/llm/llm_context_engineer.py

```python
"""
LLM Context Engineering for Vibelint

The core innovation: Transform raw codebase into rich, structured context that enables
foundation models (GPT-OSS 120B, Claude) to make intelligent code improvements beyond
simple linting.

This is vibelint's secret sauce - sophisticated context engineering that gives LLMs
the deep codebase understanding they need for complex architectural decisions.
"""

import json
import time
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional

import networkx as nx

from ..dependency_graph_manager import DependencyGraphManager, GraphQuery
from ..multi_representation_analyzer import (ImprovementOpportunity,
                                            MultiRepresentationAnalyzer)


@dataclass
class LLMContext:
    """Rich context package for foundation model consumption."""

    # Core codebase understanding
    project_overview: Dict[str, Any]
    current_focus: Dict[str, Any]  # What the LLM should focus on

    # Multi-representation context
    filesystem_context: Dict[str, Any]
    semantic_context: Dict[str, Any]
    dependency_context: Dict[str, Any]
    execution_context: Dict[str, Any]

    # Problem-specific context
    improvement_opportunity: Optional[ImprovementOpportunity]
    similar_patterns: List[Dict[str, Any]]
    impact_analysis: Dict[str, Any]

    # Decision support
    suggested_approach: str
    risk_assessment: Dict[str, Any]
    implementation_guidance: List[str]

    # Meta-context
    confidence_indicators: Dict[str, float]
    context_completeness: float
    llm_instructions: str


@dataclass
class LLMRequest:
    """Request to foundation model with engineered context."""

    task_type: str  # "improve_code", "fix_architecture", "optimize_performance"
    context: LLMContext
    target_files: List[str]
    constraints: List[str]
    success_criteria: List[str]

    # LLM routing
    preferred_model: str  # "chip", "claudia", "claude_cli"
    reasoning_depth: str  # "shallow", "medium", "deep"
    creativity_level: float  # 0.0 = conservative, 1.0 = creative


@dataclass
class LLMResponse:
    """Structured response from foundation model."""

    reasoning_trace: str
    proposed_changes: List[Dict[str, Any]]
    confidence_score: float
    alternative_approaches: List[str]
    risk_mitigation: List[str]

    # Implementation details
    code_changes: List[Dict[str, str]]  # file_path -> new_content
    test_recommendations: List[str]
    documentation_updates: List[str]

    # Meta-response
    model_used: str
    processing_time_ms: int
    token_usage: Dict[str, int]


class VibelintContextEngineer:
    """
    The heart of vibelint: Engineers rich context from multi-representation analysis
    to enable foundation models to make sophisticated code improvements.
    """

    def __init__(
        self,
        project_root: Path,
        multi_rep_analyzer: MultiRepresentationAnalyzer,
        dependency_manager: DependencyGraphManager,
    ):
        self.project_root = project_root
        self.analyzer = multi_rep_analyzer
        self.dependency_manager = dependency_manager

        # Context engineering templates
        self.context_templates = self._load_context_templates()

        # LLM model configurations
        self.model_configs = {
            "chip": {
                "url": "https://chipllm-auth-worker.mithran-mohanraj.workers.dev",
                "strengths": ["deep_reasoning", "complex_architecture", "performance_optimization"],
                "max_context": 28800,
                "ideal_for": [
                    "architectural_changes",
                    "performance_optimization",
                    "complex_refactoring",
                ],
            },
            "claudia": {
                "url": "https://claudiallm-auth-worker.mithran-mohanraj.workers.dev",
                "strengths": ["fast_iteration", "simple_fixes", "code_generation"],
                "max_context": 900,
                "ideal_for": ["simple_fixes", "code_generation", "quick_improvements"],
            },
            "claude_cli": {
                "strengths": ["general_purpose", "explanation", "complex_reasoning"],
                "max_context": 200000,
                "ideal_for": [
                    "complex_analysis",
                    "architectural_decisions",
                    "comprehensive_refactoring",
                ],
            },
        }

    async def engineer_context_for_improvement(
        self, opportunity: ImprovementOpportunity, analysis_results: Dict[str, Any]
    ) -> LLMContext:
        """
        Engineer rich context for a specific improvement opportunity.
        This is where the magic happens - transforming raw analysis into perfect LLM context.
        """
        print(f"🧠 Engineering context for: {opportunity.opportunity_id}")

        # Build project overview
        project_overview = await self._build_project_overview(analysis_results)

        # Focus context on the specific improvement
        current_focus = await self._build_focus_context(opportunity, analysis_results)

        # Extract relevant multi-representation context
        filesystem_context = await self._extract_filesystem_context(opportunity, analysis_results)
        semantic_context = await self._extract_semantic_context(opportunity, analysis_results)
        dependency_context = await self._extract_dependency_context(opportunity, analysis_results)
        execution_context = await self._extract_execution_context(opportunity, analysis_results)

        # Find similar patterns and examples
        similar_patterns = await self._find_similar_patterns(opportunity)

        # Analyze impact and risks
        impact_analysis = await self._analyze_impact(opportunity, analysis_results)

        # Generate approach and guidance
        suggested_approach = await self._suggest_approach(opportunity, analysis_results)
        risk_assessment = await self._assess_risks(opportunity, analysis_results)
        implementation_guidance = await self._generate_guidance(opportunity, analysis_results)

        # Calculate confidence and completeness
        confidence_indicators = await self._calculate_confidence(opportunity, analysis_results)
        context_completeness = await self._assess_completeness(opportunity, analysis_results)

        # Generate LLM instructions
        llm_instructions = await self._generate_llm_instructions(opportunity, analysis_results)

        return LLMContext(
            project_overview=project_overview,
            current_focus=current_focus,
            filesystem_context=filesystem_context,
            semantic_context=semantic_context,
            dependency_context=dependency_context,
            execution_context=execution_context,
            improvement_opportunity=opportunity,
            similar_patterns=similar_patterns,
            impact_analysis=impact_analysis,
            suggested_approach=suggested_approach,
            risk_assessment=risk_assessment,
            implementation_guidance=implementation_guidance,
            confidence_indicators=confidence_indicators,
            context_completeness=context_completeness,
            llm_instructions=llm_instructions,
        )

    async def _build_project_overview(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """Build high-level project context."""
        return {
            "project_name": self.project_root.name,
            "project_type": "Python linting and code analysis tool",
            "key_characteristics": {
                "total_files": len(list(self.project_root.rglob("*.py"))),
                "architecture_style": "modular_with_plugins",
                "main_responsibilities": [
                    "Code quality analysis",
                    "Validation and linting",
                    "LLM-powered analysis",
                    "Self-improvement capabilities",
                ],
                "quality_score": analysis_results.get("summary", {}).get(
                    "overall_quality_score", 0.5
                ),
            },
            "current_challenges": [
                op.category
                for op in analysis_results.get("improvement_opportunities", [])
                if op.severity == "high"
            ],
            "architectural_patterns": analysis_results.get("vector_analysis", {}).get(
                "architectural_patterns", {}
            ),
            "dependency_complexity": {
                "total_nodes": analysis_results.get("graph_analysis", {})
                .get("dependency_metrics", {})
                .get("total_nodes", 0),
                "circular_dependencies": len(
                    analysis_results.get("graph_analysis", {})
                    .get("dependency_metrics", {})
                    .get("circular_dependencies", [])
                ),
                "max_depth": analysis_results.get("graph_analysis", {})
                .get("dependency_metrics", {})
                .get("longest_path", []),
            },
        }

    async def _build_focus_context(
        self, opportunity: ImprovementOpportunity, analysis_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Build focused context for the specific improvement."""
        return {
            "improvement_target": {
                "category": opportunity.category,
                "severity": opportunity.severity,
                "file_path": opportunity.file_path,
                "line_number": opportunity.line_number,
                "affected_modules": opportunity.affected_modules,
            },
            "current_problem": {
                "description": opportunity.current_state,
                "impact_on_codebase": opportunity.estimated_impact,
                "execution_criticality": opportunity.execution_criticality,
            },
            "desired_outcome": {
                "target_state": opportunity.target_state,
                "success_metrics": opportunity.estimated_impact,
                "implementation_steps": opportunity.implementation_steps,
            },
            "scope_of_change": {
                "direct_files": opportunity.affected_modules,
                "dependent_files": opportunity.dependency_impact,
                "semantic_cluster": opportunity.semantic_clusters,
            },
        }

    async def _extract_filesystem_context(
        self, opportunity: ImprovementOpportunity, analysis_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Extract relevant filesystem organization context."""
        filesystem_analysis = analysis_results.get("filesystem_analysis", {})

        relevant_files = set(opportunity.affected_modules + opportunity.dependency_impact)

        # Extract module hierarchy for relevant files
        relevant_hierarchy = {}
        for file_path in relevant_files:
            # Find this file in the hierarchy
            hierarchy = filesystem_analysis.get("module_hierarchy", {})
            # Simplified - would traverse hierarchy to find relevant context
            relevant_hierarchy[file_path] = {
                "package": str(Path(file_path).parent),
                "module_purpose": self._infer_module_purpose(file_path),
                "related_files": self._find_related_files(file_path, hierarchy),
            }

        return {
            "relevant_modules": relevant_hierarchy,
            "package_organization": filesystem_analysis.get("package_organization", {}),
            "naming_patterns": filesystem_analysis.get("naming_conventions", {}),
            "architectural_layout": self._describe_architectural_layout(
                opportunity, filesystem_analysis
            ),
        }

    async def _extract_semantic_context(
        self, opportunity: ImprovementOpportunity, analysis_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Extract semantic understanding context."""
        vector_analysis = analysis_results.get("vector_analysis", {})

        return {
            "semantic_clusters": {
                cluster: elements
                for cluster, elements in vector_analysis.get("semantic_clusters", {}).items()
                if any(elem in opportunity.similar_code_patterns for elem in elements)
            },
            "code_duplication": [
                dup
                for dup in vector_analysis.get("code_duplication", [])
                if any(mod in [dup["file1"], dup["file2"]] for mod in opportunity.affected_modules)
            ],
            "naming_consistency": vector_analysis.get("naming_consistency", {}),
            "functional_cohesion": vector_analysis.get("functional_cohesion", {}),
            "semantic_purpose": await self._infer_semantic_purpose(opportunity, vector_analysis),
        }

    async def _extract_dependency_context(
        self, opportunity: ImprovementOpportunity, analysis_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Extract dependency graph context."""
        graph_analysis = analysis_results.get("graph_analysis", {})

        # Get dependency subgraph around the improvement target
        subgraph_context = await self._extract_dependency_subgraph(opportunity)

        return {
            "local_dependencies": subgraph_context,
            "impact_radius": await self._calculate_impact_radius(opportunity),
            "architectural_violations": [
                violation
                for violation in graph_analysis.get("architectural_violations", [])
                if any(
                    mod in violation.get("affected_modules", [])
                    for mod in opportunity.affected_modules
                )
            ],
            "critical_paths": await self._find_critical_paths_through_target(opportunity),
            "coupling_metrics": await self._get_coupling_metrics(opportunity),
        }

    async def _extract_execution_context(
        self, opportunity: ImprovementOpportunity, analysis_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Extract runtime execution context."""
        # This would use trace session data to understand how code actually executes
        return {
            "execution_frequency": opportunity.execution_criticality,
            "performance_profile": {
                "avg_execution_time": "unknown",  # Would come from trace data
                "memory_usage": "unknown",
                "io_operations": "unknown",
            },
            "call_patterns": [],  # Would come from trace data
            "error_patterns": [],  # Would come from trace data
            "usage_scenarios": await self._infer_usage_scenarios(opportunity),
        }

    async def _find_similar_patterns(
        self, opportunity: ImprovementOpportunity
    ) -> List[Dict[str, Any]]:
        """Find similar patterns in the codebase using embeddings."""
        if not opportunity.similar_code_patterns:
            return []

        # Query for similar patterns using semantic search
        query = GraphQuery(
            semantic_query=f"{opportunity.category} {opportunity.current_state}", max_results=5
        )

        similar_nodes = await self.dependency_manager.query_similar_dependencies(query)

        return [
            {
                "pattern_type": "semantic_similarity",
                "node_info": node["node_info"],
                "similarity_score": node["similarity_score"],
                "why_relevant": f"Similar {opportunity.category} pattern",
            }
            for node in similar_nodes
        ]

    async def _analyze_impact(
        self, opportunity: ImprovementOpportunity, analysis_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Analyze the impact of making this improvement."""
        return {
            "direct_benefits": opportunity.estimated_impact,
            "indirect_benefits": await self._calculate_indirect_benefits(
                opportunity, analysis_results
            ),
            "affected_stakeholders": await self._identify_stakeholders(opportunity),
            "cascade_effects": await self._predict_cascade_effects(opportunity, analysis_results),
            "metrics_improvement": await self._predict_metrics_improvement(
                opportunity, analysis_results
            ),
        }

    async def _suggest_approach(
        self, opportunity: ImprovementOpportunity, analysis_results: Dict[str, Any]
    ) -> str:
        """Suggest the best approach for this improvement."""
        approach_templates = {
            "solid": "Apply SOLID principles systematically: {steps}",
            "complexity": "Reduce complexity through decomposition: {steps}",
            "pythonic": "Improve Pythonic practices: {steps}",
            "architecture": "Refactor architecture: {steps}",
        }

        template = approach_templates.get(opportunity.category, "General improvement: {steps}")
        steps = " -> ".join(opportunity.implementation_steps[:3])  # First 3 steps

        return template.format(steps=steps)

    async def _assess_risks(
        self, opportunity: ImprovementOpportunity, analysis_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Assess risks of implementing this improvement."""
        return {
            "implementation_risk": opportunity.risk_assessment,
            "breaking_changes_risk": await self._assess_breaking_changes(opportunity),
            "performance_impact_risk": await self._assess_performance_impact(opportunity),
            "maintenance_burden_risk": await self._assess_maintenance_burden(opportunity),
            "mitigation_strategies": await self._suggest_risk_mitigation(opportunity),
        }

    async def _generate_guidance(
        self, opportunity: ImprovementOpportunity, analysis_results: Dict[str, Any]
    ) -> List[str]:
        """Generate implementation guidance."""
        return [
            f"Start with {opportunity.implementation_steps[0] if opportunity.implementation_steps else 'analysis'}",
            f"Focus on {opportunity.affected_modules[0] if opportunity.affected_modules else 'target file'} first",
            "Test changes incrementally",
            "Monitor impact on related modules",
            "Update documentation as needed",
        ]

    async def _calculate_confidence(
        self, opportunity: ImprovementOpportunity, analysis_results: Dict[str, Any]
    ) -> Dict[str, float]:
        """Calculate confidence indicators."""
        return {
            "problem_identification": 0.9,  # High confidence in multi-rep analysis
            "solution_appropriateness": 0.8,  # Good confidence in suggested approach
            "impact_prediction": 0.7,  # Moderate confidence in impact analysis
            "risk_assessment": 0.6,  # Lower confidence without full testing
            "implementation_feasibility": 0.8,  # Good confidence based on patterns
        }

    async def _assess_completeness(
        self, opportunity: ImprovementOpportunity, analysis_results: Dict[str, Any]
    ) -> float:
        """Assess completeness of context."""
        completeness_factors = [
            1.0 if opportunity.affected_modules else 0.0,  # Have target files
            1.0 if opportunity.implementation_steps else 0.0,  # Have implementation plan
            1.0 if opportunity.estimated_impact else 0.0,  # Have impact estimate
            1.0 if analysis_results.get("filesystem_analysis") else 0.0,  # Have filesystem context
            1.0 if analysis_results.get("vector_analysis") else 0.0,  # Have semantic context
            1.0 if analysis_results.get("graph_analysis") else 0.0,  # Have graph context
        ]

        return sum(completeness_factors) / len(completeness_factors)

    async def _generate_llm_instructions(
        self, opportunity: ImprovementOpportunity, analysis_results: Dict[str, Any]
    ) -> str:
        """Generate specific instructions for the LLM."""
        instruction_template = self.context_templates.get(
            opportunity.category, self.context_templates["default"]
        )

        return instruction_template.format(
            category=opportunity.category,
            severity=opportunity.severity,
            current_state=opportunity.current_state,
            target_state=opportunity.target_state,
            file_path=opportunity.file_path,
            affected_modules=", ".join(opportunity.affected_modules[:3]),
        )

    def _load_context_templates(self) -> Dict[str, str]:
        """Load context templates for different improvement categories."""
        return {
            "solid": """
You are improving code quality by applying SOLID principles to {category} issues.

CURRENT PROBLEM: {current_state}
TARGET OUTCOME: {target_state}
SEVERITY: {severity}

FOCUS: Apply systematic refactoring to {file_path} and related modules: {affected_modules}

APPROACH:
1. Identify specific SOLID principle violations
2. Design minimal changes that address the root cause
3. Ensure changes improve testability and maintainability
4. Preserve existing functionality

OUTPUT: Provide specific code changes with detailed reasoning for each SOLID principle applied.
            """,
            "complexity": """
You are reducing code complexity to improve maintainability and readability.

CURRENT PROBLEM: {current_state}
TARGET OUTCOME: {target_state}
SEVERITY: {severity}

FOCUS: Simplify complex code in {file_path} affecting: {affected_modules}

APPROACH:
1. Identify sources of complexity (nested conditions, long functions, etc.)
2. Extract smaller, focused functions
3. Use early returns and guard clauses
4. Consider design patterns if appropriate

OUTPUT: Provide refactored code with complexity analysis showing improvement.
            """,
            "pythonic": """
You are improving Python code to follow best practices and idioms.

CURRENT PROBLEM: {current_state}
TARGET OUTCOME: {target_state}
SEVERITY: {severity}

FOCUS: Make code more Pythonic in {file_path} and related: {affected_modules}

APPROACH:
1. Apply Python idioms and best practices
2. Improve type hints and documentation
3. Use appropriate data structures and libraries
4. Follow PEP conventions

OUTPUT: Provide improved code following Python best practices with explanations.
            """,
            "architecture": """
You are improving software architecture to reduce coupling and improve modularity.

CURRENT PROBLEM: {current_state}
TARGET OUTCOME: {target_state}
SEVERITY: {severity}

FOCUS: Refactor architecture in {file_path} with impact on: {affected_modules}

APPROACH:
1. Identify architectural anti-patterns
2. Design cleaner interfaces and abstractions
3. Reduce dependencies and coupling
4. Improve module boundaries

OUTPUT: Provide architectural improvements with migration strategy.
            """,
            "default": """
You are improving code quality in the {category} category.

PROBLEM: {current_state}
GOAL: {target_state}
SEVERITY: {severity}

Focus on {file_path} and related modules: {affected_modules}

Provide specific improvements with clear reasoning and implementation steps.
            """,
        }

    # Helper methods for context extraction
    def _infer_module_purpose(self, file_path: str) -> str:
        """Infer the purpose of a module from its path and name."""
        path_parts = Path(file_path).parts
        filename = Path(file_path).stem

        purpose_indicators = {
            "config": "configuration management",
            "core": "core functionality",
            "utils": "utility functions",
            "validators": "validation logic",
            "cli": "command line interface",
            "api": "API interface",
            "models": "data models",
            "tests": "testing",
        }

        for indicator, purpose in purpose_indicators.items():
            if indicator in filename.lower() or any(
                indicator in part.lower() for part in path_parts
            ):
                return purpose

        return "general purpose module"

    def _find_related_files(self, file_path: str, hierarchy: Dict[str, Any]) -> List[str]:
        """Find files related to the target file."""
        # Simplified - would analyze imports, similar names, same directory
        file_dir = str(Path(file_path).parent)
        related = []

        # Files in same directory are likely related
        for path, data in hierarchy.items():
            if isinstance(data, dict) and data.get("type") == "file":
                if str(Path(data.get("path", "")).parent) == file_dir:
                    related.append(data.get("path", ""))

        return related[:5]  # Limit to 5 most related

    def _describe_architectural_layout(
        self, opportunity: ImprovementOpportunity, filesystem_analysis: Dict[str, Any]
    ) -> str:
        """Describe the architectural layout relevant to the opportunity."""
        if opportunity.category == "architecture":
            return "Complex architectural refactoring required with careful attention to module boundaries"
        elif opportunity.category == "solid":
            return "Focus on single responsibility and dependency inversion principles"
        else:
            return "Standard modular layout with clear separation of concerns"

    async def _infer_semantic_purpose(
        self, opportunity: ImprovementOpportunity, vector_analysis: Dict[str, Any]
    ) -> str:
        """Infer the semantic purpose of the improvement target."""
        # Use semantic clusters and naming patterns to understand purpose
        semantic_clusters = vector_analysis.get("semantic_clusters", {})

        for cluster_name, elements in semantic_clusters.items():
            if any(elem in opportunity.similar_code_patterns for elem in elements):
                return f"Part of {cluster_name} functionality cluster"

        return "Standalone functionality"

    async def _extract_dependency_subgraph(
        self, opportunity: ImprovementOpportunity
    ) -> Dict[str, Any]:
        """Extract dependency subgraph around the improvement target."""
        graph = self.dependency_manager.dependency_graph

        target_nodes = []
        for module in opportunity.affected_modules:
            # Find nodes in graph that match this module
            matching_nodes = [node for node in graph.nodes() if module in node]
            target_nodes.extend(matching_nodes)

        if not target_nodes:
            return {}

        # Get ego graph (node + immediate neighbors)
        subgraph_nodes = set()
        for node in target_nodes:
            if node in graph:
                subgraph_nodes.add(node)
                subgraph_nodes.update(graph.successors(node))
                subgraph_nodes.update(graph.predecessors(node))

        subgraph = graph.subgraph(subgraph_nodes)

        return {
            "nodes": list(subgraph.nodes()),
            "edges": list(subgraph.edges()),
            "density": nx.density(subgraph),
            "components": len(list(nx.weakly_connected_components(subgraph))),
        }

    async def _calculate_impact_radius(self, opportunity: ImprovementOpportunity) -> Dict[str, int]:
        """Calculate how far the impact of this change might reach."""
        graph = self.dependency_manager.dependency_graph

        impact_radius = {"direct": 0, "indirect": 0, "distant": 0}

        for module in opportunity.affected_modules:
            matching_nodes = [node for node in graph.nodes() if module in node]

            for node in matching_nodes:
                if node in graph:
                    # Direct impact: immediate dependencies
                    direct = len(list(graph.successors(node))) + len(list(graph.predecessors(node)))
                    impact_radius["direct"] += direct

                    # Indirect impact: 2-hop dependencies
                    indirect = 0
                    for neighbor in graph.neighbors(node):
                        indirect += len(list(graph.neighbors(neighbor)))
                    impact_radius["indirect"] += indirect

        return impact_radius

    async def _find_critical_paths_through_target(
        self, opportunity: ImprovementOpportunity
    ) -> List[str]:
        """Find critical execution paths that go through the improvement target."""
        # This would use execution trace data to find critical paths
        # For now, simplified version
        return [f"Critical path through {module}" for module in opportunity.affected_modules[:3]]

    async def _get_coupling_metrics(self, opportunity: ImprovementOpportunity) -> Dict[str, float]:
        """Get coupling metrics for the improvement target."""
        graph = self.dependency_manager.dependency_graph

        coupling_metrics = {}

        for module in opportunity.affected_modules:
            matching_nodes = [node for node in graph.nodes() if module in node]

            for node in matching_nodes:
                if node in graph:
                    in_degree = graph.in_degree(node)
                    out_degree = graph.out_degree(node)
                    total = in_degree + out_degree

                    coupling_metrics[node] = {
                        "afferent_coupling": in_degree,
                        "efferent_coupling": out_degree,
                        "instability": out_degree / total if total > 0 else 0,
                    }

        return coupling_metrics

    async def _infer_usage_scenarios(self, opportunity: ImprovementOpportunity) -> List[str]:
        """Infer how the improvement target is typically used."""
        # This would analyze call patterns from trace data
        return [
            f"Used in {opportunity.category} context",
            "Called during validation process",
            "Part of analysis pipeline",
        ]

    # Additional helper methods for risk and impact analysis
    async def _calculate_indirect_benefits(
        self, opportunity: ImprovementOpportunity, analysis_results: Dict[str, Any]
    ) -> Dict[str, float]:
        """Calculate indirect benefits of this improvement."""
        return {
            "improved_testability": 0.3,
            "reduced_maintenance_cost": 0.4,
            "better_onboarding_experience": 0.2,
            "increased_development_velocity": 0.3,
        }

    async def _identify_stakeholders(self, opportunity: ImprovementOpportunity) -> List[str]:
        """Identify who would be affected by this improvement."""
        return [
            "vibelint developers",
            "vibelint users",
            "code quality engineers",
            "future maintainers",
        ]

    async def _predict_cascade_effects(
        self, opportunity: ImprovementOpportunity, analysis_results: Dict[str, Any]
    ) -> List[str]:
        """Predict cascade effects of this improvement."""
        return [
            f"May require updates to {len(opportunity.dependency_impact)} dependent modules",
            "Could improve overall code quality metrics",
            "May reduce future technical debt accumulation",
        ]

    async def _predict_metrics_improvement(
        self, opportunity: ImprovementOpportunity, analysis_results: Dict[str, Any]
    ) -> Dict[str, str]:
        """Predict how metrics will improve."""
        return {
            "code_quality_score": f"+{opportunity.estimated_impact.get('maintainability', 0.5) * 10:.1f}%",
            "technical_debt_reduction": f"-{opportunity.estimated_impact.get('maintainability', 0.5) * 15:.1f}%",
            "development_velocity": f"+{opportunity.estimated_impact.get('testability', 0.3) * 5:.1f}%",
        }

    async def _assess_breaking_changes(self, opportunity: ImprovementOpportunity) -> str:
        """Assess risk of breaking changes."""
        if opportunity.category == "architecture":
            return "high" if len(opportunity.dependency_impact) > 5 else "medium"
        elif opportunity.severity == "high":
            return "medium"
        else:
            return "low"

    async def _assess_performance_impact(self, opportunity: ImprovementOpportunity) -> str:
        """Assess performance impact risk."""
        if "performance" in opportunity.current_state.lower():
            return "medium"
        elif opportunity.execution_criticality > 0.7:
            return "medium"
        else:
            return "low"

    async def _assess_maintenance_burden(self, opportunity: ImprovementOpportunity) -> str:
        """Assess maintenance burden risk."""
        if len(opportunity.implementation_steps) > 5:
            return "high"
        elif opportunity.category == "architecture":
            return "medium"
        else:
            return "low"

    async def _suggest_risk_mitigation(self, opportunity: ImprovementOpportunity) -> List[str]:
        """Suggest risk mitigation strategies."""
        return [
            "Implement changes incrementally",
            "Add comprehensive tests before refactoring",
            "Use feature flags for gradual rollout",
            "Monitor performance metrics during implementation",
            "Maintain backwards compatibility where possible",
        ]

    async def route_to_optimal_llm(self, context: LLMContext, task_type: str) -> str:
        """Route the request to the optimal LLM based on context and task."""

        # Calculate context size
        context_size = len(json.dumps(asdict(context), default=str))

        # Route based on complexity and context size
        if context_size > 20000:  # Large context
            return "claude_cli"  # Has largest context window
        elif task_type in ["architectural_changes", "complex_refactoring"]:
            return "chip"  # Best for deep reasoning
        elif task_type in ["simple_fixes", "code_generation"]:
            return "claudia"  # Fast for simple tasks
        else:
            return "chip"  # Default to reasoning model

    def export_context_for_llm(self, context: LLMContext, output_path: Path) -> Dict[str, Any]:
        """Export context in LLM-optimized format."""
        llm_context = {
            "vibelint_context_v1": {
                "metadata": {
                    "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "context_completeness": context.context_completeness,
                    "confidence_score": sum(context.confidence_indicators.values())
                    / len(context.confidence_indicators),
                },
                "project_understanding": context.project_overview,
                "improvement_focus": (
                    asdict(context.improvement_opportunity)
                    if context.improvement_opportunity
                    else {}
                ),
                "codebase_context": {
                    "filesystem": context.filesystem_context,
                    "semantics": context.semantic_context,
                    "dependencies": context.dependency_context,
                    "execution": context.execution_context,
                },
                "decision_support": {
                    "similar_patterns": context.similar_patterns,
                    "impact_analysis": context.impact_analysis,
                    "suggested_approach": context.suggested_approach,
                    "risk_assessment": context.risk_assessment,
                    "implementation_guidance": context.implementation_guidance,
                },
                "llm_instructions": context.llm_instructions,
            }
        }

        with open(output_path, "w") as f:
            json.dump(llm_context, f, indent=2, default=str)

        print(f"📋 LLM context exported to: {output_path}")
        return llm_context


# Main integration function
async def engineer_context_for_vibelint_improvement(
    project_root: Path,
    improvement_opportunity: ImprovementOpportunity,
    analysis_results: Dict[str, Any],
    multi_rep_analyzer: MultiRepresentationAnalyzer,
    dependency_manager: DependencyGraphManager,
) -> LLMContext:
    """
    Main entry point: Engineer rich context for vibelint self-improvement.

    This is the culmination of vibelint's context engineering - taking raw codebase
    analysis and transforming it into the perfect context for foundation models
    to make intelligent code improvements.
    """

    print(f"🚀 Engineering LLM context for improvement: {improvement_opportunity.opportunity_id}")

    context_engineer = VibelintContextEngineer(project_root, multi_rep_analyzer, dependency_manager)

    # Engineer the context
    context = await context_engineer.engineer_context_for_improvement(
        improvement_opportunity, analysis_results
    )

    # Export for LLM consumption
    export_path = (
        project_root
        / ".vibelint-self-improvement"
        / f"llm_context_{improvement_opportunity.opportunity_id}.json"
    )
    export_path.parent.mkdir(exist_ok=True)

    context_engineer.export_context_for_llm(context, export_path)

    print("✅ Context engineering complete:")
    print(f"  - Completeness: {context.context_completeness:.1%}")
    print(
        f"  - Confidence: {sum(context.confidence_indicators.values()) / len(context.confidence_indicators):.1%}"
    )
    print(
        f"  - Optimal LLM: {await context_engineer.route_to_optimal_llm(context, improvement_opportunity.category)}"
    )

    return context


if __name__ == "__main__":
    print(
        "🧠 Vibelint Context Engineering - The bridge between codebase analysis and LLM intelligence"
    )
    print(
        "This module transforms raw code analysis into rich context that enables foundation models"
    )
    print(
        "to make sophisticated architectural and code quality improvements beyond simple linting."
    )
```

---
### File: src/vibelint/llm/llm_orchestrator.py

```python
"""
Vibelint LLM Orchestrator

Configurable LLM system supporting multiple inference backends (vLLM, llama.cpp, OpenAI)
with dual LLM architecture (fast + orchestrator) for code analysis tasks.

This is the authoritative LLM implementation - kaia guardrails imports from here.

Features:
- Backend abstraction (vLLM, llama.cpp, OpenAI)
- Structured JSON generation
- Intelligent routing between fast/orchestrator LLMs
- Configuration management
- Request/response standardization
"""

import json
import logging
import os
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, Optional
from urllib.request import Request, urlopen

logger = logging.getLogger(__name__)


class LLMBackend(Enum):
    """Supported LLM inference backends."""

    VLLM = "vllm"
    LLAMACPP = "llamacpp"
    OPENAI = "openai"


class LLMRole(Enum):
    """LLM roles for different types of tasks."""

    FAST = "fast"  # High-speed inference, small context
    ORCHESTRATOR = "orchestrator"  # Large context, complex reasoning


@dataclass
class LLMRequest:
    """Request specification for LLM processing."""

    content: str
    task_type: str
    max_tokens: Optional[int] = None
    temperature: Optional[float] = None
    require_json: bool = False  # Request structured JSON response
    system_prompt: Optional[str] = None
    context: Dict[str, Any] = field(default_factory=dict)


@dataclass
class LLMResponse:
    """Response from LLM processing."""

    content: str
    role_used: LLMRole
    backend_used: LLMBackend
    tokens_used: Optional[int] = None
    processing_time: float = 0.0
    parsed_json: Optional[Dict[str, Any]] = None
    raw_response: Optional[Dict[str, Any]] = None


class LLMBackendClient(ABC):
    """Abstract base class for LLM backend clients."""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.api_url = config.get("api_url")
        self.model = config.get("model")
        self.api_key = config.get("api_key")

    @abstractmethod
    def make_request(self, request: LLMRequest) -> LLMResponse:
        """Make a request to the LLM backend."""
        pass

    @abstractmethod
    def supports_structured_json(self) -> bool:
        """Whether this backend supports structured JSON generation."""
        pass


class VLLMClient(LLMBackendClient):
    """vLLM backend client with guided JSON support."""

    def supports_structured_json(self) -> bool:
        return True

    def make_request(self, request: LLMRequest) -> LLMResponse:
        """Make request to vLLM server with optional structured output."""
        start_time = time.time()

        # Build messages
        messages = []
        if request.system_prompt:
            messages.append({"role": "system", "content": request.system_prompt})
        messages.append({"role": "user", "content": request.content})

        # Base request data
        data = {
            "model": self.model,
            "messages": messages,
            "temperature": request.temperature or 0.1,
            "max_tokens": request.max_tokens or 2048,
        }

        # Add structured JSON if requested
        if request.require_json:
            # Define a flexible JSON schema for structured output
            json_schema = {
                "type": "object",
                "properties": {
                    "analysis": {"type": "string"},
                    "result": {"type": "string"},
                    "score": {"type": "number"},
                    "compliant": {"type": "boolean"},
                    "violations": {"type": "array", "items": {"type": "string"}},
                    "reasoning": {"type": "string"},
                    "suggestions": {"type": "array", "items": {"type": "string"}},
                    "severity": {"type": "string"},
                    "confidence": {"type": "number"},
                },
                "additionalProperties": True,  # Allow flexibility for different tasks
            }

            data["extra_body"] = {
                "guided_json": json_schema,
                "guided_decoding_backend": "xgrammar",
            }

        try:
            # Make HTTP request
            req = Request(
                f"{self.api_url}/v1/chat/completions",
                data=json.dumps(data).encode("utf-8"),
                headers={"Content-Type": "application/json"},
            )

            with urlopen(req, timeout=30) as response:
                result = json.loads(response.read().decode("utf-8"))
                content = result["choices"][0]["message"]["content"]

                # Parse JSON if requested
                parsed_json = None
                if request.require_json:
                    try:
                        parsed_json = json.loads(content)
                    except json.JSONDecodeError as e:
                        logger.warning(f"Failed to parse JSON from vLLM: {e}")

                return LLMResponse(
                    content=content,
                    role_used=LLMRole.FAST,  # Will be corrected by orchestrator
                    backend_used=LLMBackend.VLLM,
                    processing_time=time.time() - start_time,
                    parsed_json=parsed_json,
                    raw_response=result,
                )

        except Exception as e:
            logger.error(f"vLLM request failed: {e}")
            raise


class LlamaCppClient(LLMBackendClient):
    """llama.cpp backend client with structured JSON support."""

    def supports_structured_json(self) -> bool:
        return True

    def make_request(self, request: LLMRequest) -> LLMResponse:
        """Make request to llama.cpp server with optional JSON mode."""
        start_time = time.time()

        # Build messages for chat completions API
        messages = []
        if request.system_prompt:
            messages.append({"role": "system", "content": request.system_prompt})
        messages.append({"role": "user", "content": request.content})

        # Base request data
        data = {
            "model": self.model,
            "messages": messages,
            "temperature": request.temperature or 0.1,
            "max_tokens": request.max_tokens or 2048,
        }

        # Add JSON mode if requested
        if request.require_json:
            data["response_format"] = {"type": "json_object"}

            # Enhance prompt for JSON mode
            json_instruction = "\n\nRespond with valid JSON only. No other text."
            if messages:
                messages[-1]["content"] += json_instruction

        try:
            # Make HTTP request
            req = Request(
                f"{self.api_url}/v1/chat/completions",
                data=json.dumps(data).encode("utf-8"),
                headers={"Content-Type": "application/json"},
            )

            with urlopen(req, timeout=30) as response:
                result = json.loads(response.read().decode("utf-8"))
                content = result["choices"][0]["message"]["content"]

                # Parse JSON if requested
                parsed_json = None
                if request.require_json:
                    try:
                        parsed_json = json.loads(content)
                    except json.JSONDecodeError as e:
                        logger.warning(f"Failed to parse JSON from llama.cpp: {e}")

                return LLMResponse(
                    content=content,
                    role_used=LLMRole.FAST,  # Will be corrected by orchestrator
                    backend_used=LLMBackend.LLAMACPP,
                    processing_time=time.time() - start_time,
                    parsed_json=parsed_json,
                    raw_response=result,
                )

        except Exception as e:
            logger.error(f"llama.cpp request failed: {e}")
            raise


class OpenAIClient(LLMBackendClient):
    """OpenAI backend client with structured outputs."""

    def supports_structured_json(self) -> bool:
        return True

    def make_request(self, request: LLMRequest) -> LLMResponse:
        """Make request to OpenAI API with structured outputs."""
        start_time = time.time()

        # Build messages
        messages = []
        if request.system_prompt:
            messages.append({"role": "system", "content": request.system_prompt})
        messages.append({"role": "user", "content": request.content})

        # Base request data
        data = {
            "model": self.model,
            "messages": messages,
            "temperature": request.temperature or 0.1,
            "max_tokens": request.max_tokens or 2048,
        }

        # Add structured outputs if requested
        if request.require_json:
            data["response_format"] = {"type": "json_object"}

            # Enhance prompt for JSON mode
            json_instruction = "\n\nRespond with valid JSON only."
            if messages:
                messages[-1]["content"] += json_instruction

        try:
            # Build headers with API key
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}",
            }

            # Make HTTP request
            req = Request(
                f"{self.api_url}/v1/chat/completions",
                data=json.dumps(data).encode("utf-8"),
                headers=headers,
            )

            with urlopen(req, timeout=30) as response:
                result = json.loads(response.read().decode("utf-8"))
                content = result["choices"][0]["message"]["content"]

                # Parse JSON if requested
                parsed_json = None
                if request.require_json:
                    try:
                        parsed_json = json.loads(content)
                    except json.JSONDecodeError as e:
                        logger.warning(f"Failed to parse JSON from OpenAI: {e}")

                return LLMResponse(
                    content=content,
                    role_used=LLMRole.FAST,  # Will be corrected by orchestrator
                    backend_used=LLMBackend.OPENAI,
                    processing_time=time.time() - start_time,
                    parsed_json=parsed_json,
                    raw_response=result,
                )

        except Exception as e:
            logger.error(f"OpenAI request failed: {e}")
            raise


class LLMOrchestrator:
    """Main orchestrator for managing dual LLM setup with configurable backends."""

    def __init__(
        self,
        fast_client: Optional[LLMBackendClient] = None,
        orchestrator_client: Optional[LLMBackendClient] = None,
        context_threshold: int = 3000,
    ):
        self.fast_client = fast_client
        self.orchestrator_client = orchestrator_client
        self.context_threshold = context_threshold

    @classmethod
    def from_config(cls, config: Dict[str, Any]) -> "LLMOrchestrator":
        """Create orchestrator from configuration dict."""
        llm_config = config.get("llm", {})

        # Extract context threshold
        context_threshold = llm_config.get("context_threshold", 3000)

        # Build fast LLM client
        fast_client = None
        if llm_config.get("fast_api_url"):
            fast_config = {
                "api_url": llm_config["fast_api_url"],
                "model": llm_config.get("fast_model"),
                "api_key": llm_config.get("fast_api_key") or os.getenv("FAST_LLM_API_KEY"),
                "backend": llm_config.get("fast_backend", "vllm"),
            }
            fast_client = cls._create_client(fast_config)

        # Build orchestrator LLM client
        orchestrator_client = None
        if llm_config.get("orchestrator_api_url"):
            orchestrator_config = {
                "api_url": llm_config["orchestrator_api_url"],
                "model": llm_config.get("orchestrator_model"),
                "api_key": llm_config.get("orchestrator_api_key")
                or os.getenv("ORCHESTRATOR_LLM_API_KEY"),
                "backend": llm_config.get("orchestrator_backend", "llamacpp"),
            }
            orchestrator_client = cls._create_client(orchestrator_config)

        return cls(fast_client, orchestrator_client, context_threshold)

    @staticmethod
    def _create_client(config: Dict[str, Any]) -> LLMBackendClient:
        """Create appropriate backend client based on configuration."""
        backend = config.get("backend", "vllm").lower()

        if backend == "vllm":
            return VLLMClient(config)
        elif backend == "llamacpp":
            return LlamaCppClient(config)
        elif backend == "openai":
            return OpenAIClient(config)
        else:
            raise ValueError(f"Unsupported backend: {backend}")

    def select_llm_role(self, request: LLMRequest) -> LLMRole:
        """Select appropriate LLM role based on request characteristics."""
        content_size = len(request.content)

        # Size-based routing
        if content_size > self.context_threshold:
            return LLMRole.ORCHESTRATOR

        # Task-based routing
        complex_tasks = [
            "architecture",
            "planning",
            "summarization",
            "multi_file",
            "compliance",
            "analysis",
            "refactoring",
            "validation",
        ]
        if any(task in request.task_type.lower() for task in complex_tasks):
            return LLMRole.ORCHESTRATOR

        return LLMRole.FAST

    def process_request(self, request: LLMRequest) -> LLMResponse:
        """Process request using appropriate LLM with intelligent fallback."""
        selected_role = self.select_llm_role(request)

        # Determine primary and fallback clients
        primary_client = None
        fallback_client = None
        primary_role = selected_role
        fallback_role = None

        if selected_role == LLMRole.FAST:
            if self.fast_client:
                primary_client = self.fast_client
                primary_role = LLMRole.FAST
            if self.orchestrator_client:
                fallback_client = self.orchestrator_client
                fallback_role = LLMRole.ORCHESTRATOR
        else:  # ORCHESTRATOR
            if self.orchestrator_client:
                primary_client = self.orchestrator_client
                primary_role = LLMRole.ORCHESTRATOR
            if self.fast_client:
                fallback_client = self.fast_client
                fallback_role = LLMRole.FAST

        # Check if we have any clients configured
        if not primary_client and not fallback_client:
            raise ValueError("No LLM clients configured")

        # If no primary, use fallback as primary
        if not primary_client:
            primary_client = fallback_client
            primary_role = fallback_role
            fallback_client = None
            fallback_role = None

        # Attempt primary request
        last_exception = None
        try:
            logger.debug(f"Attempting primary request with {primary_role.value if primary_role else 'unknown'} LLM")
            response = primary_client.make_request(request)
            if primary_role:
                response.role_used = primary_role

            # Validate the response for structured JSON requests
            if request.require_json:
                if response.parsed_json is None:
                    raise ValueError("Primary LLM failed to produce valid JSON")
                # Check if JSON has required compliance fields
                if request.task_type == "compliance_assessment":
                    required_fields = {"score", "compliant", "violations", "reasoning"}
                    if not all(field in response.parsed_json for field in required_fields):
                        raise ValueError("Primary LLM JSON missing required compliance fields")

            logger.debug(f"Primary {primary_role.value if primary_role else 'unknown'} LLM request successful")
            return response

        except Exception as e:
            logger.warning(f"Primary {primary_role.value if primary_role else 'unknown'} LLM failed: {e}")
            last_exception = e

        # Try fallback if available
        if fallback_client:
            try:
                logger.info(f"Falling back to {fallback_role.value if fallback_role else 'unknown'} LLM")
                response = fallback_client.make_request(request)
                if fallback_role:
                    response.role_used = fallback_role

                # Validate fallback response for structured JSON requests
                if request.require_json:
                    if response.parsed_json is None:
                        raise ValueError("Fallback LLM failed to produce valid JSON")
                    # Check if JSON has required compliance fields
                    if request.task_type == "compliance_assessment":
                        required_fields = {"score", "compliant", "violations", "reasoning"}
                        if not all(field in response.parsed_json for field in required_fields):
                            raise ValueError("Fallback LLM JSON missing required compliance fields")

                logger.info(f"Fallback {fallback_role.value if fallback_role else 'unknown'} LLM request successful")
                return response

            except Exception as fallback_error:
                logger.error(f"Fallback {fallback_role.value if fallback_role else 'unknown'} LLM also failed: {fallback_error}")
                # Create composite error message
                error_msg = (
                    f"Both LLMs failed. Primary ({primary_role.value if primary_role else 'unknown'}): {last_exception}. "
                    f"Fallback ({fallback_role.value if fallback_role else 'unknown'}): {fallback_error}"
                )
                raise RuntimeError(error_msg) from fallback_error
        else:
            # No fallback available, re-raise original error
            logger.error(
                f"No fallback available, primary {primary_role.value if primary_role else 'unknown'} LLM failed: {last_exception}"
            )
            raise RuntimeError(
                f"Primary {primary_role.value if primary_role else 'unknown'} LLM failed and no fallback configured: {last_exception}"
            ) from last_exception


# Factory function
def create_llm_orchestrator(config: Dict[str, Any]) -> LLMOrchestrator:
    """Factory function to create LLM orchestrator from configuration."""
    return LLMOrchestrator.from_config(config)


# Export all public interfaces
__all__ = [
    "LLMOrchestrator",
    "LLMRequest",
    "LLMResponse",
    "LLMRole",
    "LLMBackend",
    "LLMBackendClient",
    "VLLMClient",
    "LlamaCppClient",
    "OpenAIClient",
    "create_llm_orchestrator",
]
```

---
### File: src/vibelint/llm/llm_retry.py

```python
"""
Generalized LLM retry pattern with few-shot learning.

Provides reusable components for robust LLM interactions that can recover
from parsing failures using few-shot examples and progressive retry strategies.

vibelint/src/vibelint/llm_retry.py
"""

import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from typing import Any, Callable, Dict, Generic, List, Optional, TypeVar

logger = logging.getLogger(__name__)

T = TypeVar("T")  # Type of items being processed
R = TypeVar("R")  # Type of parsed results


class RetryStrategy(Enum):
    """Available retry strategies."""

    NONE = "none"
    FEW_SHOT = "few_shot"
    PROGRESSIVE = "progressive"
    ADAPTIVE = "adaptive"


@dataclass
class RetryConfig:
    """Configuration for retry behavior."""

    max_retries: int = 2
    strategy: RetryStrategy = RetryStrategy.FEW_SHOT
    retry_threshold: int = 3  # Only retry if failures <= this number
    enable_logging: bool = True


@dataclass
class ParseResult(Generic[T, R]):
    """Result of a parsing attempt."""

    successful_items: List[T]
    failed_items: List[T]
    parsed_data: Dict[T, R]
    success_rate: float
    error_message: Optional[str] = None


class LLMRetryHandler(ABC, Generic[T, R]):
    """Abstract base class for LLM retry handling with few-shot learning."""

    def __init__(self, config: Optional[RetryConfig] = None):
        self.config = config or RetryConfig()
        self._llm_call_count = 0
        self._total_retries = 0

    @abstractmethod
    def create_initial_prompt(self, items: List[T]) -> str:
        """Create the initial prompt for processing items."""
        pass

    @abstractmethod
    def parse_response(self, items: List[T], response: str) -> ParseResult[T, R]:
        """Parse LLM response and return success/failure breakdown."""
        pass

    @abstractmethod
    def create_few_shot_examples(self) -> str:
        """Create few-shot examples showing correct format."""
        pass

    @abstractmethod
    def invoke_llm(self, prompt: str) -> str:
        """Make the actual LLM call."""
        pass

    def create_retry_prompt(
        self, failed_items: List[T], original_response: str, retry_attempt: int
    ) -> str:
        """Create retry prompt with few-shot learning."""
        examples = self.create_few_shot_examples()

        strategy_message = {
            RetryStrategy.FEW_SHOT: "Here are examples of the correct format:",
            RetryStrategy.PROGRESSIVE: f"Attempt {retry_attempt + 1}: Let's try a simpler approach.",
            RetryStrategy.ADAPTIVE: "The previous response had formatting issues. Let's be more specific:",
        }.get(self.config.strategy, "Please try again with the correct format:")

        item_details = self.format_items_for_retry(failed_items)

        return f"""The previous response didn't follow the expected format. {strategy_message}

{examples}

Now please re-analyze these specific items using the EXACT format shown above:

{item_details}

Remember: Follow the format exactly as shown in the examples."""

    def format_items_for_retry(self, items: List[T]) -> str:
        """Format items for inclusion in retry prompt. Override for custom formatting."""
        return "\n".join(f"Item {i+1}: {item}" for i, item in enumerate(items))

    def should_retry(self, failed_items: List[T], retry_attempt: int) -> bool:
        """Determine if retry should be attempted."""
        if retry_attempt >= self.config.max_retries:
            return False
        if len(failed_items) > self.config.retry_threshold:
            return False
        if self.config.strategy == RetryStrategy.NONE:
            return False
        return True

    def process_with_retry(self, items: List[T]) -> ParseResult[T, R]:
        """Process items with automatic retry on parsing failures."""

        if self.config.enable_logging:
            logger.info(
                f"Processing {len(items)} items with retry strategy: {self.config.strategy.value}"
            )

        # Initial attempt
        initial_prompt = self.create_initial_prompt(items)
        initial_response = self.invoke_llm(initial_prompt)
        self._llm_call_count += 1

        result = self.parse_response(items, initial_response)

        # Track overall results
        all_successful = list(result.successful_items)
        all_parsed_data = dict(result.parsed_data)

        # Retry loop for failed items
        retry_attempt = 0
        current_failed = result.failed_items

        while self.should_retry(current_failed, retry_attempt):
            if self.config.enable_logging:
                logger.info(
                    f"Retry attempt {retry_attempt + 1} for {len(current_failed)} failed items"
                )

            retry_prompt = self.create_retry_prompt(current_failed, initial_response, retry_attempt)
            retry_response = self.invoke_llm(retry_prompt)
            self._llm_call_count += 1
            self._total_retries += 1

            retry_result = self.parse_response(current_failed, retry_response)

            # Update overall results
            all_successful.extend(retry_result.successful_items)
            all_parsed_data.update(retry_result.parsed_data)

            # Update for next iteration
            current_failed = retry_result.failed_items
            retry_attempt += 1

            if self.config.enable_logging and retry_result.successful_items:
                success_count = len(retry_result.successful_items)
                total_retry_items = len(current_failed) + success_count
                logger.info(f"Retry succeeded for {success_count}/{total_retry_items} items")

        # Final result
        final_success_rate = len(all_successful) / len(items) if items else 0.0

        if self.config.enable_logging:
            logger.info(
                f"Processing complete: {len(all_successful)}/{len(items)} successful "
                f"({final_success_rate:.1%}), {self._total_retries} retries used"
            )

        return ParseResult(
            successful_items=all_successful,
            failed_items=current_failed,
            parsed_data=all_parsed_data,
            success_rate=final_success_rate,
        )

    def get_stats(self) -> Dict[str, Any]:
        """Get processing statistics."""
        return {
            "llm_calls": self._llm_call_count,
            "total_retries": self._total_retries,
            "config": {
                "max_retries": self.config.max_retries,
                "strategy": self.config.strategy.value,
                "retry_threshold": self.config.retry_threshold,
            },
        }


class SimpleTextRetryHandler(LLMRetryHandler[str, str]):
    """Example implementation for simple text processing."""

    def __init__(
        self,
        llm_callable: Callable[[str], str],
        format_instructions: str,
        few_shot_examples: str,
        config: Optional[RetryConfig] = None,
    ):
        super().__init__(config)
        self.llm_callable = llm_callable
        self.format_instructions = format_instructions
        self.few_shot_examples = few_shot_examples

    def create_initial_prompt(self, items: List[str]) -> str:
        return f"""{self.format_instructions}

Items to process:
{chr(10).join(f"- {item}" for item in items)}"""

    def parse_response(self, items: List[str], response: str) -> ParseResult[str, str]:
        # Simple implementation - assumes one response line per item
        lines = [line.strip() for line in response.split("\n") if line.strip()]

        successful = []
        parsed = {}

        for i, item in enumerate(items):
            if i < len(lines) and lines[i]:
                successful.append(item)
                parsed[item] = lines[i]

        failed = [item for item in items if item not in successful]
        success_rate = len(successful) / len(items) if items else 0.0

        return ParseResult(
            successful_items=successful,
            failed_items=failed,
            parsed_data=parsed,
            success_rate=success_rate,
        )

    def create_few_shot_examples(self) -> str:
        return self.few_shot_examples

    def invoke_llm(self, prompt: str) -> str:
        return self.llm_callable(prompt)


@dataclass
class FileSummary:
    """Structured file summary result."""

    purpose: str
    exports: str
    dependencies: str
    concerns: str


class FileAnalysisRetryHandler(LLMRetryHandler[Path, FileSummary]):
    """Specialized retry handler for file analysis tasks."""

    def __init__(self, llm_callable: Callable[[str], str], config: Optional[RetryConfig] = None):
        super().__init__(config)
        self.llm_callable = llm_callable

    def create_initial_prompt(self, files: List[Path]) -> str:
        """Create initial file analysis prompt."""
        batch_content = []
        for file_path in files:
            try:
                content = file_path.read_text(encoding="utf-8")
                batch_content.append(f"FILE: {file_path}\n{content[:1500]}")
            except Exception as e:
                batch_content.append(f"FILE: {file_path}\nERROR: Could not read file - {e}")

        return f"""Analyze these Python files and provide concise summaries for each:

{chr(10).join(batch_content)}

For each file, provide a summary in this format:
FILE: [filename]
PURPOSE: What this file does
EXPORTS: Key classes/functions it exports
DEPENDENCIES: What it imports/depends on
CONCERNS: Any potential issues
---
"""

    def parse_response(self, files: List[Path], response: str) -> ParseResult[Path, FileSummary]:
        """Parse file analysis response."""
        import re

        # Enhanced parsing with multiple strategies
        file_sections = re.split(r"FILE:\s*([^\n]+)", response)
        parsed_files = {}
        successful_files = []

        if len(file_sections) > 1:
            for i in range(1, len(file_sections), 2):
                if i + 1 < len(file_sections):
                    filename_part = file_sections[i].strip()
                    summary_content = file_sections[i + 1].split("---")[0].strip()

                    # Find matching file by name
                    for file_path in files:
                        if (
                            file_path.name in filename_part
                            or str(file_path) in filename_part
                            or str(file_path).replace("src/", "") in filename_part
                        ):

                            # Extract structured data
                            summary = self._extract_file_summary(summary_content)
                            if summary:
                                parsed_files[file_path] = summary
                                successful_files.append(file_path)
                            break

        failed_files = [f for f in files if f not in successful_files]
        success_rate = len(successful_files) / len(files) if files else 0.0

        return ParseResult(
            successful_items=successful_files,
            failed_items=failed_files,
            parsed_data=parsed_files,
            success_rate=success_rate,
        )

    def _extract_file_summary(self, content: str) -> Optional[FileSummary]:
        """Extract structured summary from content."""
        import re

        patterns = {
            "purpose": r"PURPOSE:\s*([^\n]+)",
            "exports": r"EXPORTS:\s*([^\n]+)",
            "dependencies": r"DEPENDENCIES:\s*([^\n]+)",
            "concerns": r"CONCERNS:\s*([^\n]+)",
        }

        extracted = {}
        for key, pattern in patterns.items():
            match = re.search(pattern, content, re.IGNORECASE)
            extracted[key] = match.group(1).strip() if match else "Not specified"

        # Only return if we got at least purpose
        if extracted["purpose"] != "Not specified":
            return FileSummary(**extracted)
        return None

    def create_few_shot_examples(self) -> str:
        """Create few-shot examples for file analysis."""
        return """EXAMPLE 1:
FILE: src/example/config.py
PURPOSE: Configuration loading and validation for the application
EXPORTS: ConfigLoader class, load_config() function
DEPENDENCIES: pathlib, yaml, logging
CONCERNS: No input validation on config file paths
---

EXAMPLE 2:
FILE: src/example/utils.py
PURPOSE: Utility functions for string manipulation and file operations
EXPORTS: sanitize_string(), read_file_safe(), write_file_atomic()
DEPENDENCIES: re, pathlib, tempfile
CONCERNS: Limited error handling in file operations
---"""

    def format_items_for_retry(self, files: List[Path]) -> str:
        """Format files for retry prompt."""
        formatted = []
        for file_path in files:
            try:
                content_preview = file_path.read_text(encoding="utf-8")[:500]
                formatted.append(f"FILE: {file_path}\nContent preview: {content_preview[:200]}...")
            except Exception:
                formatted.append(f"FILE: {file_path}\nERROR: Could not read file")

        return chr(10).join(formatted)

    def invoke_llm(self, prompt: str) -> str:
        return self.llm_callable(prompt)
```

---
### File: src/vibelint/llm/manager.py

```python
"""
Consolidated LLM system for vibelint.

Manages dual LLMs, tracing, and dynamic validator generation:
- Fast: High-speed inference for quick tasks
- Orchestrator: Large context for complex reasoning
- Dynamic: On-demand validator generation from prompts

vibelint/src/vibelint/llm.py
"""

import logging
import os
import time
from dataclasses import dataclass
from enum import Enum
# Load environment variables from .env files in order of preference:
# 1. Current working directory (.env)
# 2. User's home directory (~/.vibelint.env)
# 3. Project root directory (.env)
from pathlib import Path
from typing import Any, Dict, Optional

import requests
from dotenv import load_dotenv


def _load_env_files():
    """Load environment variables from multiple possible locations."""
    env_paths = [
        Path.cwd() / ".env",  # Current directory
        Path.home() / ".vibelint.env",  # User home directory
        Path(__file__).parent.parent.parent / ".env",  # Project root (fallback)
    ]

    for env_path in env_paths:
        if env_path.exists():
            load_dotenv(env_path)
            break


_load_env_files()

logger = logging.getLogger(__name__)

# Configuration constants
DEFAULT_CONTEXT_THRESHOLD = 3000
DEFAULT_FAST_TEMPERATURE = 0.1
DEFAULT_FAST_MAX_TOKENS = 2048
DEFAULT_ORCHESTRATOR_TEMPERATURE = 0.2
DEFAULT_ORCHESTRATOR_MAX_TOKENS = 8192
ORCHESTRATOR_TIMEOUT_SECONDS = 600
FAST_TIMEOUT_SECONDS = 30
TOKEN_ESTIMATION_DIVISOR = 4

__all__ = ["LLMRole", "LLMManager", "LLMRequest", "create_llm_manager"]


class LLMRole(Enum):
    """LLM roles for different types of tasks."""

    FAST = "fast"  # High-speed inference, small context
    ORCHESTRATOR = "orchestrator"  # Large context, complex reasoning


@dataclass
class LLMRequest:
    """Simple request specification for LLM processing."""

    content: str
    task_type: str
    max_tokens: Optional[int] = None
    temperature: Optional[float] = None
    json_mode: bool = False  # Request JSON-formatted response


class LLMManager:
    """Simple manager for dual LLM setup."""

    def __init__(self, config: Dict[str, Any]):
        """Initialize with vibelint configuration.

        Args:
            config: Vibelint configuration dictionary with [tool.vibelint.llm] section
        """
        llm_config = config.get("llm", {})

        # Fast LLM configuration with fallback logic
        self.fast_config = self._build_llm_config(
            llm_config,
            "fast",
            {
                "temperature": DEFAULT_FAST_TEMPERATURE,
                "max_tokens": DEFAULT_FAST_MAX_TOKENS,
                "api_key_env": "FAST_LLM_API_KEY",
            },
        )

        # Orchestrator LLM configuration with fallback logic
        self.orchestrator_config = self._build_llm_config(
            llm_config,
            "orchestrator",
            {
                "temperature": DEFAULT_ORCHESTRATOR_TEMPERATURE,
                "max_tokens": DEFAULT_ORCHESTRATOR_MAX_TOKENS,
                "api_key_env": "ORCHESTRATOR_LLM_API_KEY",
            },
        )

        # Routing configuration
        self.context_threshold = llm_config.get("context_threshold", DEFAULT_CONTEXT_THRESHOLD)
        self.enable_fallback = llm_config.get("enable_fallback", False)

        # Session for HTTP requests
        self.session = requests.Session()
        # Note: timeout is set per-request rather than on session

    def _build_llm_config(
        self, llm_config: Dict[str, Any], role: str, defaults: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Build LLM configuration with intelligent fallback logic.

        Supports multiple deployment scenarios:
        1. Two separate endpoints (fast_api_url + orchestrator_api_url)
        2. Single endpoint, two models (api_url + fast_model + orchestrator_model)
        3. Single endpoint, single model (api_url + model)
        4. Mixed scenarios with role-specific overrides
        """
        config = {}

        # Priority order for API URL:
        # 1. Role-specific URL (fast_api_url/orchestrator_api_url)
        # 2. Generic URL (api_url)
        config["api_url"] = llm_config.get(f"{role}_api_url") or llm_config.get("api_url")

        # Priority order for model:
        # 1. Role-specific model (fast_model/orchestrator_model)
        # 2. Generic model (model)
        config["model"] = llm_config.get(f"{role}_model") or llm_config.get("model")

        # Priority order for API key:
        # 1. Role-specific env var (FAST_LLM_API_KEY/ORCHESTRATOR_LLM_API_KEY)
        # 2. Generic env var (LLM_API_KEY)
        # 3. OpenAI fallback (OPENAI_API_KEY)
        config["api_key"] = (
            os.getenv(defaults["api_key_env"])
            or os.getenv("LLM_API_KEY")
            or os.getenv("OPENAI_API_KEY")
        )

        # Role-specific settings with fallbacks
        config["temperature"] = (
            llm_config.get(f"{role}_temperature")
            or llm_config.get("temperature")
            or defaults["temperature"]
        )

        config["max_tokens"] = (
            llm_config.get(f"{role}_max_tokens")
            or llm_config.get("max_tokens")
            or defaults["max_tokens"]
        )

        return config

    def select_llm(self, request: LLMRequest) -> LLMRole:
        """Select appropriate LLM based on request characteristics.

        Simple routing logic:
        - Use orchestrator for large content (>context_threshold)
        - Use orchestrator for complex tasks (architecture, planning)
        - Use fast for everything else
        """
        content_size = len(request.content)

        # Size-based routing
        if content_size > self.context_threshold:
            return LLMRole.ORCHESTRATOR

        # Task-based routing
        complex_tasks = ["architecture", "planning", "summarization", "multi_file"]
        if any(task in request.task_type.lower() for task in complex_tasks):
            return LLMRole.ORCHESTRATOR

        return LLMRole.FAST

    async def process_request(self, request: LLMRequest) -> Dict[str, Any]:
        """Process request using the appropriate LLM."""
        selected_llm = self.select_llm(request)

        # Check if required LLM is configured
        if selected_llm == LLMRole.FAST and not self.fast_config.get("api_url"):
            raise ValueError(
                f"Fast LLM required for task '{request.task_type}' but not configured. "
                f"Add [tool.vibelint.llm] fast_api_url and fast_model to pyproject.toml"
            )
        elif selected_llm == LLMRole.ORCHESTRATOR and not self.orchestrator_config.get("api_url"):
            raise ValueError(
                f"Orchestrator LLM required for task '{request.task_type}' but not configured. "
                f"Add [tool.vibelint.llm] orchestrator_api_url and orchestrator_model to pyproject.toml"
            )

        try:
            if selected_llm == LLMRole.FAST:
                return await self._call_fast_llm(request)
            else:
                return await self._call_orchestrator_llm(request)

        except (requests.exceptions.RequestException, ValueError, KeyError) as e:
            # Configured but unavailable - fail fast
            logger.error(
                f"{selected_llm.value} LLM configured but unavailable - aborting analysis: {e}"
            )
            raise RuntimeError(
                f"LLM analysis failed: {selected_llm.value} model configured but unavailable. "
                f"Check model server status and network connectivity. Error: {e}"
            ) from e

    async def _call_fast_llm(self, request: LLMRequest) -> Dict[str, Any]:
        """Call the fast LLM."""
        return await self._make_api_call(self.fast_config, request, LLMRole.FAST)

    async def _call_orchestrator_llm(self, request: LLMRequest) -> Dict[str, Any]:
        """Call the orchestrator (large context) LLM."""
        return await self._make_api_call(self.orchestrator_config, request, LLMRole.ORCHESTRATOR)

    async def _make_api_call(
        self, llm_config: Dict[str, Any], request: LLMRequest, role: LLMRole
    ) -> Dict[str, Any]:
        """Make API call to specified LLM."""
        start_time = time.time()

        if not llm_config.get("api_url"):
            raise ValueError(f"No API URL configured for {role.value} LLM")

        url = f"{llm_config['api_url'].rstrip('/')}/v1/chat/completions"

        headers = {}
        if llm_config.get("api_key"):
            headers["Authorization"] = f"Bearer {llm_config['api_key']}"

        payload = {
            "model": llm_config["model"],
            "messages": [{"role": "user", "content": request.content}],
            "temperature": request.temperature or llm_config["temperature"],
            "max_tokens": request.max_tokens or llm_config["max_tokens"],
            "stream": False,
        }

        # Add JSON mode if requested
        if request.json_mode:
            payload["response_format"] = {"type": "json_object"}

        # Debug: Log the request details
        logger.info(f"LLM Request: {role.value} to {url}")
        logger.info(f"Model: {payload['model']}, Max tokens: {payload['max_tokens']}")

        # Set timeout based on LLM role - orchestrator needs more time for large prompts
        timeout_seconds = (
            ORCHESTRATOR_TIMEOUT_SECONDS if role == LLMRole.ORCHESTRATOR else FAST_TIMEOUT_SECONDS
        )

        response = self.session.post(url, json=payload, headers=headers, timeout=timeout_seconds)
        response.raise_for_status()

        data = response.json()
        duration = time.time() - start_time

        if "choices" not in data or not data["choices"]:
            raise ValueError("Invalid LLM response format")

        return {
            "content": data["choices"][0]["message"]["content"],
            "llm_used": role.value,
            "duration_seconds": duration,
            "input_tokens": len(request.content) // TOKEN_ESTIMATION_DIVISOR,  # Rough estimate
            "success": True,
        }

    def is_llm_available(self, role: LLMRole) -> bool:
        """Check if a specific LLM is configured and available."""
        if role == LLMRole.FAST:
            return bool(self.fast_config.get("api_url") and self.fast_config.get("model"))
        else:
            return bool(
                self.orchestrator_config.get("api_url") and self.orchestrator_config.get("model")
            )

    def get_available_features(self) -> Dict[str, bool]:
        """Get which AI features are available based on LLM configuration."""
        fast_available = self.is_llm_available(LLMRole.FAST)
        orchestrator_available = self.is_llm_available(LLMRole.ORCHESTRATOR)
        any_llm_available = fast_available or orchestrator_available

        return {
            # LLM-powered features
            "architecture_analysis": orchestrator_available,  # Requires orchestrator LLM
            "docstring_generation": fast_available,  # Can use fast LLM
            "code_smell_detection": fast_available,  # Can use fast LLM
            "coverage_assessment": orchestrator_available,  # Requires orchestrator LLM
            "llm_validation": any_llm_available,  # Any LLM works
            # Embedding-only features (no LLM required)
            "semantic_similarity": True,  # Always available (uses local embeddings)
            "embedding_clustering": True,  # Always available (uses local embeddings)
            "duplicate_detection": True,  # Always available (uses local embeddings)
        }

    def get_status(self) -> Dict[str, bool | int | Dict[str, bool]]:
        """Get status of both LLMs."""
        return {
            "fast_configured": self.is_llm_available(LLMRole.FAST),
            "orchestrator_configured": self.is_llm_available(LLMRole.ORCHESTRATOR),
            "context_threshold": self.context_threshold,
            "fallback_enabled": False,  # Always disabled for predictable behavior
            "available_features": self.get_available_features(),
        }


def create_llm_manager(config: Dict[str, Any]) -> Optional[LLMManager]:
    """Create LLM manager from vibelint configuration.

    Always returns an LLMManager instance, even if no LLMs are configured.
    This allows embedding-only analysis to work without LLM endpoints.
    """
    if "llm" not in config:
        logger.info("No LLM configuration found - embedding-only analysis will be available")
        # Create empty config for embedding-only mode
        config = {"llm": {}}

    return LLMManager(config)
```

---
### File: src/vibelint/multi_representation_analyzer.py

```python
"""
Multi-Representation Architecture Analyzer

Creates a unified view of software quality using three reinforcing representations:
1. Filesystem Representation - hierarchical structure, module organization
2. Vector Representation - semantic similarity, code patterns via embeddings
3. Graph Representation - dependencies, execution flow, data flow

This enables autonomous improvement by understanding software at multiple levels.
"""

import ast
import json
from collections import defaultdict
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List

import networkx as nx
import numpy as np


@dataclass
class SoftwareQualityMetrics:
    """Comprehensive software quality metrics across all representations."""

    # SOLID Principle Violations
    srp_violations: List[Dict[str, Any]] = field(default_factory=list)
    ocp_violations: List[Dict[str, Any]] = field(default_factory=list)
    lsp_violations: List[Dict[str, Any]] = field(default_factory=list)
    isp_violations: List[Dict[str, Any]] = field(default_factory=list)
    dip_violations: List[Dict[str, Any]] = field(default_factory=list)

    # Code Quality
    cyclomatic_complexity: Dict[str, int] = field(default_factory=dict)
    cognitive_complexity: Dict[str, int] = field(default_factory=dict)
    coupling_metrics: Dict[str, float] = field(default_factory=dict)
    cohesion_metrics: Dict[str, float] = field(default_factory=dict)

    # Pythonic Practices
    naming_violations: List[Dict[str, Any]] = field(default_factory=list)
    type_hint_coverage: float = 0.0
    docstring_coverage: float = 0.0
    error_handling_patterns: Dict[str, int] = field(default_factory=dict)

    # Architecture Quality
    module_organization_score: float = 0.0
    abstraction_quality: float = 0.0
    testability_score: float = 0.0

    # Cross-representation insights
    filesystem_vector_alignment: float = 0.0
    vector_graph_consistency: float = 0.0
    graph_filesystem_coherence: float = 0.0


@dataclass
class ImprovementOpportunity:
    """A specific improvement opportunity with context from all representations."""

    opportunity_id: str
    category: str  # "solid", "complexity", "pythonic", "architecture"
    severity: str  # "high", "medium", "low"

    # Filesystem context
    file_path: str
    line_number: int
    affected_modules: List[str]

    # Vector context
    similar_code_patterns: List[str]
    semantic_clusters: List[str]

    # Graph context
    dependency_impact: List[str]
    execution_criticality: float

    # Improvement suggestion
    current_state: str
    target_state: str
    implementation_steps: List[str]
    estimated_impact: Dict[str, float]
    risk_assessment: str


class FilesystemRepresentation:
    """Analyzes project structure and organization from filesystem perspective."""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.module_tree = {}
        self.import_graph = nx.DiGraph()

    def analyze_project_structure(self) -> Dict[str, Any]:
        """Analyze filesystem organization for architectural quality."""
        analysis = {
            "module_hierarchy": self._build_module_hierarchy(),
            "package_organization": self._analyze_package_organization(),
            "import_structure": self._analyze_import_structure(),
            "naming_conventions": self._analyze_naming_conventions(),
            "file_size_distribution": self._analyze_file_sizes(),
            "directory_coupling": self._calculate_directory_coupling(),
        }

        return analysis

    def _build_module_hierarchy(self) -> Dict[str, Any]:
        """Build hierarchical representation of modules."""
        hierarchy = {}

        for python_file in self.project_root.rglob("*.py"):
            if python_file.name.startswith("."):
                continue

            relative_path = python_file.relative_to(self.project_root)
            parts = relative_path.parts

            current = hierarchy
            for part in parts[:-1]:  # Directories
                if part not in current:
                    current[part] = {"type": "directory", "children": {}}
                current = current[part]["children"]

            # File
            filename = parts[-1]
            current[filename] = {
                "type": "file",
                "path": str(python_file),
                "size_lines": len(python_file.read_text().splitlines()),
                "functions": self._extract_functions(python_file),
                "classes": self._extract_classes(python_file),
                "imports": self._extract_imports(python_file),
            }

        return hierarchy

    def _analyze_package_organization(self) -> Dict[str, Any]:
        """Analyze package organization quality."""
        packages = []

        for init_file in self.project_root.rglob("__init__.py"):
            package_dir = init_file.parent
            package_name = str(package_dir.relative_to(self.project_root))

            # Analyze package cohesion
            python_files = list(package_dir.glob("*.py"))
            if len(python_files) <= 1:
                continue

            cohesion_score = self._calculate_package_cohesion(python_files)

            packages.append(
                {
                    "name": package_name,
                    "file_count": len(python_files),
                    "cohesion_score": cohesion_score,
                    "has_clear_purpose": self._has_clear_package_purpose(package_dir),
                    "follows_naming_convention": self._follows_package_naming(package_name),
                }
            )

        return {
            "packages": packages,
            "average_cohesion": np.mean([p["cohesion_score"] for p in packages]) if packages else 0,
            "organization_violations": self._find_organization_violations(packages),
        }

    def _analyze_import_structure(self) -> Dict[str, Any]:
        """Analyze import dependencies for coupling analysis."""
        import_graph = nx.DiGraph()
        circular_imports = []

        for python_file in self.project_root.rglob("*.py"):
            if python_file.name.startswith("."):
                continue

            module_name = (
                str(python_file.relative_to(self.project_root)).replace(".py", "").replace("/", ".")
            )
            imports = self._extract_imports(python_file)

            for imported_module in imports:
                if imported_module.startswith("."):  # Relative import
                    continue

                import_graph.add_edge(module_name, imported_module)

        # Find circular dependencies
        try:
            cycles = list(nx.simple_cycles(import_graph))
            circular_imports = [cycle for cycle in cycles if len(cycle) > 1]
        except:
            pass

        return {
            "import_graph": import_graph,
            "circular_imports": circular_imports,
            "coupling_metrics": self._calculate_import_coupling(import_graph),
            "dependency_depth": self._calculate_dependency_depth(import_graph),
        }

    def _extract_functions(self, file_path: Path) -> List[Dict[str, Any]]:
        """Extract function definitions with metrics."""
        try:
            source = file_path.read_text()
            tree = ast.parse(source)
            functions = []

            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    functions.append(
                        {
                            "name": node.name,
                            "line_number": node.lineno,
                            "args_count": len(node.args.args),
                            "has_docstring": ast.get_docstring(node) is not None,
                            "has_type_hints": self._has_type_hints(node),
                            "cyclomatic_complexity": self._calculate_cyclomatic_complexity(node),
                        }
                    )

            return functions
        except Exception:
            return []

    def _extract_classes(self, file_path: Path) -> List[Dict[str, Any]]:
        """Extract class definitions with metrics."""
        try:
            source = file_path.read_text()
            tree = ast.parse(source)
            classes = []

            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    methods = [n for n in node.body if isinstance(n, ast.FunctionDef)]

                    classes.append(
                        {
                            "name": node.name,
                            "line_number": node.lineno,
                            "method_count": len(methods),
                            "has_docstring": ast.get_docstring(node) is not None,
                            "inheritance_depth": len(node.bases),
                            "follows_naming_convention": node.name[0].isupper(),
                        }
                    )

            return classes
        except Exception:
            return []

    def _extract_imports(self, file_path: Path) -> List[str]:
        """Extract import statements."""
        try:
            source = file_path.read_text()
            tree = ast.parse(source)
            imports = []

            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        imports.append(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        imports.append(node.module)

            return imports
        except Exception:
            return []

    def _calculate_cyclomatic_complexity(self, node: ast.FunctionDef) -> int:
        """Calculate cyclomatic complexity for a function."""
        complexity = 1  # Base complexity

        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.Try, ast.With)):
                complexity += 1
            elif isinstance(child, ast.BoolOp):
                complexity += len(child.values) - 1
            elif isinstance(child, ast.ExceptHandler):
                complexity += 1

        return complexity

    def _has_type_hints(self, node: ast.FunctionDef) -> bool:
        """Check if function has type hints."""
        return node.returns is not None or any(arg.annotation is not None for arg in node.args.args)

    def _calculate_package_cohesion(self, python_files: List[Path]) -> float:
        """Calculate package cohesion based on shared dependencies."""
        if len(python_files) <= 1:
            return 1.0

        all_imports = set()
        file_imports = []

        for file_path in python_files:
            imports = set(self._extract_imports(file_path))
            file_imports.append(imports)
            all_imports.update(imports)

        if not all_imports:
            return 0.5  # Neutral score for packages with no external dependencies

        # Calculate Jaccard similarity between files
        similarities = []
        for i in range(len(file_imports)):
            for j in range(i + 1, len(file_imports)):
                intersection = len(file_imports[i] & file_imports[j])
                union = len(file_imports[i] | file_imports[j])
                if union > 0:
                    similarities.append(intersection / union)

        return np.mean(similarities) if similarities else 0.0

    def _has_clear_package_purpose(self, package_dir: Path) -> bool:
        """Check if package has a clear, single purpose."""
        # Simple heuristic: check if package name suggests single purpose
        package_name = package_dir.name.lower()

        purpose_indicators = [
            "utils",
            "helpers",
            "core",
            "models",
            "views",
            "controllers",
            "validators",
            "parsers",
            "analyzers",
            "reporters",
            "config",
            "tests",
            "fixtures",
            "cli",
            "api",
            "db",
            "auth",
        ]

        return any(indicator in package_name for indicator in purpose_indicators)

    def _follows_package_naming(self, package_name: str) -> bool:
        """Check if package follows Python naming conventions."""
        return (
            package_name.islower()
            and "_" not in package_name
            or package_name.replace("_", "").isalnum()
        )

    def _find_organization_violations(self, packages: List[Dict[str, Any]]) -> List[str]:
        """Find package organization violations."""
        violations = []

        for package in packages:
            if package["cohesion_score"] < 0.3:
                violations.append(f"Low cohesion in package {package['name']}")

            if package["file_count"] > 10:
                violations.append(
                    f"Package {package['name']} may be too large ({package['file_count']} files)"
                )

            if not package["has_clear_purpose"]:
                violations.append(f"Package {package['name']} lacks clear purpose")

        return violations

    def _calculate_import_coupling(self, import_graph: nx.DiGraph) -> Dict[str, float]:
        """Calculate coupling metrics from import graph."""
        coupling = {}

        for node in import_graph.nodes():
            in_degree = import_graph.in_degree(node)  # Afferent coupling
            out_degree = import_graph.out_degree(node)  # Efferent coupling

            # Instability metric (Martin's I metric)
            total = in_degree + out_degree
            instability = out_degree / total if total > 0 else 0

            coupling[node] = {
                "afferent_coupling": in_degree,
                "efferent_coupling": out_degree,
                "instability": instability,
            }

        return coupling

    def _calculate_dependency_depth(self, import_graph: nx.DiGraph) -> Dict[str, int]:
        """Calculate maximum dependency depth for each module."""
        depths = {}

        # Find root nodes (no incoming dependencies)
        root_nodes = [n for n in import_graph.nodes() if import_graph.in_degree(n) == 0]

        for node in import_graph.nodes():
            max_depth = 0

            for root in root_nodes:
                try:
                    if nx.has_path(import_graph, root, node):
                        depth = nx.shortest_path_length(import_graph, root, node)
                        max_depth = max(max_depth, depth)
                except:
                    continue

            depths[node] = max_depth

        return depths

    def _analyze_naming_conventions(self) -> Dict[str, Any]:
        """Analyze naming convention adherence."""
        violations = []

        for python_file in self.project_root.rglob("*.py"):
            filename = python_file.name

            # Check file naming
            if not filename.islower() or not filename.replace("_", "").replace(".py", "").isalnum():
                violations.append(
                    {
                        "type": "file_naming",
                        "file": str(python_file),
                        "issue": "File name should be lowercase with underscores",
                    }
                )

            # Check function and class naming in AST
            try:
                source = python_file.read_text()
                tree = ast.parse(source)

                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef):
                        if not node.name.islower() or not node.name.replace("_", "").isalnum():
                            violations.append(
                                {
                                    "type": "function_naming",
                                    "file": str(python_file),
                                    "line": node.lineno,
                                    "name": node.name,
                                    "issue": "Function name should be lowercase with underscores",
                                }
                            )

                    elif isinstance(node, ast.ClassDef):
                        if not self._is_camel_case(node.name):
                            violations.append(
                                {
                                    "type": "class_naming",
                                    "file": str(python_file),
                                    "line": node.lineno,
                                    "name": node.name,
                                    "issue": "Class name should be CamelCase",
                                }
                            )

            except Exception:
                continue

        return {
            "violations": violations,
            "violation_count": len(violations),
            "files_checked": len(list(self.project_root.rglob("*.py"))),
        }

    def _is_camel_case(self, name: str) -> bool:
        """Check if name is CamelCase."""
        return name[0].isupper() and "_" not in name and name.isalnum()

    def _analyze_file_sizes(self) -> Dict[str, Any]:
        """Analyze file size distribution."""
        sizes = []
        large_files = []

        for python_file in self.project_root.rglob("*.py"):
            try:
                line_count = len(python_file.read_text().splitlines())
                sizes.append(line_count)

                if line_count > 500:  # Large file threshold
                    large_files.append({"file": str(python_file), "lines": line_count})
            except:
                continue

        return {
            "average_file_size": np.mean(sizes) if sizes else 0,
            "median_file_size": np.median(sizes) if sizes else 0,
            "large_files": large_files,
            "size_distribution": {
                "small (<100 lines)": len([s for s in sizes if s < 100]),
                "medium (100-300 lines)": len([s for s in sizes if 100 <= s <= 300]),
                "large (300-500 lines)": len([s for s in sizes if 300 < s <= 500]),
                "very_large (>500 lines)": len([s for s in sizes if s > 500]),
            },
        }

    def _calculate_directory_coupling(self) -> Dict[str, float]:
        """Calculate coupling between directories."""
        directory_imports = defaultdict(set)

        # Group imports by directory
        for python_file in self.project_root.rglob("*.py"):
            if python_file.name.startswith("."):
                continue

            directory = python_file.parent.relative_to(self.project_root)
            imports = self._extract_imports(python_file)

            for imp in imports:
                # Convert import to directory path
                if "." in imp:
                    import_dir = Path(imp.replace(".", "/"))
                    directory_imports[str(directory)].add(str(import_dir))

        # Calculate coupling between directories
        coupling = {}
        directories = list(directory_imports.keys())

        for dir1 in directories:
            dir1_imports = directory_imports[dir1]
            coupling_score = 0

            for dir2 in directories:
                if dir1 != dir2:
                    # Check if dir1 imports from dir2
                    if any(dir2 in imp for imp in dir1_imports):
                        coupling_score += 1

            coupling[dir1] = coupling_score / max(len(directories) - 1, 1)

        return coupling


class VectorRepresentation:
    """Analyzes code using vector embeddings for semantic understanding."""

    def __init__(self, embedding_integration):
        self.embedding_integration = embedding_integration
        self.code_vectors = {}
        self.semantic_vectors = {}
        self.similarity_clusters = {}

    async def analyze_semantic_structure(self, project_files: List[Path]) -> Dict[str, Any]:
        """Analyze semantic code structure using embeddings."""
        print("🔍 Analyzing semantic structure with embeddings...")

        # Generate embeddings for all functions and classes
        await self._generate_code_embeddings(project_files)

        analysis = {
            "semantic_clusters": await self._find_semantic_clusters(),
            "code_duplication": await self._detect_semantic_duplication(),
            "architectural_patterns": await self._identify_architectural_patterns(),
            "naming_consistency": await self._analyze_naming_consistency(),
            "functional_cohesion": await self._measure_functional_cohesion(),
        }

        return analysis

    async def _generate_code_embeddings(self, project_files: List[Path]):
        """Generate embeddings for all code elements."""
        for file_path in project_files:
            try:
                source = file_path.read_text()
                tree = ast.parse(source)

                for node in ast.walk(tree):
                    if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
                        # Extract source code for this element
                        element_source = self._extract_element_source(source, node)
                        element_id = f"{file_path.name}.{node.name}"

                        # Get code embedding
                        code_embedding = await self.embedding_integration._get_code_embedding(
                            element_source
                        )
                        if code_embedding:
                            self.code_vectors[element_id] = {
                                "embedding": code_embedding,
                                "source": element_source,
                                "type": (
                                    "function" if isinstance(node, ast.FunctionDef) else "class"
                                ),
                                "file": str(file_path),
                                "line": node.lineno,
                            }

                        # Get semantic embedding from docstring and name
                        semantic_text = self._create_semantic_text(node, element_source)
                        semantic_embedding = (
                            await self.embedding_integration._get_semantic_embedding(semantic_text)
                        )
                        if semantic_embedding:
                            self.semantic_vectors[element_id] = {
                                "embedding": semantic_embedding,
                                "text": semantic_text,
                                "file": str(file_path),
                                "line": node.lineno,
                            }

            except Exception as e:
                print(f"Failed to process {file_path}: {e}")

    def _extract_element_source(self, full_source: str, node: ast.AST) -> str:
        """Extract source code for a specific AST node."""
        lines = full_source.split("\n")
        start_line = node.lineno - 1

        # Find end line by looking for next def/class at same indentation level
        end_line = len(lines)
        if start_line < len(lines):
            base_indent = len(lines[start_line]) - len(lines[start_line].lstrip())

            for i in range(start_line + 1, len(lines)):
                line = lines[i]
                if line.strip() and len(line) - len(line.lstrip()) <= base_indent:
                    if any(line.strip().startswith(kw) for kw in ["def ", "class ", "async def "]):
                        end_line = i
                        break

        return "\n".join(lines[start_line:end_line])

    def _create_semantic_text(self, node: ast.AST, source: str) -> str:
        """Create semantic description for embedding."""
        docstring = ast.get_docstring(node) if hasattr(ast, "get_docstring") else ""

        return f"""
        Name: {node.name}
        Type: {"function" if isinstance(node, ast.FunctionDef) else "class"}
        Documentation: {docstring or "No documentation"}
        Code context: {source[:500]}
        """.strip()

    async def _find_semantic_clusters(self) -> Dict[str, List[str]]:
        """Find clusters of semantically similar code."""
        if not self.semantic_vectors:
            return {}

        # Simple clustering using cosine similarity
        clusters = defaultdict(list)
        cluster_id = 0
        processed = set()

        for element_id, vector_data in self.semantic_vectors.items():
            if element_id in processed:
                continue

            cluster_name = f"cluster_{cluster_id}"
            clusters[cluster_name].append(element_id)
            processed.add(element_id)

            # Find similar elements
            for other_id, other_data in self.semantic_vectors.items():
                if other_id in processed:
                    continue

                similarity = self._cosine_similarity(
                    vector_data["embedding"], other_data["embedding"]
                )

                if similarity > 0.8:  # High similarity threshold
                    clusters[cluster_name].append(other_id)
                    processed.add(other_id)

            cluster_id += 1

        # Filter out single-element clusters
        return {k: v for k, v in clusters.items() if len(v) > 1}

    async def _detect_semantic_duplication(self) -> List[Dict[str, Any]]:
        """Detect semantically duplicate code."""
        duplicates = []

        processed_pairs = set()

        for elem1_id, vec1_data in self.code_vectors.items():
            for elem2_id, vec2_data in self.code_vectors.items():
                if elem1_id >= elem2_id:  # Avoid duplicate comparisons
                    continue

                pair = tuple(sorted([elem1_id, elem2_id]))
                if pair in processed_pairs:
                    continue
                processed_pairs.add(pair)

                similarity = self._cosine_similarity(vec1_data["embedding"], vec2_data["embedding"])

                if similarity > 0.9:  # Very high similarity = potential duplication
                    duplicates.append(
                        {
                            "element1": elem1_id,
                            "element2": elem2_id,
                            "similarity": similarity,
                            "file1": vec1_data["file"],
                            "file2": vec2_data["file"],
                            "line1": vec1_data["line"],
                            "line2": vec2_data["line"],
                            "type": vec1_data["type"],
                        }
                    )

        return sorted(duplicates, key=lambda x: x["similarity"], reverse=True)

    async def _identify_architectural_patterns(self) -> Dict[str, List[str]]:
        """Identify architectural patterns in code."""
        patterns = {
            "factories": [],
            "singletons": [],
            "observers": [],
            "strategies": [],
            "decorators": [],
            "adapters": [],
        }

        # Simple pattern detection using naming and structure
        for element_id, vector_data in self.code_vectors.items():
            element_name = element_id.split(".")[-1].lower()
            source = vector_data["source"].lower()

            if "factory" in element_name or "create" in element_name:
                patterns["factories"].append(element_id)
            elif "singleton" in source or "_instance" in source:
                patterns["singletons"].append(element_id)
            elif "notify" in source or "observer" in source:
                patterns["observers"].append(element_id)
            elif "strategy" in element_name or "algorithm" in source:
                patterns["strategies"].append(element_id)
            elif "decorator" in element_name or "@" in source:
                patterns["decorators"].append(element_id)
            elif "adapter" in element_name or "convert" in element_name:
                patterns["adapters"].append(element_id)

        return {k: v for k, v in patterns.items() if v}

    async def _analyze_naming_consistency(self) -> Dict[str, Any]:
        """Analyze naming consistency using semantic similarity."""
        naming_issues = []

        # Group by semantic similarity
        semantic_groups = await self._find_semantic_clusters()

        for cluster_name, elements in semantic_groups.items():
            if len(elements) < 2:
                continue

            # Check if similar functions have consistent naming patterns
            names = [elem.split(".")[-1] for elem in elements]

            # Simple heuristic: similar functions should have similar naming patterns
            prefixes = set()
            suffixes = set()

            for name in names:
                if "_" in name:
                    parts = name.split("_")
                    prefixes.add(parts[0])
                    suffixes.add(parts[-1])

            if len(prefixes) > 1 or len(suffixes) > 1:
                naming_issues.append(
                    {
                        "cluster": cluster_name,
                        "elements": elements,
                        "issue": "Inconsistent naming in semantically similar functions",
                        "different_prefixes": list(prefixes),
                        "different_suffixes": list(suffixes),
                    }
                )

        return {
            "naming_issues": naming_issues,
            "consistency_score": 1.0 - (len(naming_issues) / max(len(semantic_groups), 1)),
        }

    async def _measure_functional_cohesion(self) -> Dict[str, float]:
        """Measure functional cohesion within modules."""
        module_cohesion = {}

        # Group by file/module
        modules = defaultdict(list)
        for element_id, vector_data in self.semantic_vectors.items():
            module_name = Path(vector_data["file"]).stem
            modules[module_name].append(element_id)

        # Calculate cohesion for each module
        for module_name, elements in modules.items():
            if len(elements) < 2:
                module_cohesion[module_name] = 1.0
                continue

            # Calculate pairwise semantic similarities within module
            similarities = []
            for i in range(len(elements)):
                for j in range(i + 1, len(elements)):
                    elem1 = elements[i]
                    elem2 = elements[j]

                    vec1 = self.semantic_vectors[elem1]["embedding"]
                    vec2 = self.semantic_vectors[elem2]["embedding"]

                    similarity = self._cosine_similarity(vec1, vec2)
                    similarities.append(similarity)

            # Average similarity = cohesion measure
            module_cohesion[module_name] = np.mean(similarities) if similarities else 0.0

        return module_cohesion

    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """Calculate cosine similarity between two vectors."""
        if not vec1 or not vec2 or len(vec1) != len(vec2):
            return 0.0

        vec1 = np.array(vec1)
        vec2 = np.array(vec2)

        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)

        if norm1 == 0 or norm2 == 0:
            return 0.0

        return dot_product / (norm1 * norm2)


class GraphRepresentation:
    """Analyzes code using graph structures for dependency and flow analysis."""

    def __init__(self, dependency_graph_manager):
        self.dependency_graph_manager = dependency_graph_manager
        self.call_graph = nx.DiGraph()
        self.data_flow_graph = nx.DiGraph()
        self.control_flow_graphs = {}

    def analyze_graph_structure(self) -> Dict[str, Any]:
        """Analyze graph-based code structure."""
        analysis = {
            "dependency_metrics": self._analyze_dependency_structure(),
            "control_flow_complexity": self._analyze_control_flow(),
            "data_flow_patterns": self._analyze_data_flow(),
            "graph_quality_metrics": self._calculate_graph_quality(),
            "architectural_violations": self._detect_architectural_violations(),
        }

        return analysis

    def _analyze_dependency_structure(self) -> Dict[str, Any]:
        """Analyze dependency graph structure."""
        graph = self.dependency_graph_manager.dependency_graph

        return {
            "total_nodes": graph.number_of_nodes(),
            "total_edges": graph.number_of_edges(),
            "density": nx.density(graph),
            "strongly_connected_components": len(list(nx.strongly_connected_components(graph))),
            "weakly_connected_components": len(list(nx.weakly_connected_components(graph))),
            "average_clustering": nx.average_clustering(graph.to_undirected()),
            "longest_path": self._find_longest_dependency_path(graph),
            "circular_dependencies": self._find_circular_dependencies(graph),
        }

    def _analyze_control_flow(self) -> Dict[str, Any]:
        """Analyze control flow complexity."""
        # This would build control flow graphs for each function
        # For now, simplified version using cyclomatic complexity
        return {
            "average_complexity": 5.2,  # Placeholder
            "high_complexity_functions": [],
            "control_flow_violations": [],
        }

    def _analyze_data_flow(self) -> Dict[str, Any]:
        """Analyze data flow patterns."""
        # This would track variable usage and data dependencies
        return {"data_coupling": {}, "unused_variables": [], "data_flow_violations": []}

    def _calculate_graph_quality(self) -> Dict[str, float]:
        """Calculate overall graph quality metrics."""
        graph = self.dependency_graph_manager.dependency_graph

        if graph.number_of_nodes() == 0:
            return {"error": "empty_graph"}

        try:
            return {
                "modularity": self._calculate_modularity(graph),
                "efficiency": nx.global_efficiency(graph),
                "assortativity": nx.degree_assortativity_coefficient(graph),
                "transitivity": nx.transitivity(graph),
            }
        except Exception:
            return {"error": "calculation_failed"}

    def _calculate_modularity(self, graph: nx.DiGraph) -> float:
        """Calculate graph modularity."""
        try:
            # Convert to undirected for modularity calculation
            undirected = graph.to_undirected()
            communities = nx.community.greedy_modularity_communities(undirected)
            return nx.community.modularity(undirected, communities)
        except:
            return 0.0

    def _find_longest_dependency_path(self, graph: nx.DiGraph) -> List[str]:
        """Find the longest dependency path."""
        try:
            if nx.is_directed_acyclic_graph(graph):
                return list(nx.dag_longest_path(graph))
            else:
                # For graphs with cycles, find longest simple path
                longest = []
                for node in graph.nodes():
                    for target in graph.nodes():
                        if node != target:
                            try:
                                path = nx.shortest_path(graph, node, target)
                                if len(path) > len(longest):
                                    longest = path
                            except nx.NetworkXNoPath:
                                continue
                return longest
        except:
            return []

    def _find_circular_dependencies(self, graph: nx.DiGraph) -> List[List[str]]:
        """Find circular dependencies."""
        try:
            return list(nx.simple_cycles(graph))
        except:
            return []

    def _detect_architectural_violations(self) -> List[Dict[str, Any]]:
        """Detect architectural violations in graph structure."""
        violations = []
        graph = self.dependency_graph_manager.dependency_graph

        # Detect violations of common architectural principles

        # 1. Circular dependencies
        cycles = self._find_circular_dependencies(graph)
        for cycle in cycles:
            violations.append(
                {
                    "type": "circular_dependency",
                    "severity": "high",
                    "description": f"Circular dependency detected: {' -> '.join(cycle)}",
                    "affected_modules": cycle,
                }
            )

        # 2. High fan-out (single module depending on too many others)
        for node in graph.nodes():
            out_degree = graph.out_degree(node)
            if out_degree > 10:  # Threshold for high coupling
                violations.append(
                    {
                        "type": "high_fan_out",
                        "severity": "medium",
                        "description": f"Module {node} has high fan-out ({out_degree} dependencies)",
                        "affected_modules": [node],
                    }
                )

        # 3. High fan-in (single module being depended on by too many others)
        for node in graph.nodes():
            in_degree = graph.in_degree(node)
            if in_degree > 15:  # Threshold for high coupling
                violations.append(
                    {
                        "type": "high_fan_in",
                        "severity": "medium",
                        "description": f"Module {node} has high fan-in ({in_degree} dependents)",
                        "affected_modules": [node],
                    }
                )

        return violations


class MultiRepresentationAnalyzer:
    """
    Unified analyzer that combines filesystem, vector, and graph representations
    to provide comprehensive software quality analysis and improvement suggestions.
    """

    def __init__(self, project_root: Path, embedding_integration, dependency_graph_manager):
        self.project_root = project_root
        self.filesystem_analyzer = FilesystemRepresentation(project_root)
        self.vector_analyzer = VectorRepresentation(embedding_integration)
        self.graph_analyzer = GraphRepresentation(dependency_graph_manager)

        self.quality_metrics = SoftwareQualityMetrics()
        self.improvement_opportunities = []

    async def comprehensive_analysis(self) -> Dict[str, Any]:
        """Run comprehensive analysis across all representations."""
        print("🔍 Running comprehensive multi-representation analysis...")

        # Get all Python files
        python_files = list(self.project_root.rglob("*.py"))

        # Analyze each representation
        print("📁 Analyzing filesystem representation...")
        filesystem_analysis = self.filesystem_analyzer.analyze_project_structure()

        print("🧠 Analyzing vector representation...")
        vector_analysis = await self.vector_analyzer.analyze_semantic_structure(python_files)

        print("🕸️ Analyzing graph representation...")
        graph_analysis = self.graph_analyzer.analyze_graph_structure()

        # Cross-representation analysis
        print("🔗 Performing cross-representation analysis...")
        cross_analysis = await self._cross_representation_analysis(
            filesystem_analysis, vector_analysis, graph_analysis
        )

        # Generate improvement opportunities
        print("💡 Generating improvement opportunities...")
        self.improvement_opportunities = await self._generate_improvement_opportunities(
            filesystem_analysis, vector_analysis, graph_analysis, cross_analysis
        )

        return {
            "filesystem_analysis": filesystem_analysis,
            "vector_analysis": vector_analysis,
            "graph_analysis": graph_analysis,
            "cross_representation_analysis": cross_analysis,
            "improvement_opportunities": self.improvement_opportunities,
            "quality_metrics": self.quality_metrics,
            "summary": self._generate_analysis_summary(),
        }

    async def _cross_representation_analysis(
        self, fs_analysis, vec_analysis, graph_analysis
    ) -> Dict[str, Any]:
        """Analyze relationships between different representations."""

        # Calculate alignment between representations
        fs_vector_alignment = await self._calculate_fs_vector_alignment(fs_analysis, vec_analysis)
        vector_graph_consistency = await self._calculate_vector_graph_consistency(
            vec_analysis, graph_analysis
        )
        graph_fs_coherence = await self._calculate_graph_fs_coherence(graph_analysis, fs_analysis)

        # Store in quality metrics
        self.quality_metrics.filesystem_vector_alignment = fs_vector_alignment
        self.quality_metrics.vector_graph_consistency = vector_graph_consistency
        self.quality_metrics.graph_filesystem_coherence = graph_fs_coherence

        return {
            "representation_alignment": {
                "filesystem_vector": fs_vector_alignment,
                "vector_graph": vector_graph_consistency,
                "graph_filesystem": graph_fs_coherence,
            },
            "consistency_issues": await self._find_consistency_issues(
                fs_analysis, vec_analysis, graph_analysis
            ),
            "reinforcing_patterns": await self._find_reinforcing_patterns(
                fs_analysis, vec_analysis, graph_analysis
            ),
        }

    async def _calculate_fs_vector_alignment(self, fs_analysis, vec_analysis) -> float:
        """Calculate alignment between filesystem organization and semantic similarity."""
        # Check if files in same directory have high semantic similarity
        if not vec_analysis.get("semantic_clusters"):
            return 0.5

        alignment_scores = []

        # For each semantic cluster, check if elements are in related directories
        for cluster_name, elements in vec_analysis["semantic_clusters"].items():
            directories = set()
            for element in elements:
                if element in self.vector_analyzer.semantic_vectors:
                    file_path = self.vector_analyzer.semantic_vectors[element]["file"]
                    directories.add(str(Path(file_path).parent))

            # High alignment = semantically similar code is in same/related directories
            if len(directories) == 1:
                alignment_scores.append(1.0)  # Perfect alignment
            elif len(directories) <= 2:
                alignment_scores.append(0.7)  # Good alignment
            else:
                alignment_scores.append(0.3)  # Poor alignment

        return np.mean(alignment_scores) if alignment_scores else 0.5

    async def _calculate_vector_graph_consistency(self, vec_analysis, graph_analysis) -> float:
        """Calculate consistency between semantic similarity and dependency structure."""
        # Check if semantically similar code has similar dependency patterns
        if not vec_analysis.get("semantic_clusters"):
            return 0.5

        consistency_scores = []

        for cluster_name, elements in vec_analysis["semantic_clusters"].items():
            if len(elements) < 2:
                continue

            # Check if elements in cluster have similar dependency patterns
            dependency_patterns = []
            for element in elements:
                # Get dependencies for this element from graph
                if element in self.graph_analyzer.dependency_graph_manager.dependency_graph:
                    deps = list(
                        self.graph_analyzer.dependency_graph_manager.dependency_graph.successors(
                            element
                        )
                    )
                    dependency_patterns.append(set(deps))

            if len(dependency_patterns) >= 2:
                # Calculate similarity of dependency patterns
                avg_similarity = 0
                comparisons = 0

                for i in range(len(dependency_patterns)):
                    for j in range(i + 1, len(dependency_patterns)):
                        intersection = len(dependency_patterns[i] & dependency_patterns[j])
                        union = len(dependency_patterns[i] | dependency_patterns[j])
                        similarity = intersection / union if union > 0 else 1.0
                        avg_similarity += similarity
                        comparisons += 1

                if comparisons > 0:
                    consistency_scores.append(avg_similarity / comparisons)

        return np.mean(consistency_scores) if consistency_scores else 0.5

    async def _calculate_graph_fs_coherence(self, graph_analysis, fs_analysis) -> float:
        """Calculate coherence between graph structure and filesystem organization."""
        # Check if dependency relationships match filesystem hierarchy
        coherence_score = 0.5  # Default neutral score

        # This would check if modules that depend on each other are appropriately organized
        # in the filesystem hierarchy (e.g., core modules in core directory, utilities in utils)

        return coherence_score

    async def _find_consistency_issues(
        self, fs_analysis, vec_analysis, graph_analysis
    ) -> List[Dict[str, Any]]:
        """Find inconsistencies between representations."""
        issues = []

        # Issue: Semantically similar code scattered across filesystem
        for cluster_name, elements in vec_analysis.get("semantic_clusters", {}).items():
            directories = set()
            for element in elements:
                if element in self.vector_analyzer.semantic_vectors:
                    file_path = self.vector_analyzer.semantic_vectors[element]["file"]
                    directories.add(str(Path(file_path).parent))

            if len(directories) > 2:
                issues.append(
                    {
                        "type": "semantic_filesystem_mismatch",
                        "description": f"Semantically similar code scattered across {len(directories)} directories",
                        "affected_elements": elements,
                        "directories": list(directories),
                        "severity": "medium",
                    }
                )

        # Issue: High filesystem coupling but low semantic similarity
        # (would require more detailed analysis)

        return issues

    async def _find_reinforcing_patterns(
        self, fs_analysis, vec_analysis, graph_analysis
    ) -> List[Dict[str, Any]]:
        """Find patterns that are reinforced across multiple representations."""
        patterns = []

        # Pattern: Good package organization reinforced by semantic coherence
        for package in fs_analysis.get("package_organization", {}).get("packages", []):
            if package["cohesion_score"] > 0.7:
                patterns.append(
                    {
                        "type": "well_organized_package",
                        "description": f"Package {package['name']} shows good organization across representations",
                        "package": package["name"],
                        "evidence": {
                            "filesystem_cohesion": package["cohesion_score"],
                            "clear_purpose": package["has_clear_purpose"],
                            "naming_convention": package["follows_naming_convention"],
                        },
                        "strength": "high",
                    }
                )

        return patterns

    async def _generate_improvement_opportunities(
        self, fs_analysis, vec_analysis, graph_analysis, cross_analysis
    ) -> List[ImprovementOpportunity]:
        """Generate specific improvement opportunities based on analysis."""
        opportunities = []

        # SOLID principle violations
        opportunities.extend(
            await self._generate_solid_improvements(fs_analysis, vec_analysis, graph_analysis)
        )

        # Complexity reduction opportunities
        opportunities.extend(
            await self._generate_complexity_improvements(fs_analysis, graph_analysis)
        )

        # Pythonic improvements
        opportunities.extend(await self._generate_pythonic_improvements(fs_analysis, vec_analysis))

        # Architecture improvements
        opportunities.extend(
            await self._generate_architecture_improvements(graph_analysis, cross_analysis)
        )

        return sorted(
            opportunities, key=lambda x: self._calculate_opportunity_priority(x), reverse=True
        )

    async def _generate_solid_improvements(
        self, fs_analysis, vec_analysis, graph_analysis
    ) -> List[ImprovementOpportunity]:
        """Generate SOLID principle improvement opportunities."""
        opportunities = []

        # Single Responsibility Principle violations
        for file_info in fs_analysis.get("large_files", []):
            if file_info["lines"] > 500:
                opportunities.append(
                    ImprovementOpportunity(
                        opportunity_id=f"srp_large_file_{hash(file_info['file'])}",
                        category="solid",
                        severity="medium",
                        file_path=file_info["file"],
                        line_number=1,
                        affected_modules=[file_info["file"]],
                        similar_code_patterns=[],
                        semantic_clusters=[],
                        dependency_impact=[],
                        execution_criticality=0.5,
                        current_state=f"Large file with {file_info['lines']} lines",
                        target_state="Break into focused modules with single responsibilities",
                        implementation_steps=[
                            "Identify distinct responsibilities in the file",
                            "Extract classes/functions by responsibility",
                            "Create separate modules for each responsibility",
                            "Update imports and dependencies",
                        ],
                        estimated_impact={
                            "maintainability": 0.8,
                            "testability": 0.7,
                            "readability": 0.9,
                        },
                        risk_assessment="medium",
                    )
                )

        # Dependency Inversion Principle violations (high coupling to concrete classes)
        for violation in graph_analysis.get("architectural_violations", []):
            if violation["type"] == "high_fan_out":
                opportunities.append(
                    ImprovementOpportunity(
                        opportunity_id=f"dip_high_coupling_{hash(violation['affected_modules'][0])}",
                        category="solid",
                        severity="high",
                        file_path=violation["affected_modules"][0],
                        line_number=1,
                        affected_modules=violation["affected_modules"],
                        similar_code_patterns=[],
                        semantic_clusters=[],
                        dependency_impact=violation["affected_modules"],
                        execution_criticality=0.8,
                        current_state=violation["description"],
                        target_state="Introduce abstractions to reduce coupling",
                        implementation_steps=[
                            "Identify common interfaces in dependencies",
                            "Create abstract base classes or protocols",
                            "Refactor to depend on abstractions",
                            "Use dependency injection where appropriate",
                        ],
                        estimated_impact={
                            "maintainability": 0.9,
                            "testability": 0.8,
                            "flexibility": 0.9,
                        },
                        risk_assessment="medium",
                    )
                )

        return opportunities

    async def _generate_complexity_improvements(
        self, fs_analysis, graph_analysis
    ) -> List[ImprovementOpportunity]:
        """Generate complexity reduction opportunities."""
        opportunities = []

        # High cyclomatic complexity functions
        for file_path, file_data in fs_analysis.get("module_hierarchy", {}).items():
            if isinstance(file_data, dict) and file_data.get("type") == "file":
                for func in file_data.get("functions", []):
                    if func["cyclomatic_complexity"] > 10:
                        func_identifier = f"{file_path}_{func['name']}"
                        opportunities.append(
                            ImprovementOpportunity(
                                opportunity_id=f"complexity_function_{hash(func_identifier)}",
                                category="complexity",
                                severity="high" if func["cyclomatic_complexity"] > 15 else "medium",
                                file_path=file_data["path"],
                                line_number=func["line_number"],
                                affected_modules=[file_data["path"]],
                                similar_code_patterns=[],
                                semantic_clusters=[],
                                dependency_impact=[],
                                execution_criticality=0.7,
                                current_state=f"Function {func['name']} has complexity {func['cyclomatic_complexity']}",
                                target_state="Reduce complexity to below 10",
                                implementation_steps=[
                                    "Extract complex conditional logic into separate functions",
                                    "Use early returns to reduce nesting",
                                    "Consider using strategy pattern for complex branching",
                                    "Split function into smaller, focused functions",
                                ],
                                estimated_impact={
                                    "maintainability": 0.8,
                                    "testability": 0.9,
                                    "readability": 0.8,
                                },
                                risk_assessment="low",
                            )
                        )

        return opportunities

    async def _generate_pythonic_improvements(
        self, fs_analysis, vec_analysis
    ) -> List[ImprovementOpportunity]:
        """Generate Pythonic best practices improvements."""
        opportunities = []

        # Missing type hints
        total_functions = 0
        functions_with_hints = 0

        for file_path, file_data in fs_analysis.get("module_hierarchy", {}).items():
            if isinstance(file_data, dict) and file_data.get("type") == "file":
                for func in file_data.get("functions", []):
                    total_functions += 1
                    if func["has_type_hints"]:
                        functions_with_hints += 1

        type_hint_coverage = functions_with_hints / total_functions if total_functions > 0 else 1.0

        if type_hint_coverage < 0.8:
            opportunities.append(
                ImprovementOpportunity(
                    opportunity_id="pythonic_type_hints",
                    category="pythonic",
                    severity="medium",
                    file_path=str(self.project_root),
                    line_number=1,
                    affected_modules=["project_wide"],
                    similar_code_patterns=[],
                    semantic_clusters=[],
                    dependency_impact=[],
                    execution_criticality=0.3,
                    current_state=f"Type hint coverage: {type_hint_coverage:.1%}",
                    target_state="Achieve >80% type hint coverage",
                    implementation_steps=[
                        "Add type hints to function parameters and return values",
                        "Use typing module for complex types",
                        "Add mypy to CI/CD pipeline",
                        "Gradually improve coverage over time",
                    ],
                    estimated_impact={
                        "maintainability": 0.7,
                        "ide_support": 0.9,
                        "documentation": 0.6,
                    },
                    risk_assessment="low",
                )
            )

        # Naming convention violations
        naming_violations = fs_analysis.get("naming_conventions", {}).get("violations", [])
        if len(naming_violations) > 10:
            opportunities.append(
                ImprovementOpportunity(
                    opportunity_id="pythonic_naming",
                    category="pythonic",
                    severity="low",
                    file_path=str(self.project_root),
                    line_number=1,
                    affected_modules=["project_wide"],
                    similar_code_patterns=[],
                    semantic_clusters=[],
                    dependency_impact=[],
                    execution_criticality=0.2,
                    current_state=f"{len(naming_violations)} naming convention violations",
                    target_state="Follow PEP 8 naming conventions consistently",
                    implementation_steps=[
                        "Rename functions to use snake_case",
                        "Rename classes to use PascalCase",
                        "Rename files to use lowercase with underscores",
                        "Update all references and imports",
                    ],
                    estimated_impact={
                        "readability": 0.6,
                        "consistency": 0.8,
                        "professionalism": 0.7,
                    },
                    risk_assessment="low",
                )
            )

        return opportunities

    async def _generate_architecture_improvements(
        self, graph_analysis, cross_analysis
    ) -> List[ImprovementOpportunity]:
        """Generate architecture improvement opportunities."""
        opportunities = []

        # Circular dependencies
        circular_deps = graph_analysis.get("dependency_metrics", {}).get(
            "circular_dependencies", []
        )
        for cycle in circular_deps:
            opportunities.append(
                ImprovementOpportunity(
                    opportunity_id=f"arch_circular_dep_{hash(str(cycle))}",
                    category="architecture",
                    severity="high",
                    file_path=cycle[0] if cycle else "",
                    line_number=1,
                    affected_modules=cycle,
                    similar_code_patterns=[],
                    semantic_clusters=[],
                    dependency_impact=cycle,
                    execution_criticality=0.9,
                    current_state=f"Circular dependency: {' -> '.join(cycle)}",
                    target_state="Break circular dependency",
                    implementation_steps=[
                        "Identify the reason for the circular dependency",
                        "Extract common functionality to a shared module",
                        "Use dependency inversion to break the cycle",
                        "Consider using events or callbacks instead of direct dependencies",
                    ],
                    estimated_impact={
                        "maintainability": 0.9,
                        "testability": 0.8,
                        "modularity": 0.9,
                    },
                    risk_assessment="high",
                )
            )

        # Poor representation alignment
        if cross_analysis["representation_alignment"]["filesystem_vector"] < 0.5:
            opportunities.append(
                ImprovementOpportunity(
                    opportunity_id="arch_filesystem_semantic_mismatch",
                    category="architecture",
                    severity="medium",
                    file_path=str(self.project_root),
                    line_number=1,
                    affected_modules=["project_wide"],
                    similar_code_patterns=[],
                    semantic_clusters=[],
                    dependency_impact=[],
                    execution_criticality=0.6,
                    current_state="Filesystem organization doesn't match semantic relationships",
                    target_state="Reorganize files to group semantically related code",
                    implementation_steps=[
                        "Identify semantically related code that's scattered",
                        "Create logical package structure based on functionality",
                        "Move related modules to appropriate packages",
                        "Update imports and documentation",
                    ],
                    estimated_impact={
                        "discoverability": 0.8,
                        "maintainability": 0.7,
                        "onboarding": 0.9,
                    },
                    risk_assessment="medium",
                )
            )

        return opportunities

    def _calculate_opportunity_priority(self, opportunity: ImprovementOpportunity) -> float:
        """Calculate priority score for an improvement opportunity."""
        severity_weights = {"high": 1.0, "medium": 0.6, "low": 0.3}
        category_weights = {"solid": 1.0, "architecture": 0.9, "complexity": 0.8, "pythonic": 0.5}

        severity_score = severity_weights.get(opportunity.severity, 0.5)
        category_score = category_weights.get(opportunity.category, 0.5)
        impact_score = np.mean(list(opportunity.estimated_impact.values()))
        criticality_score = opportunity.execution_criticality

        return (
            severity_score * 0.3
            + category_score * 0.3
            + impact_score * 0.2
            + criticality_score * 0.2
        )

    def _generate_analysis_summary(self) -> Dict[str, Any]:
        """Generate summary of analysis results."""
        return {
            "total_improvement_opportunities": len(self.improvement_opportunities),
            "high_priority_opportunities": len(
                [op for op in self.improvement_opportunities if op.severity == "high"]
            ),
            "representation_alignment_score": np.mean(
                [
                    self.quality_metrics.filesystem_vector_alignment,
                    self.quality_metrics.vector_graph_consistency,
                    self.quality_metrics.graph_filesystem_coherence,
                ]
            ),
            "top_improvement_categories": self._get_top_categories(),
            "overall_quality_score": self._calculate_overall_quality_score(),
        }

    def _get_top_categories(self) -> List[str]:
        """Get top improvement categories by count."""
        category_counts = {}
        for op in self.improvement_opportunities:
            category_counts[op.category] = category_counts.get(op.category, 0) + 1

        return sorted(category_counts.keys(), key=lambda x: category_counts[x], reverse=True)

    def _calculate_overall_quality_score(self) -> float:
        """Calculate overall software quality score."""
        # Combine multiple factors into overall score
        high_priority_penalty = (
            len([op for op in self.improvement_opportunities if op.severity == "high"]) * 0.1
        )
        alignment_bonus = (
            np.mean(
                [
                    self.quality_metrics.filesystem_vector_alignment,
                    self.quality_metrics.vector_graph_consistency,
                    self.quality_metrics.graph_filesystem_coherence,
                ]
            )
            * 0.3
        )

        base_score = 0.7  # Baseline quality
        quality_score = base_score + alignment_bonus - high_priority_penalty

        return max(0.0, min(1.0, quality_score))


# Main integration function
async def analyze_vibelint_quality(
    project_root: Path, embedding_integration, dependency_graph_manager
) -> Dict[str, Any]:
    """
    Run complete multi-representation quality analysis on vibelint.
    This is the main entry point for autonomous software improvement.
    """
    print("🔍 Starting comprehensive vibelint quality analysis...")

    analyzer = MultiRepresentationAnalyzer(
        project_root, embedding_integration, dependency_graph_manager
    )

    analysis_results = await analyzer.comprehensive_analysis()

    # Export results for LLM consumption
    export_path = project_root / ".vibelint-self-improvement" / "quality_analysis.json"
    export_path.parent.mkdir(exist_ok=True)

    with open(export_path, "w") as f:
        json.dump(analysis_results, f, indent=2, default=str)

    print("📊 Quality analysis complete:")
    print(
        f"  - {analysis_results['summary']['total_improvement_opportunities']} improvement opportunities found"
    )
    print(f"  - Overall quality score: {analysis_results['summary']['overall_quality_score']:.2f}")
    print(f"  - Report saved to: {export_path}")

    return analysis_results


if __name__ == "__main__":
    # Example usage
    async def demo():
        from pathlib import Path

        from vibelint.dependency_graph_manager import DependencyGraphManager
        from vibelint.runtime_tracer import VanguardEmbeddingIntegration

        project_root = Path(__file__).parent.parent.parent
        embedding_integration = VanguardEmbeddingIntegration()
        dependency_manager = DependencyGraphManager()

        results = await analyze_vibelint_quality(
            project_root, embedding_integration, dependency_manager
        )

        print(
            f"Analysis complete! Found {len(results['improvement_opportunities'])} opportunities."
        )

    import asyncio

    asyncio.run(demo())
```

---
### File: src/vibelint/multitool_safeguards.py

```python
"""
Multitool Safeguards System

Implements judge model safeguards for multitool calls, especially for write and execute
operations. Uses claudiallm as the judge model to evaluate whether multitool operations
are safe to perform.

This addresses security concerns around batch operations that could cause system damage.
"""

import asyncio
import json
import logging
from dataclasses import dataclass
from enum import Enum
from typing import Any, Dict, List, Optional

logger = logging.getLogger(__name__)


class SafeguardDecision(Enum):
    """Judge model decision on multitool operation safety."""

    APPROVE = "approve"
    DENY = "deny"
    REQUIRE_HUMAN = "require_human"


@dataclass
class ToolCall:
    """Represents a single tool call in a multitool operation."""

    tool_name: str
    parameters: Dict[str, Any]
    description: str
    risk_level: str = "unknown"


@dataclass
class SafeguardResult:
    """Result from judge model evaluation."""

    decision: SafeguardDecision
    confidence: float
    reasoning: str
    risk_assessment: str
    alternative_suggestions: List[str]
    requires_human_approval: bool = False


class MultitoolSafeguards:
    """
    Judge model safeguards for multitool operations.

    Uses claudiallm to evaluate whether batch operations are safe to perform,
    especially for write/execute operations that could cause system damage.
    """

    # High-risk tool patterns that always require judgment
    HIGH_RISK_TOOLS = {"Write", "MultiEdit", "NotebookEdit", "Bash", "mcp__ide__executeCode"}

    # Tool combinations that are particularly dangerous
    DANGEROUS_COMBINATIONS = [
        {"Write", "Bash"},  # Writing files then executing
        {"MultiEdit", "Bash"},  # Editing multiple files then executing
        {"Write", "Write", "Write"},  # Multiple file writes
    ]

    def __init__(self, llm_client=None):
        """Initialize safeguards with optional LLM client."""
        self.llm_client = llm_client
        self._initialize_judge_model()

    def _initialize_judge_model(self):
        """Initialize the claudiallm judge model."""
        if self.llm_client is None:
            try:
                # Try to import and initialize claudiallm
                # This would be replaced with actual claudiallm initialization
                logger.info("Initializing claudiallm judge model for safeguards")
                # self.llm_client = claudiallm.Client(...)
                logger.warning("claudiallm not available - using fallback safety rules")
            except ImportError:
                logger.warning("claudiallm not available - using fallback safety rules")

    async def evaluate_multitool_operation(
        self, tool_calls: List[ToolCall], context: Optional[str] = None
    ) -> SafeguardResult:
        """
        Evaluate whether a multitool operation is safe to perform.

        Args:
            tool_calls: List of tool calls to evaluate
            context: Optional context about the operation

        Returns:
            SafeguardResult with judge decision and reasoning
        """
        # Quick safety checks first
        if len(tool_calls) == 1:
            # Single tool calls are generally safe, unless high-risk
            if tool_calls[0].tool_name in self.HIGH_RISK_TOOLS:
                return await self._evaluate_single_high_risk_tool(tool_calls[0], context)
            else:
                return SafeguardResult(
                    decision=SafeguardDecision.APPROVE,
                    confidence=0.95,
                    reasoning="Single tool call with low-risk tool",
                    risk_assessment="low",
                    alternative_suggestions=[],
                )

        # Multiple tool calls - requires more careful evaluation
        return await self._evaluate_multitool_batch(tool_calls, context)

    async def _evaluate_single_high_risk_tool(
        self, tool_call: ToolCall, context: Optional[str]
    ) -> SafeguardResult:
        """Evaluate a single high-risk tool call."""
        if self.llm_client:
            return await self._llm_judge_evaluation([tool_call], context, focus="single_high_risk")

        # Fallback rules for high-risk tools
        if tool_call.tool_name == "Bash":
            # Check for dangerous bash commands
            command = tool_call.parameters.get("command", "")
            if self._contains_dangerous_bash_patterns(command):
                return SafeguardResult(
                    decision=SafeguardDecision.DENY,
                    confidence=0.9,
                    reasoning=f"Dangerous bash command detected: {command}",
                    risk_assessment="high",
                    alternative_suggestions=["Break down into smaller, safer commands"],
                )

        return SafeguardResult(
            decision=SafeguardDecision.REQUIRE_HUMAN,
            confidence=0.8,
            reasoning=f"High-risk tool {tool_call.tool_name} requires human approval",
            risk_assessment="medium",
            alternative_suggestions=["Review the operation manually"],
            requires_human_approval=True,
        )

    async def _evaluate_multitool_batch(
        self, tool_calls: List[ToolCall], context: Optional[str]
    ) -> SafeguardResult:
        """Evaluate a batch of multiple tool calls."""
        # Check for dangerous combinations
        tool_names = {tc.tool_name for tc in tool_calls}
        for dangerous_combo in self.DANGEROUS_COMBINATIONS:
            if dangerous_combo.issubset(tool_names):
                if self.llm_client:
                    return await self._llm_judge_evaluation(
                        tool_calls, context, focus="dangerous_combination"
                    )
                else:
                    return SafeguardResult(
                        decision=SafeguardDecision.DENY,
                        confidence=0.95,
                        reasoning=f"Dangerous tool combination detected: {dangerous_combo}",
                        risk_assessment="high",
                        alternative_suggestions=[
                            "Execute tools individually with human review",
                            "Use safer alternatives where possible",
                        ],
                    )

        # Check for too many high-risk operations
        high_risk_count = sum(1 for tc in tool_calls if tc.tool_name in self.HIGH_RISK_TOOLS)
        if high_risk_count >= 3:
            return SafeguardResult(
                decision=SafeguardDecision.REQUIRE_HUMAN,
                confidence=0.85,
                reasoning=f"Too many high-risk operations in batch: {high_risk_count}",
                risk_assessment="high",
                alternative_suggestions=["Break into smaller batches"],
                requires_human_approval=True,
            )

        # Use LLM judge if available
        if self.llm_client:
            return await self._llm_judge_evaluation(tool_calls, context, focus="batch")

        # Fallback: require human approval for complex multitool operations
        return SafeguardResult(
            decision=SafeguardDecision.REQUIRE_HUMAN,
            confidence=0.7,
            reasoning="Complex multitool operation requires human review",
            risk_assessment="medium",
            alternative_suggestions=["Review each tool call individually"],
            requires_human_approval=True,
        )

    async def _llm_judge_evaluation(
        self, tool_calls: List[ToolCall], context: Optional[str], focus: str
    ) -> SafeguardResult:
        """Use claudiallm to evaluate the tool calls."""
        try:
            # Prepare prompt for judge model
            evaluation_prompt = self._build_judge_prompt(tool_calls, context, focus)

            # Call claudiallm (this would be the actual implementation)
            # response = await self.llm_client.complete(evaluation_prompt)
            # For now, simulate LLM response
            response = await self._simulate_llm_judge(evaluation_prompt, tool_calls)

            return self._parse_judge_response(response)

        except Exception as e:
            logger.error(f"Error in LLM judge evaluation: {e}")
            # Fail safe: require human approval on error
            return SafeguardResult(
                decision=SafeguardDecision.REQUIRE_HUMAN,
                confidence=0.5,
                reasoning=f"LLM judge error, requiring human review: {e}",
                risk_assessment="unknown",
                alternative_suggestions=["Manual review recommended"],
                requires_human_approval=True,
            )

    def _build_judge_prompt(
        self, tool_calls: List[ToolCall], context: Optional[str], focus: str
    ) -> str:
        """Build prompt for the judge model."""
        tool_descriptions = []
        for i, tool_call in enumerate(tool_calls, 1):
            tool_descriptions.append(
                f"{i}. {tool_call.tool_name}: {tool_call.description}\n"
                f"   Parameters: {json.dumps(tool_call.parameters, indent=2)}"
            )

        prompt = f"""You are a security judge evaluating whether a multitool operation is safe to execute.

CONTEXT: {context or 'No additional context provided'}

FOCUS: {focus}

TOOL CALLS TO EVALUATE:
{chr(10).join(tool_descriptions)}

Please evaluate this multitool operation and respond with a JSON object containing:
{{
    "decision": "approve" | "deny" | "require_human",
    "confidence": 0.0-1.0,
    "reasoning": "Clear explanation of your decision",
    "risk_assessment": "low" | "medium" | "high",
    "alternative_suggestions": ["list", "of", "alternatives"]
}}

EVALUATION CRITERIA:
- Data safety: Could this damage or corrupt files?
- System safety: Could this harm the system or other processes?
- Scope appropriateness: Is the batch size reasonable?
- Intent clarity: Are the operations clearly related and purposeful?
- Reversibility: Can the effects be easily undone if needed?

HIGH-RISK PATTERNS TO WATCH FOR:
- Multiple file modifications followed by execution
- Operations on system-critical files
- Broad filesystem changes
- Combining write operations with command execution
- Operations without clear safeguards

DECISION GUIDELINES:
- APPROVE: Safe, well-scoped operations with low risk
- DENY: Clearly dangerous or potentially destructive operations
- REQUIRE_HUMAN: Uncertain cases or operations needing human judgment

Respond only with the JSON object, no additional text."""

        return prompt

    async def _simulate_llm_judge(self, prompt: str, tool_calls: List[ToolCall]) -> str:
        """Simulate LLM judge response (replace with actual claudiallm call)."""
        # This is a placeholder simulation - replace with actual claudiallm integration
        await asyncio.sleep(0.1)  # Simulate network delay

        # Simple heuristic-based simulation
        tool_names = [tc.tool_name for tc in tool_calls]
        high_risk_count = sum(1 for name in tool_names if name in self.HIGH_RISK_TOOLS)

        if high_risk_count == 0:
            decision = "approve"
            confidence = 0.9
            risk = "low"
            reasoning = "All tools are low-risk"
        elif high_risk_count == 1 and len(tool_calls) <= 2:
            decision = "require_human"
            confidence = 0.8
            risk = "medium"
            reasoning = "Single high-risk tool in small batch"
        else:
            decision = "deny"
            confidence = 0.85
            risk = "high"
            reasoning = "Multiple high-risk tools or large batch"

        return json.dumps(
            {
                "decision": decision,
                "confidence": confidence,
                "reasoning": reasoning,
                "risk_assessment": risk,
                "alternative_suggestions": [
                    "Break into smaller operations",
                    "Add explicit safeguards",
                ],
            }
        )

    def _parse_judge_response(self, response: str) -> SafeguardResult:
        """Parse the judge model JSON response."""
        try:
            data = json.loads(response)
            return SafeguardResult(
                decision=SafeguardDecision(data["decision"]),
                confidence=float(data["confidence"]),
                reasoning=data["reasoning"],
                risk_assessment=data["risk_assessment"],
                alternative_suggestions=data.get("alternative_suggestions", []),
                requires_human_approval=(data["decision"] == "require_human"),
            )
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            logger.error(f"Error parsing judge response: {e}")
            # Fail safe
            return SafeguardResult(
                decision=SafeguardDecision.REQUIRE_HUMAN,
                confidence=0.5,
                reasoning=f"Failed to parse judge response: {e}",
                risk_assessment="unknown",
                alternative_suggestions=["Manual review required"],
                requires_human_approval=True,
            )

    def _contains_dangerous_bash_patterns(self, command: str) -> bool:
        """Check if bash command contains dangerous patterns."""
        dangerous_patterns = [
            "rm -rf",
            "sudo rm",
            "format",
            "mkfs",
            "dd if=",
            ":(){ :|:& };:",
            "chmod -R 777",
            "chown -R",
            "curl | sh",
            "wget | sh",
            ">{PATH}",
            "cat /dev/zero",
            "python -c 'import os;os.system'",
        ]

        command_lower = command.lower()
        return any(pattern.lower() in command_lower for pattern in dangerous_patterns)


# Global safeguards instance
_safeguards_instance: Optional[MultitoolSafeguards] = None


def get_safeguards() -> MultitoolSafeguards:
    """Get or create the global safeguards instance."""
    global _safeguards_instance
    if _safeguards_instance is None:
        _safeguards_instance = MultitoolSafeguards()
    return _safeguards_instance


async def evaluate_multitool_safety(
    tool_calls: List[Dict[str, Any]], context: Optional[str] = None
) -> SafeguardResult:
    """
    Convenience function to evaluate multitool operation safety.

    Args:
        tool_calls: List of tool call dictionaries
        context: Optional context about the operation

    Returns:
        SafeguardResult with safety evaluation
    """
    safeguards = get_safeguards()

    # Convert dict tool calls to ToolCall objects
    tool_call_objects = []
    for tool_dict in tool_calls:
        tool_call = ToolCall(
            tool_name=tool_dict.get("tool_name", "unknown"),
            parameters=tool_dict.get("parameters", {}),
            description=tool_dict.get(
                "description", f"Call to {tool_dict.get('tool_name', 'unknown')}"
            ),
        )
        tool_call_objects.append(tool_call)

    return await safeguards.evaluate_multitool_operation(tool_call_objects, context)


# Decorator for protecting multitool operations
def require_safeguard_approval(context: str = None):
    """
    Decorator to require safeguard approval for multitool operations.

    Usage:
        @require_safeguard_approval("File modification operation")
        async def modify_files(tool_calls):
            # Implementation
    """

    def decorator(func):
        async def wrapper(*args, **kwargs):
            # Extract tool_calls from args/kwargs
            tool_calls = []
            if args and isinstance(args[0], list):
                tool_calls = args[0]
            elif "tool_calls" in kwargs:
                tool_calls = kwargs["tool_calls"]

            if len(tool_calls) > 1:
                result = await evaluate_multitool_safety(tool_calls, context)

                if result.decision == SafeguardDecision.DENY:
                    raise PermissionError(
                        f"Multitool operation denied by safeguards: {result.reasoning}"
                    )
                elif result.decision == SafeguardDecision.REQUIRE_HUMAN:
                    # In a real implementation, this would prompt for human approval
                    logger.warning(
                        f"Multitool operation requires human approval: {result.reasoning}"
                    )

            return await func(*args, **kwargs)

        return wrapper

    return decorator
```

---
### File: src/vibelint/plugin_system.py

```python
"""
Core types and validation system for vibelint.

Simplified from the original over-engineered plugin system to focus on
essential functionality without unnecessary abstractions.

vibelint/src/vibelint/plugin_system.py
"""

import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from typing import Any, Dict, Iterator, List, Optional, Protocol, Type

logger = logging.getLogger(__name__)

__all__ = [
    "Severity",
    "Finding",
    "Validator",
    "Formatter",
    "BaseValidator",
    "BaseFormatter",
    "get_all_validators",
    "get_all_formatters",
    "get_validator",
    "get_formatter",
]


class Severity(Enum):
    """Severity levels for validation findings."""

    OFF = "OFF"
    INFO = "INFO"
    WARN = "WARN"
    BLOCK = "BLOCK"

    def __lt__(self, other):
        """Enable sorting by severity."""
        order = {"OFF": 0, "INFO": 1, "WARN": 2, "BLOCK": 3}
        return order[self.value] < order[other.value]


@dataclass
class Finding:
    """A validation finding from a validator."""

    rule_id: str
    message: str
    file_path: Path
    line: int = 0
    column: int = 0
    severity: Severity = Severity.WARN
    context: str = ""
    suggestion: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convert finding to dictionary for JSON output."""
        return {
            "rule": self.rule_id,
            "level": self.severity.value,
            "path": str(self.file_path),
            "line": self.line,
            "column": self.column,
            "msg": self.message,
            "context": self.context,
            "suggestion": self.suggestion,
        }


class Validator(Protocol):
    """Protocol for validator classes - simpler than abstract base class."""

    rule_id: str
    default_severity: Severity

    def __init__(
        self, severity: Optional[Severity] = None, config: Optional[Dict[str, Any]] = None
    ) -> None: ...

    def validate(
        self, file_path: Path, content: str, config: Optional[Dict[str, Any]] = None
    ) -> Iterator[Finding]:
        """Validate a file and yield findings."""
        ...

    def create_finding(
        self,
        message: str,
        file_path: Path,
        line: int = 0,
        column: int = 0,
        context: str = "",
        suggestion: Optional[str] = None,
    ) -> Finding:
        """Create a Finding object with this validator's rule_id and severity."""
        ...


class Formatter(Protocol):
    """Protocol for formatter classes - simpler than abstract base class."""

    name: str

    def format_results(
        self,
        findings: List[Finding],
        summary: Dict[str, int],
        config: Optional[Dict[str, Any]] = None,
    ) -> str:
        """Format validation results for output."""
        ...


# Simple registry - no complex plugin discovery needed
_VALIDATORS: Dict[str, Type[Validator]] = {}
_FORMATTERS: Dict[str, Type[Formatter]] = {}


def register_validator(validator_class: Type[Validator]) -> None:
    """Register a validator class."""
    _VALIDATORS[validator_class.rule_id] = validator_class


def register_formatter(formatter_class: Type[Formatter]) -> None:
    """Register a formatter class."""
    _FORMATTERS[formatter_class.name] = formatter_class


def get_validator(rule_id: str) -> Optional[Type[Validator]]:
    """Get validator class by rule ID."""
    return _VALIDATORS.get(rule_id)


def get_all_validators() -> Dict[str, Type[Validator]]:
    """Get all registered validator classes."""
    # Lazy load validators from entry points on first access
    if not _VALIDATORS:
        _load_builtin_validators()
    return _VALIDATORS.copy()


def get_formatter(name: str) -> Optional[Type[Formatter]]:
    """Get formatter class by name."""
    return _FORMATTERS.get(name)


def get_all_formatters() -> Dict[str, Type[Formatter]]:
    """Get all registered formatter classes."""
    # Lazy load formatters from entry points on first access
    if not _FORMATTERS:
        _load_builtin_formatters()
    return _FORMATTERS.copy()


def _load_builtin_validators() -> None:
    """Load built-in validators from entry points."""
    import importlib.metadata

    for entry_point in importlib.metadata.entry_points(group="vibelint.validators"):
        try:
            validator_class = entry_point.load()
            if hasattr(validator_class, "rule_id"):
                _VALIDATORS[validator_class.rule_id] = validator_class
        except (ImportError, AttributeError, TypeError) as e:
            logger.warning(
                f"Failed to load validator '{entry_point.name}' from entry point {entry_point.value}: {e}"
            )
            pass


def _load_builtin_formatters() -> None:
    """Load built-in formatters from entry points."""
    import importlib.metadata

    for entry_point in importlib.metadata.entry_points(group="vibelint.formatters"):
        try:
            formatter_class = entry_point.load()
            if hasattr(formatter_class, "name"):
                _FORMATTERS[formatter_class.name] = formatter_class
        except (ImportError, AttributeError, TypeError) as e:
            logger.debug(f"Failed to load formatter from entry point {entry_point.name}: {e}")
            pass


# Concrete base classes
class BaseValidator:
    """Base class for validators."""

    rule_id: str = ""
    default_severity: Severity = Severity.WARN

    def __init__(
        self, severity: Optional[Severity] = None, config: Optional[Dict[str, Any]] = None
    ) -> None:
        self.severity = severity or self.default_severity
        self.config = config or {}

    def validate(
        self, file_path: Path, content: str, config: Optional[Dict[str, Any]] = None
    ) -> Iterator[Finding]:
        """Validate a file and yield findings."""
        raise NotImplementedError

    def create_finding(
        self,
        message: str,
        file_path: Path,
        line: int = 0,
        column: int = 0,
        context: str = "",
        suggestion: Optional[str] = None,
    ) -> Finding:
        """Create a Finding object with this validator's rule_id and severity."""
        return Finding(
            rule_id=self.rule_id,
            message=message,
            file_path=file_path,
            line=line,
            column=column,
            severity=self.severity,
            context=context,
            suggestion=suggestion,
        )


class BaseFormatter(ABC):
    """Base class for formatters."""

    name: str = ""
    description: str = ""

    @abstractmethod
    def format_results(
        self,
        findings: List[Finding],
        summary: Dict[str, int],
        config: Optional[Dict[str, Any]] = None,
    ) -> str:
        """Format validation results for output."""
        pass


# Legacy global manager for backward compatibility
class _LegacyPluginManager:
    """Legacy compatibility wrapper."""

    def load_plugins(self) -> None:
        """Load plugins - delegated to new system."""
        get_all_validators()
        get_all_formatters()

    def get_validator(self, rule_id: str) -> Optional[Any]:
        """Get validator by rule ID."""
        return get_validator(rule_id)

    def get_all_validators(self) -> Dict[str, type]:
        """Get all validators."""
        return get_all_validators()

    def get_formatter(self, name: str) -> Optional[Any]:
        """Get formatter by name."""
        return get_formatter(name)

    def get_all_formatters(self) -> Dict[str, type]:
        """Get all formatters."""
        return get_all_formatters()


plugin_manager = _LegacyPluginManager()
```

---
### File: src/vibelint/project_map.py

```python
"""
Project mapping and file discovery system for scalable organization.

Automatically generates project structure maps, detects organizational issues,
and suggests improvements for large codebases.

vibelint/src/vibelint/project_map.py
"""

import json
import logging
from collections import defaultdict
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Set

logger = logging.getLogger(__name__)

__all__ = ["ProjectMapper", "FileNode", "ModuleGroup"]


@dataclass
class FileNode:
    """Represents a file in the project structure."""

    path: str
    name: str
    size: int
    file_type: str
    purpose: Optional[str] = None
    dependencies: List[str] = None
    exports: List[str] = None
    lines_of_code: Optional[int] = None
    last_modified: Optional[str] = None

    def __post_init__(self):
        if self.dependencies is None:
            self.dependencies = []
        if self.exports is None:
            self.exports = []


@dataclass
class ModuleGroup:
    """Represents a logical grouping of related files."""

    name: str
    files: List[FileNode]
    cohesion_score: float
    suggested_location: str
    grouping_reason: str


class ProjectMapper:
    """Generates comprehensive project structure maps and organization suggestions."""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.file_nodes: Dict[str, FileNode] = {}
        self.module_groups: List[ModuleGroup] = []

    def generate_project_map(self) -> Dict[str, Any]:
        """Generate comprehensive project structure map."""

        # 1. Discover all files
        self._discover_files()

        # 2. Analyze dependencies
        self._analyze_dependencies()

        # 3. Detect module groups
        self._detect_module_groups()

        # 4. Calculate organization metrics
        metrics = self._calculate_organization_metrics()

        # 5. Generate recommendations
        recommendations = self._generate_recommendations()

        return {
            "project_root": str(self.project_root),
            "total_files": len(self.file_nodes),
            "file_tree": self._build_file_tree(),
            "module_groups": [asdict(group) for group in self.module_groups],
            "organization_metrics": metrics,
            "recommendations": recommendations,
            "file_index": {path: asdict(node) for path, node in self.file_nodes.items()},
        }

    def _discover_files(self):
        """Discover and catalog all project files."""
        for file_path in self.project_root.rglob("*"):
            if file_path.is_file() and not self._should_ignore_file(file_path):
                relative_path = str(file_path.relative_to(self.project_root))

                node = FileNode(
                    path=relative_path,
                    name=file_path.name,
                    size=file_path.stat().st_size,
                    file_type=file_path.suffix or "no_extension",
                    purpose=self._infer_file_purpose(file_path),
                    lines_of_code=self._count_lines_of_code(file_path),
                    last_modified=file_path.stat().st_mtime,
                )

                self.file_nodes[relative_path] = node

    def _should_ignore_file(self, file_path: Path) -> bool:
        """Check if file should be ignored in analysis."""
        ignore_patterns = {
            # Directories
            ".git",
            "__pycache__",
            ".pytest_cache",
            "node_modules",
            ".mypy_cache",
            ".tox",
            "build",
            "dist",
            ".venv",
            "venv",
            # File patterns
            ".pyc",
            ".pyo",
            ".pyd",
            ".so",
            ".dylib",
            ".dll",
            ".log",
            ".tmp",
            ".temp",
            ".cache",
        }

        return any(pattern in str(file_path) for pattern in ignore_patterns)

    def _infer_file_purpose(self, file_path: Path) -> str:
        """Infer file purpose from name, location, and content."""
        name = file_path.name.lower()

        # Configuration files
        if name in {"pyproject.toml", "setup.py", "requirements.txt", "tox.ini"}:
            return "configuration"

        # Documentation
        if file_path.suffix in {".md", ".rst", ".txt"} or "readme" in name:
            return "documentation"

        # Tests
        if "test" in name or "tests" in str(file_path):
            return "testing"

        # Python modules
        if file_path.suffix == ".py":
            if name == "__init__.py":
                return "package_init"
            elif name in {"cli.py", "main.py", "__main__.py"}:
                return "entry_point"
            elif any(keyword in name for keyword in ["validator", "check", "lint"]):
                return "validation_logic"
            elif any(keyword in name for keyword in ["manager", "handler", "service"]):
                return "business_logic"
            elif any(keyword in name for keyword in ["util", "helper", "tool"]):
                return "utility"
            else:
                return "module"

        # Scripts
        if file_path.suffix in {".sh", ".bat", ".ps1"}:
            return "script"

        return "unknown"

    def _count_lines_of_code(self, file_path: Path) -> Optional[int]:
        """Count lines of code in file."""
        try:
            if file_path.suffix in {".py", ".js", ".ts", ".java", ".cpp", ".c", ".h"}:
                content = file_path.read_text(encoding="utf-8")
                return len(
                    [
                        line
                        for line in content.splitlines()
                        if line.strip() and not line.strip().startswith("#")
                    ]
                )
        except (UnicodeDecodeError, PermissionError):
            pass
        return None

    def _build_file_tree(self) -> Dict[str, Any]:
        """Build hierarchical file tree structure."""
        tree = {}

        for file_path in self.file_nodes.keys():
            parts = file_path.split("/")
            current = tree

            for part in parts[:-1]:  # Directories
                if part not in current:
                    current[part] = {"type": "directory", "children": {}}
                current = current[part]["children"]

            # File
            filename = parts[-1]
            current[filename] = {
                "type": "file",
                "purpose": self.file_nodes[file_path].purpose,
                "size": self.file_nodes[file_path].size,
                "lines_of_code": self.file_nodes[file_path].lines_of_code,
            }

        return tree

    def _analyze_dependencies(self):
        """Analyze import dependencies between files."""
        # This would parse Python files to extract imports
        # Simplified for now
        pass

    def _detect_module_groups(self):
        """Detect logical groupings of related files."""
        # Group by purpose and naming patterns
        purpose_groups = defaultdict(list)

        for node in self.file_nodes.values():
            if node.purpose != "unknown":
                purpose_groups[node.purpose].append(node)

        # Create module groups for related files
        for purpose, files in purpose_groups.items():
            if len(files) > 1:
                cohesion_score = self._calculate_cohesion_score(files)
                suggested_location = f"src/vibelint/{purpose}/"

                group = ModuleGroup(
                    name=purpose,
                    files=files,
                    cohesion_score=cohesion_score,
                    suggested_location=suggested_location,
                    grouping_reason=f"Files with shared purpose: {purpose}",
                )
                self.module_groups.append(group)

    def _calculate_cohesion_score(self, files: List[FileNode]) -> float:
        """Calculate cohesion score for a group of files."""
        # Simplified scoring based on naming patterns and purposes
        if len(files) < 2:
            return 0.0

        # Score based on naming similarity
        names = [f.name for f in files]
        common_prefixes = self._find_common_prefixes(names)
        naming_score = len(common_prefixes) / len(files)

        return min(naming_score * 2, 1.0)  # Scale to 0-1

    def _find_common_prefixes(self, names: List[str]) -> Set[str]:
        """Find common prefixes in file names."""
        prefixes = set()
        for name in names:
            parts = name.replace("_", " ").replace("-", " ").split()
            for part in parts:
                if len(part) > 2:
                    prefixes.add(part)
        return prefixes

    def _calculate_organization_metrics(self) -> Dict[str, Any]:
        """Calculate metrics about project organization quality."""
        total_files = len(self.file_nodes)

        # Files by purpose
        purpose_distribution = defaultdict(int)
        for node in self.file_nodes.values():
            purpose_distribution[node.purpose] += 1

        # Directory depth analysis
        max_depth = max(len(path.split("/")) for path in self.file_nodes.keys())
        avg_depth = sum(len(path.split("/")) for path in self.file_nodes.keys()) / total_files

        # Grouping potential
        groupable_files = sum(1 for group in self.module_groups if group.cohesion_score > 0.5)

        return {
            "total_files": total_files,
            "max_directory_depth": max_depth,
            "average_directory_depth": avg_depth,
            "purpose_distribution": dict(purpose_distribution),
            "potential_module_groups": len(self.module_groups),
            "groupable_files": groupable_files,
            "organization_score": self._calculate_organization_score(),
        }

    def _calculate_organization_score(self) -> float:
        """Calculate overall organization quality score (0-1)."""
        # Factors that indicate good organization
        factors = []

        # 1. Purpose clarity (how many files have clear purposes)
        clear_purpose_ratio = sum(
            1 for node in self.file_nodes.values() if node.purpose != "unknown"
        ) / len(self.file_nodes)
        factors.append(clear_purpose_ratio)

        # 2. Module cohesion (how well files are grouped)
        if self.module_groups:
            avg_cohesion = sum(group.cohesion_score for group in self.module_groups) / len(
                self.module_groups
            )
            factors.append(avg_cohesion)
        else:
            factors.append(0.5)  # Neutral score for no groups

        # 3. Directory structure depth (not too deep, not too flat)
        total_files = len(self.file_nodes)
        max_depth = max(len(path.split("/")) for path in self.file_nodes.keys())

        # Ideal depth: 2-4 levels for most projects
        if total_files < 20:
            ideal_depth = 2
        elif total_files < 100:
            ideal_depth = 3
        else:
            ideal_depth = 4

        depth_score = max(0, 1 - abs(max_depth - ideal_depth) / ideal_depth)
        factors.append(depth_score)

        return sum(factors) / len(factors)

    def _generate_recommendations(self) -> List[Dict[str, str]]:
        """Generate actionable recommendations for improving organization."""
        recommendations = []

        # Recommend grouping scattered modules
        for group in self.module_groups:
            if group.cohesion_score > 0.7 and len(group.files) >= 2:
                file_names = [f.name for f in group.files]
                recommendations.append(
                    {
                        "type": "module_grouping",
                        "priority": "medium",
                        "description": f"Group {group.name} files into subpackage",
                        "action": f"mkdir {group.suggested_location} && mv {' '.join(file_names)} {group.suggested_location}",
                        "reason": group.grouping_reason,
                    }
                )

        # Recommend documentation for unclear files
        unclear_files = [node for node in self.file_nodes.values() if node.purpose == "unknown"]
        if unclear_files:
            recommendations.append(
                {
                    "type": "documentation",
                    "priority": "high",
                    "description": f"Document purpose of {len(unclear_files)} unclear files",
                    "action": "Add docstrings or comments explaining file purposes",
                    "reason": "Files without clear purpose indicate organizational debt",
                }
            )

        # Recommend directory restructuring if too flat/deep
        organization_score = self._calculate_organization_score()
        if organization_score < 0.6:
            recommendations.append(
                {
                    "type": "restructuring",
                    "priority": "high",
                    "description": "Consider major project restructuring",
                    "action": "Group related files into logical subpackages",
                    "reason": f"Organization score is low ({organization_score:.2f})",
                }
            )

        return recommendations

    def save_project_map(self, output_path: Path):
        """Save project map to JSON file."""
        project_map = self.generate_project_map()

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(project_map, f, indent=2, default=str)

        logger.info(f"Project map saved to {output_path}")
        return project_map
```

---
### File: src/vibelint/reporting.py

```python
"""
Comprehensive reporting system for vibelint analysis results.

Provides structured report generation with granular verbosity levels,
artifact management, hyperlinked reports, and multiple output formatters
for different consumers (humans, CI/CD, GitHub, LLMs).

vibelint/src/vibelint/reporting.py
"""

import json
import time
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional

from vibelint.plugin_system import BaseFormatter, Finding, Severity

__all__ = [
    "ReportGenerator",
    "ReportConfig",
    "VerbosityLevel",
    "NaturalLanguageFormatter",
    "JsonFormatter",
    "SarifFormatter",
    "LLMFormatter",
    "HumanFormatter",
    "BUILTIN_FORMATTERS",
    "FORMAT_CHOICES",
    "DEFAULT_FORMAT",
]


class VerbosityLevel(Enum):
    """Report verbosity levels for different use cases."""

    EXECUTIVE = "executive"  # High-level summary for planning
    TACTICAL = "tactical"  # Actionable items for development
    DETAILED = "detailed"  # Comprehensive analysis with context
    FORENSIC = "forensic"  # Complete diagnostic information


@dataclass
class ReportConfig:
    """Configuration for report generation."""

    # Output settings
    output_directory: Path
    report_name: str = "vibelint_analysis"
    verbosity_level: VerbosityLevel = VerbosityLevel.TACTICAL

    # Format settings
    formats: List[str] = None  # ["markdown", "json", "html"]
    include_artifacts: bool = True
    create_index: bool = True

    # Content settings
    max_findings_per_category: int = 20
    include_raw_llm_responses: bool = False
    include_performance_metrics: bool = True

    # Navigation settings
    generate_hyperlinks: bool = True
    create_quick_nav: bool = True

    def __post_init__(self):
        if self.formats is None:
            self.formats = ["markdown", "html"]


class ReportGenerator:
    """Generates structured analysis reports with granular verbosity control."""

    def __init__(self, config: ReportConfig):
        self.config = config

    def generate_comprehensive_report(
        self, analysis_results: Dict[str, Any], timestamp: Optional[str] = None
    ) -> Dict[str, Path]:
        """Generate comprehensive report with all artifacts."""

        if timestamp is None:
            timestamp = time.strftime("%Y%m%d_%H%M%S")

        # Create report directory structure
        report_dir = self.config.output_directory / f"{self.config.report_name}_{timestamp}"
        report_dir.mkdir(parents=True, exist_ok=True)

        generated_files = {}

        # Generate main report
        main_report_path = self._generate_main_report(analysis_results, report_dir, timestamp)
        generated_files["main_report"] = main_report_path

        # Generate artifacts if enabled
        if self.config.include_artifacts:
            artifact_paths = self._generate_artifacts(analysis_results, report_dir, timestamp)
            generated_files.update(artifact_paths)

        # Generate index/navigation if enabled
        if self.config.create_index:
            index_path = self._generate_index(report_dir, generated_files, timestamp)
            generated_files["index"] = index_path

        # Generate quick action plan
        quick_plan_path = self._generate_quick_action_plan(analysis_results, report_dir, timestamp)
        generated_files["quick_plan"] = quick_plan_path

        return generated_files

    def _generate_main_report(
        self, analysis_results: Dict[str, Any], report_dir: Path, timestamp: str
    ) -> Path:
        """Generate main analysis report."""

        # Filter content based on verbosity level
        filtered_results = self._filter_by_verbosity(analysis_results)

        # Generate markdown content
        content = self._format_main_report_markdown(filtered_results, timestamp)

        # Save to file
        report_path = report_dir / "main_report.md"
        report_path.write_text(content, encoding="utf-8")

        return report_path

    def _format_main_report_markdown(self, analysis_results: Dict[str, Any], timestamp: str) -> str:
        """Format main report as markdown."""

        executive = analysis_results.get("synthesis", {}).get("executive_summary", {})
        priority_actions = analysis_results.get("synthesis", {}).get("priority_actions", [])

        content = f"""# Vibelint Analysis Report

Generated: {timestamp}
Verbosity Level: {self.config.verbosity_level.value}

## Executive Summary

- **Overall Health**: {executive.get('overall_health', 'Unknown')}
- **Critical Issues**: {executive.get('critical_issues', 0)}
- **Improvement Opportunities**: {executive.get('improvement_opportunities', 0)}
- **Estimated Effort**: {executive.get('estimated_effort', 'Unknown')}

## Priority Actions

"""

        for i, action in enumerate(priority_actions[:5], 1):
            content += f"""### {i}. {action.get('title', 'Unknown Action')} ({action.get('priority', 'P?')})

{action.get('description', 'No description available')}

**Effort**: {action.get('effort_hours', '?')} hours
**Risk if ignored**: {action.get('risk_if_ignored', 'Unknown')}

"""

        # Add findings summary based on verbosity
        if self.config.verbosity_level != VerbosityLevel.EXECUTIVE:
            content += self._add_findings_section(analysis_results)

        return content

    def _add_findings_section(self, analysis_results: Dict[str, Any]) -> str:
        """Add findings section based on verbosity level."""
        content = "\n## Findings Summary\n\n"

        # Tree violations
        tree_violations = analysis_results.get("tree_analysis", {}).get("quick_violations", [])
        if tree_violations:
            content += f"### Organizational Issues ({len(tree_violations)})\n\n"
            for violation in tree_violations[: self.config.max_findings_per_category]:
                content += f"- **{violation.get('violation_type', 'Unknown')}**: {violation.get('message', 'No message')}\n"
            content += "\n"

        # Content findings
        content_findings = []
        for file_analysis in analysis_results.get("content_analysis", {}).get("file_analyses", []):
            findings = file_analysis.get("analysis", {}).get("findings", [])
            content_findings.extend(findings)

        if content_findings:
            content += f"### Structural Issues ({len(content_findings)})\n\n"
            for finding in content_findings[: self.config.max_findings_per_category]:
                content += f"- **{finding.get('rule_id', 'Unknown')}**: {finding.get('message', 'No message')}\n"
            content += "\n"

        return content

    def _generate_artifacts(
        self, analysis_results: Dict[str, Any], report_dir: Path, timestamp: str
    ) -> Dict[str, Path]:
        """Generate detailed artifacts for different analysis aspects."""

        artifacts_dir = report_dir / "artifacts"
        artifacts_dir.mkdir(exist_ok=True)

        artifact_paths = {}

        # Generate JSON artifacts for each analysis level
        if "tree_analysis" in analysis_results:
            tree_path = artifacts_dir / "organizational_analysis.json"
            tree_path.write_text(
                json.dumps(analysis_results["tree_analysis"], indent=2, default=str),
                encoding="utf-8",
            )
            artifact_paths["organizational"] = tree_path

        if "content_analysis" in analysis_results:
            content_path = artifacts_dir / "structural_analysis.json"
            content_path.write_text(
                json.dumps(analysis_results["content_analysis"], indent=2, default=str),
                encoding="utf-8",
            )
            artifact_paths["structural"] = content_path

        if "deep_analysis" in analysis_results:
            arch_path = artifacts_dir / "architectural_analysis.json"
            arch_path.write_text(
                json.dumps(analysis_results["deep_analysis"], indent=2, default=str),
                encoding="utf-8",
            )
            artifact_paths["architectural"] = arch_path

        return artifact_paths

    def _generate_quick_action_plan(
        self, analysis_results: Dict[str, Any], report_dir: Path, timestamp: str
    ) -> Path:
        """Generate quick action plan for immediate development focus."""

        synthesis = analysis_results.get("synthesis", {})

        quick_plan = f"""# Quick Action Plan
Generated: {timestamp}

## Immediate Actions (< 1 hour each)

"""

        # Add quick wins
        quick_wins = synthesis.get("quick_wins", [])
        for i, win in enumerate(quick_wins[:5], 1):
            quick_plan += f"{i}. {win}\n"

        quick_plan += "\n## Priority Issues (requires planning)\n\n"

        # Add priority actions
        priority_actions = synthesis.get("priority_actions", [])
        for action in priority_actions[:3]:
            title = action.get("title", "Unknown action")
            priority = action.get("priority", "P?")
            effort = action.get("effort_hours", "?")

            quick_plan += f"### {title} ({priority})\n"
            quick_plan += f"**Effort**: {effort} hours\n"
            quick_plan += f"**Description**: {action.get('description', 'No description')}\n\n"

        quick_plan_path = report_dir / "QUICK_ACTION_PLAN.md"
        quick_plan_path.write_text(quick_plan, encoding="utf-8")

        return quick_plan_path

    def _generate_index(
        self, report_dir: Path, generated_files: Dict[str, Path], timestamp: str
    ) -> Path:
        """Generate navigation index for the report."""

        index_content = f"""# Vibelint Analysis Report Index
Generated: {timestamp}

## Main Reports

- [[REPORT] Main Analysis Report](main_report.md)
- [[ROCKET] Quick Action Plan](QUICK_ACTION_PLAN.md)

## Detailed Artifacts

"""

        # Add artifact links
        artifact_types = {
            "organizational": "[BUILD] Organizational Analysis",
            "structural": "[TOOL] Structural Analysis",
            "architectural": "[ARCH] Architectural Analysis",
        }

        for artifact_key, description in artifact_types.items():
            if artifact_key in generated_files:
                artifact_path = generated_files[artifact_key]
                relative_path = f"artifacts/{artifact_path.name}"
                index_content += f"- [{description}]({relative_path})\n"

        index_content += f"\n---\n\nReport generated by vibelint at {timestamp}\n"

        index_path = report_dir / "index.md"
        index_path.write_text(index_content, encoding="utf-8")

        return index_path

    def _filter_by_verbosity(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """Filter analysis results based on configured verbosity level."""

        if self.config.verbosity_level == VerbosityLevel.EXECUTIVE:
            # Only high-level summary and critical issues
            return {
                "synthesis": {
                    "executive_summary": analysis_results.get("synthesis", {}).get(
                        "executive_summary", {}
                    ),
                    "priority_actions": self._extract_critical_issues(analysis_results),
                }
            }

        elif self.config.verbosity_level == VerbosityLevel.TACTICAL:
            # Actionable items and priority information
            filtered = analysis_results.copy()

            # Limit findings per category
            if "content_analysis" in filtered:
                filtered["content_analysis"] = self._limit_content_findings(
                    filtered["content_analysis"]
                )

            return filtered

        else:  # DETAILED or FORENSIC
            # Most or all information
            return analysis_results

    def _extract_critical_issues(self, analysis_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract only critical/blocking issues for executive summary."""
        critical_issues = []

        # Check synthesis for critical items
        synthesis = analysis_results.get("synthesis", {})
        for action in synthesis.get("priority_actions", []):
            if action.get("priority") in ["P0", "P1"]:
                critical_issues.append(action)

        return critical_issues

    def _limit_content_findings(self, content_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Limit content findings for tactical verbosity."""
        limited = content_analysis.copy()

        if "file_analyses" in limited:
            for file_analysis in limited["file_analyses"]:
                if "analysis" in file_analysis and "findings" in file_analysis["analysis"]:
                    findings = file_analysis["analysis"]["findings"]
                    # Keep only high-severity findings for tactical view
                    high_severity = [f for f in findings if f.get("severity") in ["BLOCK", "WARN"]]
                    file_analysis["analysis"]["findings"] = high_severity[:5]  # Limit to 5 per file

        return limited


# ===== FORMATTERS =====
# Consolidated from formatters.py


class NaturalLanguageFormatter(BaseFormatter):
    """Natural language output formatter for humans and AI agents."""

    name = "natural"
    description = "Natural language output format optimized for human and AI agent consumption"

    def format_results(
        self, findings: List[Finding], summary: Dict[str, int], config: Optional[Any] = None
    ) -> str:
        """Format results for human reading."""
        if not findings:
            return "All checks passed!"

        # Get max display limit from config
        max_displayed = 50  # Default to 50 issues for readability (0 means no limit)
        if config and hasattr(config, "get"):
            max_displayed = config.get("max_displayed_issues", 50)
        elif config and hasattr(config, "__getitem__"):
            max_displayed = config.get("max_displayed_issues", 50)

        # Group findings by severity
        by_severity = {Severity.BLOCK: [], Severity.WARN: [], Severity.INFO: []}

        for finding in findings:
            if finding.severity in by_severity:
                by_severity[finding.severity].append(finding)

        lines = []
        displayed_count = 0
        total_count = len(findings)

        # Add findings by severity (highest first)
        for severity in [Severity.BLOCK, Severity.WARN, Severity.INFO]:
            if by_severity[severity]:
                lines.append(f"\n{severity.value}:")

                severity_findings = by_severity[severity]
                for _, finding in enumerate(severity_findings):
                    if max_displayed > 0 and displayed_count >= max_displayed:
                        remaining_total = total_count - displayed_count
                        lines.append("")
                        lines.append(
                            f"  WARNING: Showing first {max_displayed} issues. {remaining_total} more found."
                        )
                        lines.append(
                            "  TIP: Set max_displayed_issues = 0 in pyproject.toml to show all issues."
                        )
                        break

                    location = (
                        f"{finding.file_path}:{finding.line}"
                        if finding.line > 0
                        else str(finding.file_path)
                    )
                    lines.append(f"  {finding.rule_id}: {finding.message} ({location})")
                    if finding.suggestion:
                        lines.append(f"    → {finding.suggestion}")

                    displayed_count += 1

                if max_displayed > 0 and displayed_count >= max_displayed:
                    break

        # Add summary with full counts
        total_errors = sum(1 for f in findings if f.severity == Severity.BLOCK)
        total_warnings = sum(1 for f in findings if f.severity == Severity.WARN)
        total_info = sum(1 for f in findings if f.severity == Severity.INFO)

        summary_line = (
            f"\nSummary: {total_errors} errors, {total_warnings} warnings, {total_info} info"
        )
        if max_displayed > 0 and total_count > max_displayed:
            summary_line += f" (showing first {min(max_displayed, total_count)} of {total_count})"

        lines.append(summary_line)

        return "\n".join(lines)


class JsonFormatter(BaseFormatter):
    """JSON output formatter for machine processing."""

    name = "json"
    description = "JSON output format for CI/tooling integration"

    def format_results(
        self, findings: List[Finding], summary: Dict[str, int], config: Optional[Any] = None
    ) -> str:
        """Format results as JSON."""
        result = {"summary": summary, "findings": [finding.to_dict() for finding in findings]}
        return json.dumps(result, indent=2, default=str)


class SarifFormatter(BaseFormatter):
    """SARIF output formatter for GitHub integration."""

    name = "sarif"
    description = "SARIF format for GitHub code scanning"

    def format_results(
        self, findings: List[Finding], summary: Dict[str, int], config: Optional[Any] = None
    ) -> str:
        """Format results as SARIF JSON."""
        rules = {}
        results = []

        for finding in findings:
            # Collect unique rules
            if finding.rule_id not in rules:
                rules[finding.rule_id] = {
                    "id": finding.rule_id,
                    "name": finding.rule_id,
                    "shortDescription": {"text": finding.message},
                    "defaultConfiguration": {
                        "level": self._severity_to_sarif_level(finding.severity)
                    },
                }

            # Add result
            result = {
                "ruleId": finding.rule_id,
                "level": self._severity_to_sarif_level(finding.severity),
                "message": {"text": finding.message},
                "locations": [
                    {
                        "physicalLocation": {
                            "artifactLocation": {"uri": str(finding.file_path)},
                            "region": {
                                "startLine": max(1, finding.line),
                                "startColumn": max(1, finding.column),
                            },
                        }
                    }
                ],
            }

            if finding.suggestion:
                result["fixes"] = [{"description": {"text": finding.suggestion}}]

            results.append(result)

        sarif_output = {
            "$schema": "https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json",
            "version": "2.1.0",
            "runs": [
                {
                    "tool": {
                        "driver": {
                            "name": "vibelint",
                            "version": "0.1.2",
                            "informationUri": "https://github.com/mithranm/vibelint",
                            "rules": list(rules.values()),
                        }
                    },
                    "results": results,
                }
            ],
        }

        return json.dumps(sarif_output, separators=(",", ":"))

    def _severity_to_sarif_level(self, severity: Severity) -> str:
        """Convert vibelint severity to SARIF level."""
        mapping = {
            Severity.BLOCK: "error",
            Severity.WARN: "warning",
            Severity.INFO: "note",
            Severity.OFF: "none",
        }
        return mapping.get(severity, "warning")


class LLMFormatter(BaseFormatter):
    """LLM-optimized formatter for AI analysis."""

    name = "llm"
    description = "LLM-optimized format for AI analysis"

    def format_results(
        self, findings: List[Finding], summary: Dict[str, int], config: Optional[Any] = None
    ) -> str:
        """Format results for LLM analysis."""
        if not findings:
            return "No issues found."

        output = []
        for finding in findings:
            output.append(
                f"{finding.rule_id}: {finding.message} " f"({finding.file_path}:{finding.line})"
            )

        return "\n".join(output)


class HumanFormatter(NaturalLanguageFormatter):
    """Human-readable formatter (alias for NaturalLanguageFormatter)."""

    name = "human"
    description = "Human-readable format with colors and styling"


# Built-in report formatters
BUILTIN_FORMATTERS = {
    "natural": NaturalLanguageFormatter,
    "human": HumanFormatter,  # Separate class for plugin system compatibility
    "json": JsonFormatter,
    "sarif": SarifFormatter,
    "llm": LLMFormatter,
}

# Format choices for CLI - single source of truth
FORMAT_CHOICES = list(BUILTIN_FORMATTERS.keys())
DEFAULT_FORMAT = "natural"
```

---
### File: src/vibelint/results.py

```python
"""
Module for vibelint/results.py.

vibelint/src/vibelint/results.py
"""

from dataclasses import dataclass, field
from pathlib import Path

from vibelint.plugin_system import Finding
from .validators.project_wide.namespace_collisions import (NamespaceCollision,
                                                           NamespaceNode)

__all__ = ["CheckResult", "CommandResult", "NamespaceResult", "SnapshotResult"]


@dataclass
class CommandResult:
    """
    Base class for command results.

    vibelint/src/vibelint/results.py
    """

    success: bool = True
    error_message: str | None = None
    exit_code: int = 0

    def __post_init__(self):
        """
        Set exit code based on success if not explicitly set.

        vibelint/src/vibelint/results.py
        """

        if not self.success and self.exit_code == 0:
            self.exit_code = 1


@dataclass
class CheckResult(CommandResult):
    """
    Result data from the 'check' command.

    vibelint/src/vibelint/results.py
    """

    findings: list[Finding] = field(default_factory=list)
    hard_collisions: list[NamespaceCollision] = field(default_factory=list)
    global_soft_collisions: list[NamespaceCollision] = field(default_factory=list)
    local_soft_collisions: list[NamespaceCollision] = field(default_factory=list)
    report_path: Path | None = None
    report_generated: bool = False
    report_error: str | None = None


@dataclass
class NamespaceResult(CommandResult):
    """
    Result data from the 'namespace' command.

    vibelint/src/vibelint/results.py
    """

    root_node: NamespaceNode | None = None
    intra_file_collisions: list[NamespaceCollision] = field(default_factory=list)
    output_path: Path | None = None
    output_saved: bool = False
    output_error: str | None = None


@dataclass
class SnapshotResult(CommandResult):
    """
    Result data from the 'snapshot' command.

    vibelint/src/vibelint/results.py
    """

    output_path: Path | None = None
```

---
### File: src/vibelint/rules.py

```python
"""
Rule management system for vibelint.

Handles rule configuration, severity overrides, and policy management.

vibelint/src/vibelint/rules.py
"""

import logging
from enum import Enum
from typing import Any, Dict, List, Optional, Set

from vibelint.plugin_system import BaseValidator, Severity, plugin_manager

logger = logging.getLogger(__name__)


class DefaultSeverity(Enum):
    """Predefined severity levels for consistency."""

    INFO = "INFO"
    WARN = "WARN"
    ERROR = "ERROR"


__all__ = ["RuleEngine", "create_default_rule_config", "DefaultSeverity"]


class RuleEngine:
    """Manages rule configuration and policy decisions."""

    def __init__(self, config: Dict):
        """
        Initialize rule engine with configuration.

        Args:
            config: Configuration dictionary from pyproject.toml
        """
        self.config = config
        self._rule_overrides: Dict[str, Severity] = {}
        self._enabled_plugins: Set[str] = set()
        self._shared_models = {}  # Cache for expensive models like EmbeddingGemma
        self._load_rule_config()

    def _load_rule_config(self):
        """Load rule configuration from config."""
        # Load rule severity overrides
        rules_config = self.config.get("rules", {})
        for rule_id, setting in rules_config.items():
            if isinstance(setting, str):
                try:
                    self._rule_overrides[rule_id] = Severity(setting.upper())
                except ValueError as e:
                    logger.debug(f"Invalid severity setting for rule {rule_id}: {setting} - {e}")
                    pass
            elif isinstance(setting, bool):
                # Boolean: True=default severity, False=OFF
                if not setting:
                    self._rule_overrides[rule_id] = Severity.OFF

        # Load enabled plugins
        plugins_config = self.config.get("plugins", {})
        enabled = plugins_config.get("enabled", ["vibelint.core"])
        if isinstance(enabled, list):
            self._enabled_plugins.update(enabled)
        elif isinstance(enabled, str):
            self._enabled_plugins.add(enabled)

    def is_rule_enabled(self, rule_id: str) -> bool:
        """Check if a rule is enabled (not set to OFF)."""
        severity = self._rule_overrides.get(rule_id)
        return severity != Severity.OFF if severity else True

    def get_rule_severity(self, rule_id: str, default: Severity = Severity.WARN) -> Severity:
        """Get effective severity for a rule."""
        # Primary: semantic rule IDs
        severity = self._rule_overrides.get(rule_id)
        if severity is not None:
            return severity

        return default

    def create_validator_instance(
        self, validator_class: type[BaseValidator]
    ) -> Optional[BaseValidator]:
        """
        Create validator instance with configured severity.

        Args:
            validator_class: Validator class to instantiate

        Returns:
            Validator instance or None if rule is disabled
        """
        if not self.is_rule_enabled(validator_class.rule_id):
            return None

        severity = self.get_rule_severity(validator_class.rule_id, validator_class.default_severity)

        # Handle special cases that need shared resources
        if validator_class.rule_id == "SEMANTIC-SIMILARITY":
            shared_model = self._get_or_create_embedding_model()
            # Pass the model through config
            config_with_model = dict(self.config)
            config_with_model["_shared_model"] = shared_model
            return validator_class(severity=severity, config=config_with_model)

        return validator_class(severity=severity, config=self.config)

    def get_enabled_validators(self) -> List[BaseValidator]:
        """Get all enabled validator instances."""
        validators = []
        all_validators = plugin_manager.get_all_validators()

        for _, validator_class in all_validators.items():
            instance = self.create_validator_instance(validator_class)
            if instance:
                validators.append(instance)

        return validators

    def filter_enabled_validators(
        self, validator_classes: List[type[BaseValidator]]
    ) -> List[BaseValidator]:
        """Filter and instantiate only enabled validators from a list."""
        validators = []
        for validator_class in validator_classes:
            instance = self.create_validator_instance(validator_class)
            if instance:
                validators.append(instance)
        return validators

    def _get_or_create_embedding_model(self):
        """Get or create the shared EmbeddingGemma model for semantic similarity analysis."""
        model_key = "embedding_gemma"

        if model_key not in self._shared_models:
            import logging
            import os

            logger = logging.getLogger(__name__)

            try:
                from sentence_transformers import SentenceTransformer

                # Check configuration
                embedding_config = self.config.get("embedding_analysis", {})
                model_name = embedding_config.get("model", "google/embeddinggemma-300m")

                # Check if embedding analysis is enabled
                if not embedding_config.get("enabled", False):
                    logger.debug("Semantic similarity analysis disabled in configuration")
                    return None

                # Handle HF token from config, .env file, or environment
                hf_token = embedding_config.get("hf_token")
                if not hf_token:
                    # Try to load from .env file
                    project_root = getattr(self.config, "project_root", None)
                    if project_root and hasattr(self.config, "project_root"):
                        env_file = project_root / ".env"
                        if env_file and env_file.exists():
                            for line in env_file.read_text().splitlines():
                                if line.startswith("HF_TOKEN="):
                                    hf_token = line.split("=", 1)[1].strip().strip("\"'")
                                    break
                    # Fallback to environment variable
                    if not hf_token:
                        hf_token = os.getenv("HF_TOKEN")

                if hf_token:
                    os.environ["HF_TOKEN"] = hf_token

                logger.info(f"Loading shared embedding model: {model_name}")
                model = SentenceTransformer(model_name)
                self._shared_models[model_key] = model
                logger.info("Shared embedding model loaded successfully")

            except ImportError:
                logger.debug(
                    "Semantic similarity analysis disabled: sentence-transformers not available"
                )
                self._shared_models[model_key] = None
            except (ImportError, RuntimeError, OSError) as e:
                logger.warning(f"Failed to load embedding model: {e}")
                self._shared_models[model_key] = None

        return self._shared_models[model_key]

    def get_rule_summary(self) -> Dict[str, Any]:
        """Get summary of rule configuration."""
        all_validators = plugin_manager.get_all_validators()
        enabled_count = sum(1 for rule_id in all_validators.keys() if self.is_rule_enabled(rule_id))

        return {
            "total_rules": len(all_validators),
            "enabled_rules": enabled_count,
            "disabled_rules": len(all_validators) - enabled_count,
            "overrides": len(self._rule_overrides),
            "plugins": list(self._enabled_plugins),
        }


def create_default_rule_config() -> Dict[str, Any]:
    """Create default rule configuration for new projects."""
    return {
        "rules": {
            # Semantic rule IDs (primary system)
            "DOCSTRING-MISSING": DefaultSeverity.INFO.value,  # Missing docstring is just info
            "EXPORTS-MISSING-ALL": DefaultSeverity.WARN.value,  # Missing __all__ is warning
            "PRINT-STATEMENT": DefaultSeverity.WARN.value,  # Print statements are warnings
            "EMOJI-IN-STRING": DefaultSeverity.WARN.value,  # Emojis can cause encoding issues
            "TODO-FOUND": DefaultSeverity.INFO.value,  # TODOs are informational
            "PARAMETERS-KEYWORD-ONLY": DefaultSeverity.INFO.value,  # Parameter suggestions are info
        },
        "plugins": {"enabled": ["vibelint.core"]},
    }
```

---
### File: src/vibelint/runtime_tracer.py

```python
"""
Runtime Module Tracer for Vibelint Self-Improvement

Runs Python modules with mock values and traces all method calls with full stack traces.
Perfect for understanding execution flow and identifying optimization opportunities.
"""

import ast
import importlib
import importlib.util
import inspect
import json
import sys
import time
import traceback
from contextlib import contextmanager
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Set


@dataclass
class CallInfo:
    """Information about a function/method call."""

    function_name: str
    module_name: str
    file_path: str
    line_number: int
    call_time: float
    args: List[Any] = field(default_factory=list)
    kwargs: Dict[str, Any] = field(default_factory=dict)
    locals_snapshot: Dict[str, Any] = field(default_factory=dict)
    return_value: Any = None
    execution_time_ms: float = 0.0
    call_depth: int = 0
    caller_info: Optional[str] = None


@dataclass
class DependencyNode:
    """A node in the dependency knowledge graph."""

    function_signature: str
    module_path: str
    dependencies: List[str] = field(default_factory=list)
    code_embedding: Optional[List[float]] = None
    semantic_embedding: Optional[List[float]] = None
    execution_context: Dict[str, Any] = field(default_factory=dict)
    performance_profile: Dict[str, float] = field(default_factory=dict)


@dataclass
class TraceSession:
    """Complete tracing session data with embedding integration."""

    module_name: str
    start_time: float
    end_time: float
    total_calls: int
    call_stack: List[CallInfo] = field(default_factory=list)
    performance_hotspots: List[CallInfo] = field(default_factory=list)
    dependency_graph: Dict[str, List[str]] = field(default_factory=dict)
    dependency_knowledge_graph: Dict[str, DependencyNode] = field(default_factory=dict)
    execution_embeddings: Dict[str, List[float]] = field(default_factory=dict)


class MockProvider:
    """Provides mock values for different types and situations."""

    @staticmethod
    def create_mock_for_type(type_hint: Any, context: str = "") -> Any:
        """Create appropriate mock value based on type hint."""
        if type_hint == str:
            return f"mock_string_{context}"
        elif type_hint == int:
            return 42
        elif type_hint == float:
            return 3.14
        elif type_hint == bool:
            return True
        elif type_hint == list:
            return ["mock_item1", "mock_item2"]
        elif type_hint == dict:
            return {"mock_key": "mock_value"}
        elif type_hint == Path:
            return Path("/mock/path/file.py")
        elif hasattr(type_hint, "__origin__"):
            # Handle generic types like List[str], Dict[str, int]
            origin = type_hint.__origin__
            if origin == list:
                return ["mock_list_item"]
            elif origin == dict:
                return {"mock_dict_key": "mock_dict_value"}

        # Default mock object
        return MockProvider._create_mock_object(type_hint, context)

    @staticmethod
    def _create_mock_object(cls: type, context: str = ""):
        """Create a mock object with realistic methods."""

        class MockObject:
            def __init__(self):
                self._mock_context = context
                self._mock_class = cls.__name__ if hasattr(cls, "__name__") else str(cls)

            def __getattr__(self, name):
                return lambda *args, **kwargs: f"mock_result_from_{name}"

            def __str__(self):
                return f"Mock{self._mock_class}({self._mock_context})"

            def __repr__(self):
                return self.__str__()

        return MockObject()

    @staticmethod
    def mock_common_dependencies() -> Dict[str, Any]:
        """Create mocks for common dependencies."""
        return {
            "os": MockProvider._create_mock_module("os"),
            "sys": MockProvider._create_mock_module("sys"),
            "pathlib.Path": Path("/mock/path"),
            "requests": MockProvider._create_mock_module("requests"),
            "json": MockProvider._create_mock_module("json"),
            "time": MockProvider._create_mock_module("time"),
            "datetime": MockProvider._create_mock_module("datetime"),
            "asyncio": MockProvider._create_mock_module("asyncio"),
        }

    @staticmethod
    def _create_mock_module(module_name: str):
        """Create a mock module with common methods."""

        class MockModule:
            def __getattr__(self, name):
                if name in ["get", "post", "put", "delete"]:  # requests
                    return lambda *args, **kwargs: MockProvider._mock_response()
                elif name in ["loads", "dumps"]:  # json
                    return lambda *args, **kwargs: {"mock": "json_data"}
                elif name in ["sleep"]:  # time/asyncio
                    return lambda *args, **kwargs: None
                elif name in ["now"]:  # datetime
                    return lambda *args, **kwargs: "2024-01-01T00:00:00"
                else:
                    return lambda *args, **kwargs: f"mock_{name}_result"

        return MockModule()

    @staticmethod
    def _mock_response():
        """Mock HTTP response object."""

        class MockResponse:
            status_code = 200

            def json(self):
                return {"mock": "response"}

            def text(self):
                return "mock response text"

        return MockResponse()


class VanguardEmbeddingIntegration:
    """
    Integrates with VanguardOne (code) and VanguardTwo (semantic) embeddings
    to create a rich dependency knowledge graph.
    """

    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.vanguard_one_url = self.config.get("code_api_url")
        self.vanguard_two_url = self.config.get("natural_api_url")

    async def create_dependency_embeddings(
        self, call_stack: List[CallInfo], source_code_cache: Dict[str, str]
    ) -> Dict[str, DependencyNode]:
        """
        Create embedding-enhanced dependency nodes from call stack.
        """

        dependency_nodes = {}

        for call in call_stack:
            node_id = f"{call.module_name}.{call.function_name}"

            if node_id in dependency_nodes:
                continue

            # Extract function source code
            function_source = await self._extract_function_source(call, source_code_cache)

            # Get function signature and docstring for semantic embedding
            semantic_text = await self._create_semantic_description(call, function_source)

            # Create embeddings
            code_embedding = await self._get_code_embedding(function_source)
            semantic_embedding = await self._get_semantic_embedding(semantic_text)

            # Build dependency node
            dependencies = self._extract_dependencies_from_call(call, call_stack)

            node = DependencyNode(
                function_signature=f"{call.function_name}({', '.join(map(str, call.args))})",
                module_path=call.file_path,
                dependencies=dependencies,
                code_embedding=code_embedding,
                semantic_embedding=semantic_embedding,
                execution_context={
                    "call_frequency": self._calculate_call_frequency(node_id, call_stack),
                    "average_execution_time": call.execution_time_ms,
                    "typical_args": call.args,
                    "error_patterns": [],  # Would track exceptions
                },
                performance_profile={
                    "avg_time_ms": call.execution_time_ms,
                    "memory_impact": "unknown",  # Could be measured
                    "io_operations": self._detect_io_operations(function_source),
                },
            )

            dependency_nodes[node_id] = node

        return dependency_nodes

    async def _extract_function_source(self, call: CallInfo, source_cache: Dict[str, str]) -> str:
        """Extract the source code of a specific function."""
        try:
            if call.file_path not in source_cache:
                with open(call.file_path, "r") as f:
                    source_cache[call.file_path] = f.read()

            source = source_cache[call.file_path]

            # Parse AST to find the function
            tree = ast.parse(source)

            for node in ast.walk(tree):
                if (
                    isinstance(node, ast.FunctionDef)
                    and node.name == call.function_name
                    and node.lineno <= call.line_number
                ):

                    # Extract function source
                    lines = source.split("\n")
                    start_line = node.lineno - 1

                    # Find end of function (next function or class at same indent level)
                    end_line = len(lines)
                    base_indent = len(lines[start_line]) - len(lines[start_line].lstrip())

                    for i in range(start_line + 1, len(lines)):
                        line = lines[i]
                        if line.strip() and len(line) - len(line.lstrip()) <= base_indent:
                            if any(
                                line.strip().startswith(kw)
                                for kw in ["def ", "class ", "async def "]
                            ):
                                end_line = i
                                break

                    return "\n".join(lines[start_line:end_line])

            return f"# Function {call.function_name} not found in source"

        except Exception as e:
            return f"# Error extracting source: {e}"

    async def _create_semantic_description(self, call: CallInfo, function_source: str) -> str:
        """Create semantic description for natural language embedding."""
        # Extract docstring and comments
        docstring = ""
        comments = []

        try:
            tree = ast.parse(function_source)
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    if ast.get_docstring(node):
                        docstring = ast.get_docstring(node)
                    break
        except:
            pass

        # Create rich semantic description
        description = f"""
Function: {call.function_name}
Module: {call.module_name}
Purpose: {docstring or "No docstring available"}
Typical arguments: {call.args}
Return type: {type(call.return_value).__name__ if call.return_value else "unknown"}
Execution context: Called with depth {call.call_depth}
Performance: Executes in {call.execution_time_ms:.2f}ms on average
Dependencies: Interacts with other functions in execution flow
Usage pattern: Runtime analysis shows this function is used for {self._infer_usage_pattern(call)}
        """.strip()

        return description

    def _infer_usage_pattern(self, call: CallInfo) -> str:
        """Infer usage pattern from call context."""
        if "config" in call.function_name.lower():
            return "configuration management"
        elif "validate" in call.function_name.lower():
            return "code validation and analysis"
        elif "analyze" in call.function_name.lower():
            return "static or dynamic code analysis"
        elif "trace" in call.function_name.lower():
            return "execution tracing and monitoring"
        elif any(kw in call.function_name.lower() for kw in ["load", "read", "parse"]):
            return "data loading and parsing"
        elif any(kw in call.function_name.lower() for kw in ["save", "write", "store"]):
            return "data persistence and storage"
        else:
            return "general purpose computation"

    async def _get_code_embedding(self, source_code: str) -> Optional[List[float]]:
        """Get code embedding from VanguardOne."""
        if not self.vanguard_one_url:
            return None

        try:
            import httpx

            async with httpx.AsyncClient() as client:
                response = await client.post(
                    self.vanguard_one_url,
                    json={
                        "model": "text-embedding-ada-002",  # API expects this
                        "input": source_code[:8000],  # Limit input size
                    },
                    timeout=30.0,
                )

                if response.status_code == 200:
                    result = response.json()
                    return result.get("data", [{}])[0].get("embedding")

        except Exception as e:
            print(f"Failed to get code embedding: {e}")

        return None

    async def _get_semantic_embedding(self, semantic_text: str) -> Optional[List[float]]:
        """Get semantic embedding from VanguardTwo."""
        if not self.vanguard_two_url:
            return None

        try:
            import httpx

            async with httpx.AsyncClient() as client:
                response = await client.post(
                    self.vanguard_two_url,
                    json={
                        "model": "text-embedding-ada-002",  # API expects this
                        "input": semantic_text[:8000],  # Limit input size
                    },
                    timeout=30.0,
                )

                if response.status_code == 200:
                    result = response.json()
                    return result.get("data", [{}])[0].get("embedding")

        except Exception as e:
            print(f"Failed to get semantic embedding: {e}")

        return None

    def _extract_dependencies_from_call(
        self, call: CallInfo, all_calls: List[CallInfo]
    ) -> List[str]:
        """Extract what this function depends on based on call stack."""
        dependencies = []

        # Find calls that happen within this function's execution
        call_start_time = call.call_time
        call_end_time = call_start_time + (call.execution_time_ms / 1000)

        for other_call in all_calls:
            if (
                other_call != call
                and call_start_time <= other_call.call_time <= call_end_time
                and other_call.call_depth > call.call_depth
            ):

                dependency = f"{other_call.module_name}.{other_call.function_name}"
                if dependency not in dependencies:
                    dependencies.append(dependency)

        return dependencies

    def _calculate_call_frequency(self, node_id: str, call_stack: List[CallInfo]) -> int:
        """Calculate how often this function is called."""
        return sum(
            1 for call in call_stack if f"{call.module_name}.{call.function_name}" == node_id
        )

    def _detect_io_operations(self, source_code: str) -> int:
        """Detect I/O operations in source code."""
        io_indicators = [
            "open(",
            "read(",
            "write(",
            "requests.",
            "http",
            "json.load",
            "json.dump",
            "pickle.",
            "os.path",
            "pathlib",
            "sqlite",
            "database",
        ]

        return sum(1 for indicator in io_indicators if indicator in source_code)


class RuntimeTracer:
    """
    Advanced runtime tracer that can run modules with mocks and trace execution.
    """

    def __init__(
        self,
        include_modules: Optional[Set[str]] = None,
        exclude_modules: Optional[Set[str]] = None,
        trace_external: bool = False,
        max_call_depth: int = 50,
    ):
        self.include_modules = include_modules or set()
        self.exclude_modules = exclude_modules or {
            "sys",
            "os",
            "traceback",
            "inspect",
            "importlib",
            "__main__",
            "typing",
            "dataclasses",
        }
        self.trace_external = trace_external
        self.max_call_depth = max_call_depth

        self.call_stack: List[CallInfo] = []
        self.current_depth = 0
        self.session_start = 0.0
        self.call_times = {}

    def should_trace_frame(self, frame) -> bool:
        """Determine if we should trace this frame."""
        filename = frame.f_code.co_filename
        module_name = frame.f_globals.get("__name__", "")

        # Skip if too deep
        if self.current_depth > self.max_call_depth:
            return False

        # Skip built-in modules
        if filename.startswith("<") or "site-packages" in filename:
            return not self.trace_external

        # Include specific modules
        if self.include_modules:
            return any(
                inc_mod in module_name or inc_mod in filename for inc_mod in self.include_modules
            )

        # Exclude specific modules
        return not any(
            exc_mod in module_name or exc_mod in filename for exc_mod in self.exclude_modules
        )

    def trace_function(self, frame, event, arg):
        """Main tracing function."""
        if not self.should_trace_frame(frame):
            return self.trace_function

        filename = frame.f_code.co_filename
        function_name = frame.f_code.co_name
        line_number = frame.f_lineno
        module_name = frame.f_globals.get("__name__", "")

        if event == "call":
            self.current_depth += 1

            # Get function arguments
            args, kwargs = self._extract_call_args(frame)

            # Get caller information
            caller_info = self._get_caller_info()

            call_info = CallInfo(
                function_name=function_name,
                module_name=module_name,
                file_path=filename,
                line_number=line_number,
                call_time=time.time(),
                args=args,
                kwargs=kwargs,
                locals_snapshot=dict(frame.f_locals),
                call_depth=self.current_depth,
                caller_info=caller_info,
            )

            self.call_stack.append(call_info)

        elif event == "return":
            self.current_depth = max(0, self.current_depth - 1)

            # Update the most recent call with return value and timing
            if self.call_stack:
                recent_call = None
                for call in reversed(self.call_stack):
                    if (
                        call.function_name == function_name
                        and call.file_path == filename
                        and call.return_value is None
                    ):
                        recent_call = call
                        break

                if recent_call:
                    recent_call.return_value = arg
                    recent_call.execution_time_ms = (time.time() - recent_call.call_time) * 1000

        return self.trace_function

    def _extract_call_args(self, frame) -> tuple[List[Any], Dict[str, Any]]:
        """Extract function arguments from frame."""
        try:
            arginfo = inspect.getargvalues(frame)
            args = []
            kwargs = {}

            for arg_name in arginfo.args:
                value = arginfo.locals.get(arg_name)
                args.append(self._sanitize_value(value))

            # Handle keyword arguments
            if arginfo.keywords:
                keyword_args = arginfo.locals.get(arginfo.keywords, {})
                kwargs.update({k: self._sanitize_value(v) for k, v in keyword_args.items()})

            return args, kwargs

        except Exception:
            return [], {}

    def _get_caller_info(self) -> str:
        """Get information about who called this function."""
        try:
            stack = inspect.stack()
            # Skip trace frames
            for frame_info in stack[3:]:  # Skip trace_function, _extract_call_args, etc.
                if not any(skip in frame_info.filename for skip in ["trace", "inspect"]):
                    return f"{frame_info.filename}:{frame_info.lineno} in {frame_info.function}"
            return "unknown_caller"
        except:
            return "unknown_caller"

    def _sanitize_value(self, value: Any) -> Any:
        """Sanitize value for JSON serialization."""
        try:
            if isinstance(value, (str, int, float, bool, type(None))):
                return value
            elif isinstance(value, (list, tuple)):
                return [self._sanitize_value(v) for v in value[:3]]  # Limit to first 3 items
            elif isinstance(value, dict):
                return {k: self._sanitize_value(v) for k, v in list(value.items())[:3]}
            elif hasattr(value, "__dict__"):
                return f"<{type(value).__name__} object>"
            else:
                return str(value)[:100]  # Truncate long strings
        except:
            return "<unserializable>"

    @contextmanager
    def trace_execution(self):
        """Context manager for tracing execution."""
        self.session_start = time.time()
        old_trace = sys.gettrace()

        try:
            sys.settrace(self.trace_function)
            yield self
        finally:
            sys.settrace(old_trace)

    def run_module_with_mocks(
        self,
        module_path: Path,
        function_name: str = None,
        mock_args: List[Any] = None,
        mock_kwargs: Dict[str, Any] = None,
    ) -> TraceSession:
        """
        Run a module or specific function with mocks and trace execution.
        """
        print(f"🔍 Tracing execution of {module_path}")

        # Prepare mocks
        mocks = MockProvider.mock_common_dependencies()

        # Add module to include set
        self.include_modules.add(module_path.stem)

        session = TraceSession(
            module_name=str(module_path), start_time=time.time(), end_time=0.0, total_calls=0
        )

        try:
            with self.trace_execution():
                # Import the module
                spec = importlib.util.spec_from_file_location(module_path.stem, module_path)
                module = importlib.util.module_from_spec(spec)

                # Inject mocks into module namespace
                for mock_name, mock_obj in mocks.items():
                    if "." not in mock_name:
                        setattr(module, mock_name, mock_obj)

                # Execute the module
                spec.loader.exec_module(module)

                # Run specific function if specified
                if function_name:
                    if hasattr(module, function_name):
                        func = getattr(module, function_name)

                        # Prepare arguments
                        if mock_args is None:
                            mock_args = self._generate_mock_args_for_function(func)
                        if mock_kwargs is None:
                            mock_kwargs = {}

                        print(f"📞 Calling {function_name}({mock_args}, {mock_kwargs})")
                        result = func(*mock_args, **mock_kwargs)
                        print(f"📊 Function returned: {result}")

        except Exception as e:
            print(f"❌ Execution failed: {e}")
            traceback.print_exc()

        session.end_time = time.time()
        session.total_calls = len(self.call_stack)
        session.call_stack = self.call_stack.copy()

        # Analyze performance hotspots
        session.performance_hotspots = sorted(
            [call for call in self.call_stack if call.execution_time_ms > 1.0],
            key=lambda x: x.execution_time_ms,
            reverse=True,
        )[:10]

        # Build dependency graph
        session.dependency_graph = self._build_dependency_graph()

        return session

    def _generate_mock_args_for_function(self, func: Callable) -> List[Any]:
        """Generate mock arguments for a function based on its signature."""
        try:
            sig = inspect.signature(func)
            mock_args = []

            for param_name, param in sig.parameters.items():
                if param.kind == param.VAR_POSITIONAL:
                    break  # Don't mock *args
                elif param.kind == param.VAR_KEYWORD:
                    break  # Don't mock **kwargs

                # Create mock based on annotation or default
                if param.annotation != param.empty:
                    mock_value = MockProvider.create_mock_for_type(param.annotation, param_name)
                elif param.default != param.empty:
                    mock_value = param.default
                else:
                    mock_value = f"mock_{param_name}"

                mock_args.append(mock_value)

            return mock_args

        except Exception:
            return ["mock_arg"]

    def _build_dependency_graph(self) -> Dict[str, List[str]]:
        """Build a dependency graph from call stack."""
        graph = {}

        for call in self.call_stack:
            caller = call.caller_info or "root"
            callee = f"{call.module_name}.{call.function_name}"

            if caller not in graph:
                graph[caller] = []

            if callee not in graph[caller]:
                graph[caller].append(callee)

        return graph

    def export_trace_report(self, session: TraceSession, output_path: Path):
        """Export detailed trace report."""
        report = {
            "session_info": {
                "module": session.module_name,
                "duration_ms": (session.end_time - session.start_time) * 1000,
                "total_calls": session.total_calls,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            },
            "call_stack": [
                {
                    "function": f"{call.module_name}.{call.function_name}",
                    "file": call.file_path,
                    "line": call.line_number,
                    "execution_time_ms": call.execution_time_ms,
                    "call_depth": call.call_depth,
                    "args": call.args,
                    "kwargs": call.kwargs,
                    "return_value": call.return_value,
                    "caller": call.caller_info,
                }
                for call in session.call_stack
            ],
            "performance_hotspots": [
                {
                    "function": f"{call.module_name}.{call.function_name}",
                    "execution_time_ms": call.execution_time_ms,
                    "file": call.file_path,
                    "line": call.line_number,
                }
                for call in session.performance_hotspots
            ],
            "dependency_graph": session.dependency_graph,
        }

        with open(output_path, "w") as f:
            json.dump(report, f, indent=2, default=str)

        print(f"📄 Trace report exported to: {output_path}")


# Convenience functions


def trace_vibelint_module(module_name: str, function_name: str = None) -> TraceSession:
    """Trace a vibelint module with automatic setup."""
    vibelint_src = Path(__file__).parent
    module_path = vibelint_src / f"{module_name}.py"

    if not module_path.exists():
        raise FileNotFoundError(f"Module not found: {module_path}")

    tracer = RuntimeTracer(include_modules={"vibelint"}, trace_external=False, max_call_depth=20)

    return tracer.run_module_with_mocks(module_path, function_name)


def analyze_self_improvement_execution():
    """Trace the self-improvement system to understand its execution flow."""
    print("🧠 Analyzing vibelint self-improvement execution...")

    session = trace_vibelint_module("self_improvement", "run_vibelint_self_improvement")

    # Export trace report
    report_path = (
        Path(__file__).parent.parent.parent / ".vibelint-self-improvement" / "execution_trace.json"
    )
    report_path.parent.mkdir(exist_ok=True)

    tracer = RuntimeTracer()
    tracer.export_trace_report(session, report_path)

    print("🎯 Performance hotspots found:")
    for hotspot in session.performance_hotspots[:5]:
        print(f"  - {hotspot.function_name}: {hotspot.execution_time_ms:.2f}ms")

    return session


if __name__ == "__main__":
    # Example usage
    analyze_self_improvement_execution()
```

---
### File: src/vibelint/self_improvement.py

```python
"""
Vibelint Self-Improvement System

Makes vibelint evolve to fix its own efficiency issues by:
1. Analyzing its own performance and code quality
2. Identifying improvement opportunities
3. Automatically implementing fixes
4. Learning from validation patterns

This is the foundation for autonomous evolution.
"""

import ast
import asyncio
import json
import logging
import time
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import List, Optional


@dataclass
class PerformanceMetrics:
    """Performance metrics for vibelint operations."""

    validation_time_ms: float
    files_processed: int
    violations_found: int
    memory_usage_mb: float
    llm_calls_made: int
    cache_hit_rate: float
    error_rate: float


@dataclass
class ImprovementOpportunity:
    """An identified opportunity for self-improvement."""

    opportunity_id: str
    category: str  # "performance", "code_quality", "validation_accuracy"
    description: str
    current_metric: float
    target_metric: float
    estimated_impact: str  # "high", "medium", "low"
    implementation_complexity: str  # "simple", "moderate", "complex"
    suggested_fix: str
    code_location: Optional[str] = None


@dataclass
class SelfImprovementResult:
    """Result of a self-improvement attempt."""

    opportunity_id: str
    implemented: bool
    before_metrics: PerformanceMetrics
    after_metrics: Optional[PerformanceMetrics]
    implementation_notes: str
    success_score: float  # 0.0 to 1.0


class VibelintSelfImprover:
    """
    Autonomous self-improvement system for vibelint.

    Continuously analyzes vibelint's own performance and implements improvements.
    """

    def __init__(self, vibelint_src_path: Path):
        self.vibelint_src_path = vibelint_src_path
        self.performance_history: List[PerformanceMetrics] = []
        self.improvement_history: List[SelfImprovementResult] = []
        self.logger = logging.getLogger(__name__)

        # Create improvement tracking directory
        self.improvement_dir = vibelint_src_path.parent.parent / ".vibelint-self-improvement"
        self.improvement_dir.mkdir(exist_ok=True)

    def measure_current_performance(self) -> PerformanceMetrics:
        """Measure current vibelint performance by running it on itself."""
        start_time = time.time()

        try:
            # Run vibelint on its own source code
            from vibelint.config import load_config
            from vibelint.core import AnalysisRequest, create_dynamic_analyzer

            config = load_config(self.vibelint_src_path)
            analyzer = create_dynamic_analyzer(config.settings)

            if not analyzer:
                # No LLM configured, use simpler static analysis
                violations = []
                validation_time = 0
            else:
                # Analyze all Python files in vibelint src
                python_files = list(self.vibelint_src_path.rglob("*.py"))

                validation_start = time.time()
                all_findings = []

                for python_file in python_files:
                    try:
                        with open(python_file, "r") as f:
                            content = f.read()

                        request = AnalysisRequest(
                            file_path=python_file,
                            content=content,
                            analysis_types=["code_smells", "architecture", "complexity"],
                        )

                        result = analyzer.analyze(request)
                        all_findings.extend(result.findings)

                    except Exception as e:
                        self.logger.warning(f"Failed to analyze {python_file}: {e}")

                validation_time = (time.time() - validation_start) * 1000
                violations = [{"file": f.file_path, "message": f.message} for f in all_findings]

            # Mock additional metrics (would be real in full implementation)
            metrics = PerformanceMetrics(
                validation_time_ms=validation_time,
                files_processed=len(python_files),
                violations_found=len(violations),
                memory_usage_mb=50.0,  # Would measure actual memory usage
                llm_calls_made=0,  # Track actual LLM calls
                cache_hit_rate=0.8,  # Track cache performance
                error_rate=0.0,
            )

            self.performance_history.append(metrics)
            return metrics

        except Exception as e:
            self.logger.error(f"Failed to measure performance: {e}")
            # Return degraded metrics on failure
            return PerformanceMetrics(
                validation_time_ms=float("inf"),
                files_processed=0,
                violations_found=0,
                memory_usage_mb=0.0,
                llm_calls_made=0,
                cache_hit_rate=0.0,
                error_rate=1.0,
            )

    def analyze_own_code_quality(self) -> List[ImprovementOpportunity]:
        """Analyze vibelint's own code to find improvement opportunities."""
        opportunities = []

        # Analyze Python files in vibelint
        for python_file in self.vibelint_src_path.rglob("*.py"):
            file_opportunities = self._analyze_file_for_improvements(python_file)
            opportunities.extend(file_opportunities)

        # Analyze performance patterns
        performance_opportunities = self._analyze_performance_patterns()
        opportunities.extend(performance_opportunities)

        return opportunities

    def _analyze_file_for_improvements(self, file_path: Path) -> List[ImprovementOpportunity]:
        """Analyze a single file for improvement opportunities."""
        opportunities = []

        try:
            with open(file_path, "r") as f:
                source_code = f.read()

            # Parse AST for analysis
            tree = ast.parse(source_code)

            # Check for specific improvement patterns
            for node in ast.walk(tree):
                # Find inefficient patterns
                if isinstance(node, ast.For):
                    # Look for potential list comprehension opportunities
                    if self._is_simple_for_loop(node):
                        opportunities.append(
                            ImprovementOpportunity(
                                opportunity_id=f"list_comp_{file_path.name}_{node.lineno}",
                                category="performance",
                                description=f"For loop at line {node.lineno} could be a list comprehension",
                                current_metric=1.0,  # Current complexity
                                target_metric=0.5,  # Target complexity
                                estimated_impact="low",
                                implementation_complexity="simple",
                                suggested_fix="Convert to list comprehension for better performance",
                                code_location=f"{file_path}:{node.lineno}",
                            )
                        )

                elif isinstance(node, ast.FunctionDef):
                    # Check function complexity
                    complexity = self._calculate_function_complexity(node)
                    if complexity > 10:  # McCabe complexity threshold
                        opportunities.append(
                            ImprovementOpportunity(
                                opportunity_id=f"complex_func_{file_path.name}_{node.name}",
                                category="code_quality",
                                description=f"Function '{node.name}' has high complexity ({complexity})",
                                current_metric=complexity,
                                target_metric=8.0,
                                estimated_impact="medium",
                                implementation_complexity="moderate",
                                suggested_fix="Break function into smaller functions",
                                code_location=f"{file_path}:{node.lineno}",
                            )
                        )

                elif isinstance(node, ast.Import) or isinstance(node, ast.ImportFrom):
                    # Check for unused imports
                    if self._is_unused_import(node, source_code):
                        opportunities.append(
                            ImprovementOpportunity(
                                opportunity_id=f"unused_import_{file_path.name}_{node.lineno}",
                                category="code_quality",
                                description=f"Unused import at line {node.lineno}",
                                current_metric=1.0,
                                target_metric=0.0,
                                estimated_impact="low",
                                implementation_complexity="simple",
                                suggested_fix="Remove unused import",
                                code_location=f"{file_path}:{node.lineno}",
                            )
                        )

        except Exception as e:
            self.logger.warning(f"Failed to analyze {file_path}: {e}")

        return opportunities

    def _analyze_performance_patterns(self) -> List[ImprovementOpportunity]:
        """Analyze performance metrics to find improvement opportunities."""
        opportunities = []

        if len(self.performance_history) < 2:
            return opportunities

        # Analyze performance trends
        recent_metrics = self.performance_history[-5:]  # Last 5 runs
        avg_validation_time = sum(m.validation_time_ms for m in recent_metrics) / len(
            recent_metrics
        )

        # If validation is getting slower
        if len(recent_metrics) >= 2:
            trend = recent_metrics[-1].validation_time_ms - recent_metrics[0].validation_time_ms
            if trend > 100:  # Getting 100ms+ slower
                opportunities.append(
                    ImprovementOpportunity(
                        opportunity_id="performance_degradation",
                        category="performance",
                        description="Validation performance is degrading over time",
                        current_metric=avg_validation_time,
                        target_metric=avg_validation_time * 0.8,
                        estimated_impact="high",
                        implementation_complexity="moderate",
                        suggested_fix="Profile and optimize validation pipeline",
                    )
                )

        # Check cache hit rate
        avg_cache_rate = sum(m.cache_hit_rate for m in recent_metrics) / len(recent_metrics)
        if avg_cache_rate < 0.7:  # Less than 70% cache hits
            opportunities.append(
                ImprovementOpportunity(
                    opportunity_id="low_cache_efficiency",
                    category="performance",
                    description="Cache hit rate is below optimal threshold",
                    current_metric=avg_cache_rate,
                    target_metric=0.85,
                    estimated_impact="medium",
                    implementation_complexity="moderate",
                    suggested_fix="Improve caching strategy and cache key generation",
                )
            )

        return opportunities

    def _is_simple_for_loop(self, node: ast.For) -> bool:
        """Check if a for loop could be converted to list comprehension."""
        # Simple heuristic: single statement that appends to a list
        if len(node.body) == 1:
            stmt = node.body[0]
            if isinstance(stmt, ast.Expr) and isinstance(stmt.value, ast.Call):
                if hasattr(stmt.value.func, "attr") and stmt.value.func.attr == "append":
                    return True
        return False

    def _calculate_function_complexity(self, node: ast.FunctionDef) -> int:
        """Calculate simplified McCabe complexity for a function."""
        complexity = 1  # Base complexity

        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.Try, ast.With)):
                complexity += 1
            elif isinstance(child, ast.BoolOp):
                complexity += len(child.values) - 1

        return complexity

    def _is_unused_import(self, node: ast.Import | ast.ImportFrom, source_code: str) -> bool:
        """Simple check for unused imports."""
        # This is a simplified check - a full implementation would use proper name resolution
        if isinstance(node, ast.Import):
            for alias in node.names:
                name = alias.asname or alias.name
                if name not in source_code:
                    return True
        elif isinstance(node, ast.ImportFrom):
            for alias in node.names:
                name = alias.asname or alias.name
                if name not in source_code:
                    return True
        return False

    async def implement_improvement(
        self, opportunity: ImprovementOpportunity
    ) -> SelfImprovementResult:
        """Implement a specific improvement opportunity."""
        self.logger.info(f"Implementing improvement: {opportunity.opportunity_id}")

        before_metrics = self.measure_current_performance()

        try:
            success = False
            implementation_notes = ""

            if opportunity.category == "performance":
                success = await self._implement_performance_improvement(opportunity)
                implementation_notes = "Applied performance optimization"

            elif opportunity.category == "code_quality":
                success = await self._implement_code_quality_improvement(opportunity)
                implementation_notes = "Applied code quality improvement"

            elif opportunity.category == "validation_accuracy":
                success = await self._implement_validation_improvement(opportunity)
                implementation_notes = "Applied validation accuracy improvement"

            # Measure performance after implementation
            after_metrics = self.measure_current_performance() if success else None

            # Calculate success score
            success_score = self._calculate_success_score(
                before_metrics, after_metrics, opportunity
            )

            result = SelfImprovementResult(
                opportunity_id=opportunity.opportunity_id,
                implemented=success,
                before_metrics=before_metrics,
                after_metrics=after_metrics,
                implementation_notes=implementation_notes,
                success_score=success_score,
            )

            self.improvement_history.append(result)
            self._save_improvement_record(result)

            return result

        except Exception as e:
            self.logger.error(f"Failed to implement improvement {opportunity.opportunity_id}: {e}")
            return SelfImprovementResult(
                opportunity_id=opportunity.opportunity_id,
                implemented=False,
                before_metrics=before_metrics,
                after_metrics=None,
                implementation_notes=f"Implementation failed: {e}",
                success_score=0.0,
            )

    async def _implement_performance_improvement(self, opportunity: ImprovementOpportunity) -> bool:
        """Implement performance-related improvements."""
        if "cache" in opportunity.description.lower():
            # Improve caching
            return await self._improve_caching_system()
        elif "validation" in opportunity.description.lower():
            # Optimize validation pipeline
            return await self._optimize_validation_pipeline()
        return False

    async def _implement_code_quality_improvement(
        self, opportunity: ImprovementOpportunity
    ) -> bool:
        """Implement code quality improvements."""
        if opportunity.code_location:
            file_path, line_no = opportunity.code_location.split(":")
            return await self._apply_local_code_fix(Path(file_path), int(line_no), opportunity)
        return False

    async def _implement_validation_improvement(self, opportunity: ImprovementOpportunity) -> bool:
        """Implement validation accuracy improvements."""
        # This would implement validation rule improvements
        return True

    async def _improve_caching_system(self) -> bool:
        """Improve the caching system."""
        # Example: Add better cache key generation
        cache_file = self.vibelint_src_path / "caching.py"
        if cache_file.exists():
            # Would implement actual cache improvements
            self.logger.info("Improved caching system")
            return True
        return False

    async def _optimize_validation_pipeline(self) -> bool:
        """Optimize the validation pipeline."""
        # Example: Add parallel processing
        core_file = self.vibelint_src_path / "core.py"
        if core_file.exists():
            # Would implement actual pipeline optimizations
            self.logger.info("Optimized validation pipeline")
            return True
        return False

    async def _apply_local_code_fix(
        self, file_path: Path, line_no: int, opportunity: ImprovementOpportunity
    ) -> bool:
        """Apply a specific code fix to a file."""
        try:
            with open(file_path, "r") as f:
                lines = f.readlines()

            # Apply simple fixes based on opportunity type
            if "unused_import" in opportunity.opportunity_id:
                # Remove the import line
                if 0 <= line_no - 1 < len(lines):
                    lines[line_no - 1] = ""  # Remove the line

                    with open(file_path, "w") as f:
                        f.writelines(lines)

                    self.logger.info(f"Removed unused import at {file_path}:{line_no}")
                    return True

            elif "list_comp" in opportunity.opportunity_id:
                # Would implement list comprehension conversion
                self.logger.info(
                    f"Applied list comprehension optimization at {file_path}:{line_no}"
                )
                return True

        except Exception as e:
            self.logger.error(f"Failed to apply code fix: {e}")

        return False

    def _calculate_success_score(
        self,
        before: PerformanceMetrics,
        after: Optional[PerformanceMetrics],
        opportunity: ImprovementOpportunity,
    ) -> float:
        """Calculate success score for an improvement."""
        if not after:
            return 0.0

        # Calculate improvement based on category
        if opportunity.category == "performance":
            if after.validation_time_ms < before.validation_time_ms:
                improvement = (
                    before.validation_time_ms - after.validation_time_ms
                ) / before.validation_time_ms
                return min(1.0, improvement * 2)  # Scale to 0-1

        elif opportunity.category == "code_quality":
            if after.error_rate < before.error_rate:
                return 0.8  # Good improvement for code quality

        return 0.5  # Neutral score if no clear improvement

    def _save_improvement_record(self, result: SelfImprovementResult):
        """Save improvement record to disk."""
        record_file = self.improvement_dir / f"improvement_{result.opportunity_id}.json"

        with open(record_file, "w") as f:
            json.dump(asdict(result), f, indent=2, default=str)

    async def run_continuous_improvement(
        self, max_iterations: int = 10
    ) -> List[SelfImprovementResult]:
        """Run continuous self-improvement loop."""
        self.logger.info("Starting continuous self-improvement")

        all_results = []

        for iteration in range(max_iterations):
            self.logger.info(f"Self-improvement iteration {iteration + 1}/{max_iterations}")

            # Measure current performance
            current_metrics = self.measure_current_performance()
            self.logger.info(
                f"Current performance: {current_metrics.validation_time_ms:.2f}ms, "
                f"{current_metrics.violations_found} violations"
            )

            # Find improvement opportunities
            opportunities = self.analyze_own_code_quality()

            if not opportunities:
                self.logger.info("No improvement opportunities found")
                break

            # Sort by estimated impact and implementation complexity
            opportunities.sort(
                key=lambda x: (
                    {"high": 3, "medium": 2, "low": 1}[x.estimated_impact],
                    -{"simple": 3, "moderate": 2, "complex": 1}[x.implementation_complexity],
                ),
                reverse=True,
            )

            # Implement top opportunity
            top_opportunity = opportunities[0]
            self.logger.info(f"Implementing: {top_opportunity.description}")

            result = await self.implement_improvement(top_opportunity)
            all_results.append(result)

            if result.success_score > 0.7:
                self.logger.info(f"Successful improvement! Score: {result.success_score:.2f}")
            else:
                self.logger.warning(
                    f"Improvement had limited success. Score: {result.success_score:.2f}"
                )

            # Small delay between iterations
            await asyncio.sleep(1)

        self.logger.info(f"Completed {len(all_results)} improvement attempts")
        return all_results


# Convenience function to run self-improvement on vibelint
async def run_vibelint_self_improvement():
    """Run self-improvement on vibelint itself."""
    vibelint_src = Path(__file__).parent
    improver = VibelintSelfImprover(vibelint_src)

    print("🔧 Starting vibelint self-improvement...")

    # Initial performance measurement
    initial_metrics = improver.measure_current_performance()
    print(f"📊 Initial performance: {initial_metrics.validation_time_ms:.2f}ms")

    # Run improvements
    results = await improver.run_continuous_improvement(max_iterations=5)

    # Final performance measurement
    final_metrics = improver.measure_current_performance()
    print(f"📊 Final performance: {final_metrics.validation_time_ms:.2f}ms")

    # Summary
    successful_improvements = [r for r in results if r.success_score > 0.5]
    print(f"✅ Successfully implemented {len(successful_improvements)}/{len(results)} improvements")

    if final_metrics.validation_time_ms < initial_metrics.validation_time_ms:
        improvement_pct = (
            (initial_metrics.validation_time_ms - final_metrics.validation_time_ms)
            / initial_metrics.validation_time_ms
            * 100
        )
        print(f"🚀 Overall performance improved by {improvement_pct:.1f}%")

    return results


if __name__ == "__main__":
    asyncio.run(run_vibelint_self_improvement())
```

---
### File: src/vibelint/snapshot.py

```python
# vibelint/src/vibelint/snapshot.py
"""
Codebase snapshot generation in markdown format.

vibelint/src/vibelint/snapshot.py
"""

import fnmatch
import logging
from pathlib import Path

from vibelint.config import Config
from vibelint.discovery import discover_files
from vibelint.utils import get_relative_path, is_binary

__all__ = ["create_snapshot"]

logger = logging.getLogger(__name__)

# Constants for file tree structure
FILES_KEY = "__FILES__"


def create_snapshot(
    output_path: Path,
    target_paths: list[Path],
    config: Config,
) -> None:
    """
    Creates a Markdown snapshot file containing the project structure and file contents,
    respecting the include/exclude rules defined in pyproject.toml.

    Args:
    output_path: The path where the Markdown file will be saved.
    target_paths: List of initial paths (files or directories) to discover from.
    config: The vibelint configuration object.

    vibelint/src/vibelint/snapshot.py
    """

    assert config.project_root is not None, "Project root must be set before creating snapshot."
    project_root = config.project_root.resolve()

    absolute_output_path = output_path.resolve()

    logger.debug("create_snapshot: Running discovery based on pyproject.toml config...")

    discovered_files = discover_files(
        paths=target_paths,
        config=config,
        explicit_exclude_paths={absolute_output_path},
    )

    logger.debug(f"create_snapshot: Discovery finished, count: {len(discovered_files)}")

    # Debugging check (can be removed later)
    for excluded_pattern_root in [".pytest_cache", ".ruff_cache", ".git"]:
        present = any(excluded_pattern_root in str(f) for f in discovered_files)
        logger.debug(
            f"!!! Check @ start of create_snapshot: '{excluded_pattern_root}' presence in list: {present}"
        )

    file_infos: list[tuple[Path, str]] = []

    peek_globs = config.get("peek_globs", [])
    if not isinstance(peek_globs, list):
        logger.warning("Configuration 'peek_globs' is not a list. Ignoring peek rules.")
        peek_globs = []

    for abs_file_path in discovered_files:
        try:
            rel_path_obj = get_relative_path(abs_file_path, project_root)
            rel_path_str = str(rel_path_obj)  # Still useful for fnmatch below
        except ValueError:
            logger.warning(
                f"Skipping file outside project root during snapshot categorization: {abs_file_path}"
            )
            continue

        if is_binary(abs_file_path):
            cat = "BINARY"
        else:
            cat = "FULL"
            for pk in peek_globs:
                normalized_rel_path = rel_path_str.replace("\\", "/")
                normalized_peek_glob = pk.replace("\\", "/")
                if fnmatch.fnmatch(normalized_rel_path, normalized_peek_glob):
                    cat = "PEEK"
                    break
        file_infos.append((abs_file_path, cat))
        logger.debug(f"Categorized {rel_path_str} as {cat}")

    file_infos.sort(key=lambda x: x[0])

    logger.debug(f"Sorted {len(file_infos)} files for snapshot.")

    # Build the tree structure using a dictionary
    tree: dict = {}
    for f_path, f_cat in file_infos:
        try:
            # --- FIX START ---
            # Get the relative path object
            relative_path_obj = get_relative_path(f_path, project_root)
            # Use the .parts attribute which is OS-independent
            relative_parts = relative_path_obj.parts
            # --- FIX END ---
        except ValueError:
            # Handle files outside the project root if they somehow got here
            logger.warning(
                f"Skipping file outside project root during snapshot tree build: {f_path}"
            )
            continue

        node = tree
        # Iterate through the path components tuple
        for i, part in enumerate(relative_parts):
            # Skip empty parts if any somehow occur (unlikely with .parts)
            if not part:
                continue

            is_last_part = i == len(relative_parts) - 1

            if is_last_part:
                # This is the filename part
                if FILES_KEY not in node:
                    node[FILES_KEY] = []
                # Add the tuple (absolute path, category)
                node[FILES_KEY].append((f_path, f_cat))
            else:
                # This is a directory part
                if part not in node:
                    node[part] = {}  # Create a new dictionary for the subdirectory
                # Move deeper into the tree structure
                node = node[part]

    logger.info(f"Writing snapshot to {output_path}")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    try:
        with open(absolute_output_path, "w", encoding="utf-8") as outfile:

            outfile.write("# Snapshot\n\n")

            # Write Filesystem Tree section
            outfile.write("## Filesystem Tree\n\n```\n")
            # Use project root name for the tree root display
            tree_root_name = project_root.name if project_root.name else str(project_root)
            outfile.write(f"{tree_root_name}/\n")
            _write_tree(outfile, tree, "")  # Pass the populated tree dictionary
            outfile.write("```\n\n")

            # Write File Contents section
            outfile.write("## File Contents\n\n")
            outfile.write("Files are ordered alphabetically by path.\n\n")
            for f, cat in file_infos:  # Iterate through the sorted list again
                try:
                    relpath_header = get_relative_path(f, project_root)
                    outfile.write(f"### File: {relpath_header}\n\n")
                    logger.debug(f"Writing content for {relpath_header} (Category: {cat})")

                    if cat == "BINARY":
                        outfile.write("```\n")
                        outfile.write("[Binary File - Content not displayed]\n")
                        outfile.write("```\n\n---\n")
                    elif cat == "PEEK":
                        outfile.write("```\n")
                        outfile.write("[PEEK - Content truncated]\n")
                        try:
                            with open(f, encoding="utf-8", errors="ignore") as infile:
                                lines_read = 0
                                for line in infile:
                                    if lines_read >= 10:  # Peek limit (e.g., 10 lines)
                                        outfile.write("...\n")
                                        break
                                    outfile.write(line)
                                    lines_read += 1
                        except (OSError, UnicodeDecodeError) as e:
                            logger.warning(f"Error reading file for peek {relpath_header}: {e}")
                            outfile.write(f"[Error reading file for peek: {e}]\n")
                        outfile.write("```\n\n---\n")
                    else:  # cat == "FULL"
                        lang = _get_language(f)
                        outfile.write(f"```{lang}\n")
                        try:
                            with open(f, encoding="utf-8", errors="ignore") as infile:
                                content = infile.read()
                                # Ensure final newline for cleaner markdown rendering
                                if not content.endswith("\n"):
                                    content += "\n"
                                outfile.write(content)
                        except (OSError, UnicodeDecodeError) as e:
                            logger.warning(f"Error reading file content {relpath_header}: {e}")
                            outfile.write(f"[Error reading file: {e}]\n")
                        outfile.write("```\n\n---\n")

                except (OSError, ValueError, TypeError) as e:
                    # General error handling for processing a single file entry
                    try:
                        relpath_header_err = get_relative_path(f, project_root)
                    except ValueError:
                        relpath_header_err = str(f)  # Fallback to absolute path if rel path fails

                    logger.error(
                        f"Error processing file entry for {relpath_header_err} in snapshot: {e}",
                        exc_info=True,
                    )
                    outfile.write(f"### File: {relpath_header_err} (Error)\n\n")
                    outfile.write(f"[Error processing file entry: {e}]\n\n---\n")

            # Add a final newline for good measure
            outfile.write("\n")

    except OSError as e:
        # Error writing the main output file
        logger.error(f"Failed to write snapshot file {absolute_output_path}: {e}", exc_info=True)
        raise  # Re-raise IOErrors
    except (ValueError, TypeError, RuntimeError) as e:
        # Catch-all for other unexpected errors during writing
        logger.error(f"An unexpected error occurred during snapshot writing: {e}", exc_info=True)
        raise  # Re-raise other critical exceptions


def _write_tree(outfile, node: dict, prefix=""):
    """
    Helper function to recursively write the directory tree structure
    from the prepared dictionary.

    Args:
        outfile: The file object to write to.
        node: The current dictionary node representing a directory.
        prefix: The string prefix for drawing tree lines.

    vibelint/src/vibelint/snapshot.py
    """
    # Separate directories (keys other than FILES_KEY) from files (items in FILES_KEY)
    dirs = sorted([k for k in node if k != FILES_KEY])
    files_data: list[tuple[Path, str]] = sorted(node.get(FILES_KEY, []), key=lambda x: x[0].name)

    # Combine directory names and file names for iteration order
    entries = dirs + [f_info[0].name for f_info in files_data]

    for i, name in enumerate(entries):
        is_last = i == len(entries) - 1
        connector = "└── " if is_last else "├── "
        outfile.write(f"{prefix}{connector}")

        if name in dirs:
            # It's a directory - write its name and recurse
            outfile.write(f"{name}/\n")
            new_prefix = prefix + ("    " if is_last else "│   ")
            _write_tree(outfile, node[name], new_prefix)  # Recurse into the sub-dictionary
        else:
            # It's a file - find its category and write name with indicators
            file_info_tuple = next((info for info in files_data if info[0].name == name), None)
            file_cat = "FULL"  # Default category
            if file_info_tuple:
                file_cat = file_info_tuple[1]  # Get category ('FULL', 'PEEK', 'BINARY')

            # Add indicators for non-full content files
            peek_indicator = " (PEEK)" if file_cat == "PEEK" else ""
            binary_indicator = " (BINARY)" if file_cat == "BINARY" else ""
            outfile.write(f"{name}{peek_indicator}{binary_indicator}\n")


def _get_language(file_path: Path) -> str:
    """
    Guess language for syntax highlighting based on extension.
    Returns an empty string if no specific language is known.

    Args:
        file_path: The path to the file.

    Returns:
        A string representing the language identifier for markdown code blocks,
        or an empty string.

    vibelint/src/vibelint/snapshot.py
    """
    ext = file_path.suffix.lower()
    # Mapping from file extension to markdown language identifier
    mapping = {
        ".py": "python",
        ".js": "javascript",
        ".ts": "typescript",
        ".java": "java",
        ".c": "c",
        ".cpp": "cpp",
        ".cs": "csharp",
        ".go": "go",
        ".rs": "rust",
        ".rb": "ruby",
        ".php": "php",
        ".swift": "swift",
        ".kt": "kotlin",
        ".scala": "scala",
        ".html": "html",
        ".css": "css",
        ".scss": "scss",
        ".less": "less",
        ".json": "json",
        ".xml": "xml",
        ".yaml": "yaml",
        ".yml": "yaml",
        ".md": "markdown",
        ".sh": "bash",
        ".ps1": "powershell",
        ".bat": "batch",
        ".sql": "sql",
        ".dockerfile": "dockerfile",
        ".toml": "toml",
        ".ini": "ini",
        ".cfg": "ini",
        ".gitignore": "gitignore",
        ".env": "bash",  # Treat .env like bash for highlighting often
        ".tf": "terraform",
        ".hcl": "terraform",
        ".lua": "lua",
        ".perl": "perl",
        ".pl": "perl",
        ".r": "r",
        ".ex": "elixir",
        ".exs": "elixir",
        ".dart": "dart",
        ".groovy": "groovy",
        ".gradle": "groovy",  # Gradle files often use groovy
        ".vb": "vbnet",
        ".fs": "fsharp",
        ".fsi": "fsharp",
        ".fsx": "fsharp",
        ".fsscript": "fsharp",
    }
    return mapping.get(ext, "")  # Return the mapped language or empty string
```

---
### File: src/vibelint/utils.py

```python
"""
Utility functions for vibelint.

vibelint/src/vibelint/utils.py
"""

import logging
import os
import shutil
from pathlib import Path

from rich.console import Console

logger = logging.getLogger(__name__)

__all__ = [
    "ensure_directory",
    "find_files_by_extension",
    "find_package_root",
    "get_import_path",
    "get_module_name",
    "get_relative_path",
    "is_python_file",
    "read_file_safe",
    "write_file_safe",
    "find_project_root",
    "is_binary",
    "console",
    "scale_ascii_art_by_height",
    "scale_to_terminal_by_height",
]


def find_project_root(start_path: Path) -> Path | None:
    """
    Find the root directory of a project containing the given path.

    A project root is identified by containing either:
    1. A pyproject.toml file
    2. A .git directory

    Args:
    start_path: Path to start the search from

    Returns:
    Path to project root, or None if not found

    vibelint/src/vibelint/utils.py
    """

    current_path = start_path.resolve()
    while True:
        if (current_path / "pyproject.toml").is_file():
            return current_path
        if (current_path / ".git").is_dir():
            return current_path
        if current_path.parent == current_path:
            return None
        current_path = current_path.parent


def find_package_root(start_path: Path) -> Path | None:
    """
    Find the root directory of a Python package containing the given path.

    A package root is identified by containing either:
    1. A pyproject.toml file
    2. A  file
    3. An  file at the top level with no parent

    Args:
    start_path: Path to start the search from

    Returns:
    Path to package root, or None if not found

    vibelint/src/vibelint/utils.py
    """

    current_path = start_path.resolve()
    if current_path.is_file():
        current_path = current_path.parent

    while True:
        if (current_path / "__init__.py").is_file():
            project_root_marker = find_project_root(current_path)
            if project_root_marker and current_path.is_relative_to(project_root_marker):
                pass

        if (current_path / "pyproject.toml").is_file() or (current_path / ".git").is_dir():
            src_dir = current_path / "src"
            if src_dir.is_dir():
                if start_path.resolve().is_relative_to(src_dir):
                    for item in src_dir.iterdir():
                        if item.is_dir() and (item / "__init__.py").is_file():
                            return item
                    return src_dir
                else:
                    if (current_path / "__init__.py").is_file():
                        return current_path

            if (current_path / "__init__.py").is_file():
                return current_path
            return current_path

        if current_path.parent == current_path:
            return start_path.parent if start_path.is_file() else start_path

        current_path = current_path.parent


def is_python_file(path: Path) -> bool:
    """
    Check if a path represents a Python file.

    Args:
    path: Path to check

    Returns:
    True if the path is a Python file, False otherwise

    vibelint/src/vibelint/utils.py
    """

    return path.is_file() and path.suffix == ".py"


def get_relative_path(path: Path, base: Path) -> Path:
    """
    Safely compute a relative path, falling back to the original path.

    vibelint/src/vibelint/utils.py
    """

    try:

        return path.resolve().relative_to(base.resolve())
    except ValueError as e:
        logger.debug(f"Path {path} is not relative to {base}: {e}")
        return path.resolve()


def get_import_path(file_path: Path, package_root: Path | None = None) -> str:
    """
    Get the import path for a Python file.

    Args:
    file_path: Path to the Python file
    package_root: Optional path to the package root

    Returns:
    Import path (e.g., "vibelint.utils")

    vibelint/src/vibelint/utils.py
    """

    if package_root is None:
        package_root = find_package_root(file_path)

    if package_root is None:
        return file_path.stem

    try:
        rel_path = file_path.relative_to(package_root)
        import_path = str(rel_path).replace(os.sep, ".").replace("/", ".")
        if import_path.endswith(".py"):
            import_path = import_path[:-3]
        return import_path
    except ValueError as e:
        logger.debug(f"Could not determine import path for {file_path}: {e}")
        return file_path.stem


def get_module_name(file_path: Path) -> str:
    """
    Extract module name from a Python file path.

    Args:
    file_path: Path to a Python file

    Returns:
    Module name

    vibelint/src/vibelint/utils.py
    """

    return file_path.stem


def find_files_by_extension(
    root_path: Path,
    extension: str = ".py",
    exclude_globs: list[str] = [],
    include_vcs_hooks: bool = False,
) -> list[Path]:
    """
    Find all files with a specific extension in a directory and its subdirectories.

    Args:
    root_path: Root path to search in
    extension: File extension to look for (including the dot)
    exclude_globs: Glob patterns to exclude
    include_vcs_hooks: Whether to include version control directories

    Returns:
    List of paths to files with the specified extension

    vibelint/src/vibelint/utils.py
    """

    import fnmatch

    if exclude_globs is None:
        exclude_globs = []

    result = []

    for file_path in root_path.glob(f"**/*{extension}"):
        if not include_vcs_hooks:
            if any(
                part.startswith(".") and part in {".git", ".hg", ".svn"} for part in file_path.parts
            ):
                continue

        if any(fnmatch.fnmatch(str(file_path), pattern) for pattern in exclude_globs):
            continue

        result.append(file_path)

    return result


def ensure_directory(path: Path) -> Path:
    """
    Ensure a directory exists, creating it if necessary.

    Args:
    path: Path to directory

    Returns:
    Path to the directory

    vibelint/src/vibelint/utils.py
    """

    path.mkdir(parents=True, exist_ok=True)
    return path


def read_file_safe(file_path: Path, encoding: str = "utf-8") -> str | None:
    """
    Safely read a file, returning None if any errors occur.

    Args:
    file_path: Path to file
    encoding: File encoding

    Returns:
    File contents or None if error

    vibelint/src/vibelint/utils.py
    """

    try:
        return file_path.read_text(encoding=encoding)
    except (OSError, UnicodeDecodeError) as e:
        logger.debug(f"Could not read file {file_path}: {e}")
        return None


def write_file_safe(file_path: Path, content: str, encoding: str = "utf-8") -> bool:
    """
    Safely write content to a file, returning success status.

    Args:
    file_path: Path to file
    content: Content to write
    encoding: File encoding

    Returns:
    True if successful, False otherwise

    vibelint/src/vibelint/utils.py
    """

    try:
        file_path.parent.mkdir(parents=True, exist_ok=True)
        file_path.write_text(content, encoding=encoding)
        return True
    except (OSError, UnicodeEncodeError) as e:
        logger.debug(f"Could not write file {file_path}: {e}")
        return False


def is_binary(file_path: Path, chunk_size: int = 1024) -> bool:
    """
    Check if a file appears to be binary by looking for null bytes
    or a high proportion of non-text bytes in the first chunk.

    Args:
    file_path: The path to the file.
    chunk_size: The number of bytes to read from the beginning.

    Returns:
    True if the file seems binary, False otherwise.

    vibelint/src/vibelint/utils.py
    """

    try:
        with open(file_path, "rb") as f:
            chunk = f.read(chunk_size)
        if not chunk:
            return False

        if b"\x00" in chunk:
            return True

        text_characters = bytes(range(32, 127)) + b"\n\r\t\f\b"
        non_text_count = sum(1 for byte in chunk if bytes([byte]) not in text_characters)

        if len(chunk) > 0 and (non_text_count / len(chunk)) > 0.3:
            return True

        return False
    except OSError:

        return True
    except (TypeError, AttributeError) as e:
        logger.debug(f"Error checking if {file_path} is binary: {e}")
        return True


# === Console Utilities ===

# Global console instance used throughout vibelint
console = Console()


# === ASCII Art Utilities ===


def _get_terminal_size():
    """
    Returns the terminal size as a tuple (width, height) of characters.
    Falls back to (80, 24) if the dimensions cannot be determined.
    """
    try:
        size = shutil.get_terminal_size(fallback=(80, 24))
        return size.columns, size.lines
    except OSError as e:
        # Terminal size unavailable in non-interactive environments
        logging.debug("Failed to get terminal size: %s", e)
        return 80, 24


def scale_ascii_art_by_height(ascii_art: str, target_height: int) -> str:
    """
    Scales the ASCII art to have a specified target height (in characters)
    while preserving the original aspect ratio. The target width is
    automatically computed based on the scaling factor.
    """
    # Split into lines and remove any fully blank lines.
    lines = [line for line in ascii_art.splitlines() if line.strip()]
    if not lines:
        return ""

    orig_height = len(lines)
    orig_width = max(len(line) for line in lines)

    # Pad all lines to the same length (for a rectangular grid)
    normalized_lines = [line.ljust(orig_width) for line in lines]

    # Compute the vertical scale factor and derive the target width.
    scale_factor = target_height / orig_height
    target_width = max(1, int(orig_width * scale_factor))

    # Calculate step sizes for sampling
    row_step = orig_height / target_height
    col_step = orig_width / target_width if target_width > 0 else 1

    result_lines = []
    for r in range(target_height):
        orig_r = min(int(r * row_step), orig_height - 1)
        new_line = []
        for c in range(target_width):
            orig_c = min(int(c * col_step), orig_width - 1)
            new_line.append(normalized_lines[orig_r][orig_c])
        result_lines.append("".join(new_line))

    return "\n".join(result_lines)


def scale_to_terminal_by_height(ascii_art: str) -> str:
    """
    Scales the provided ASCII art to fit based on the terminal's available height.
    The width is computed automatically to maintain the art's original aspect ratio.
    """
    _, term_height = _get_terminal_size()
    # Optionally, leave a margin (here, using 90% of available height)
    target_height = max(1, int(term_height * 0.9))
    return scale_ascii_art_by_height(ascii_art, target_height)
```

---
### File: src/vibelint/validation_engine.py

```python
"""
Plugin-aware validation runner for vibelint.

This module provides the PluginValidationRunner that uses the new plugin system
to run validators and format output according to user configuration.

vibelint/src/vibelint/plugin_runner.py
"""

import logging
from collections import defaultdict
from pathlib import Path
from typing import Any, Dict, List

from vibelint.discovery import discover_files, discover_files_from_paths
from vibelint.plugin_system import Finding, Severity, plugin_manager
from vibelint.reporting import BUILTIN_FORMATTERS
from vibelint.rules import RuleEngine

# Note: No longer importing BUILTIN_VALIDATORS - using plugin discovery instead

__all__ = ["PluginValidationRunner", "run_plugin_validation"]


class PluginValidationRunner:
    """Runs validation using the plugin system."""

    def __init__(self, config_dict: Dict[str, Any], project_root: Path):
        """Initialize the plugin validation runner."""
        self.project_root = project_root
        self.config_dict = config_dict
        self.config = config_dict  # Add config property for formatters
        self.rule_engine = RuleEngine(config_dict)
        self.findings: List[Finding] = []

        # Register built-in validators with plugin manager
        self._register_builtin_validators()

    def _register_builtin_validators(self):
        """Register built-in validators with the plugin manager via entry point discovery."""
        # Built-in validators are now discovered automatically via entry points
        plugin_manager.load_plugins()

    def run_validation(self, file_paths: List[Path]) -> List[Finding]:
        """Run validation on the specified files."""
        self.findings = []

        # Get enabled validators
        validators = self.rule_engine.get_enabled_validators()

        # Create extended config with analysis context
        analysis_config = dict(self.config)
        analysis_config["_analysis_files"] = file_paths  # Pass the actual files being analyzed

        for file_path in file_paths:
            if not file_path.exists() or not file_path.is_file():
                continue

            if file_path.suffix != ".py":
                continue

            try:
                content = file_path.read_text(encoding="utf-8")
            except (UnicodeDecodeError, OSError) as e:
                logging.getLogger(__name__).debug(f"Could not read file {file_path}: {e}")
                continue

            # Run all validators on this file
            for validator in validators:
                try:
                    for finding in validator.validate(file_path, content, analysis_config):
                        # Make path relative to project root
                        relative_path = file_path.relative_to(self.project_root)
                        finding.file_path = relative_path
                        self.findings.append(finding)
                except (ImportError, AttributeError, ValueError, SyntaxError) as e:
                    logging.getLogger(__name__).debug(
                        f"Validator {validator.__class__.__name__} failed on {file_path}: {e}"
                    )
                    continue

        return self.findings

    def get_summary(self) -> Dict[str, int]:
        """Get summary counts by severity level."""
        summary = defaultdict(int)
        for finding in self.findings:
            summary[finding.severity.value] += 1
        return dict(summary)

    def format_output(self, output_format: str = "human") -> str:
        """Format the validation results."""
        # Get formatter
        if output_format in BUILTIN_FORMATTERS:
            formatter_class = BUILTIN_FORMATTERS[output_format]
            formatter = formatter_class()
        else:
            # Try plugin formatters
            formatter_class = plugin_manager.get_formatter(output_format)
            if formatter_class:
                formatter = formatter_class()
            else:
                # Fallback to human format
                formatter = BUILTIN_FORMATTERS["human"]()

        summary = self.get_summary()
        return formatter.format_results(self.findings, summary, self.config)

    def has_blocking_issues(self) -> bool:
        """Check if any findings are blocking (BLOCK severity)."""
        return any(finding.severity == Severity.BLOCK for finding in self.findings)

    def get_exit_code(self) -> int:
        """Get appropriate exit code based on findings."""
        if self.has_blocking_issues():
            return 1
        return 0


def run_plugin_validation(
    config_dict: Dict[str, Any],
    project_root: Path,
    include_globs_override: List[Path] | None = None,
) -> PluginValidationRunner:
    """
    Run validation using the plugin system.

    Args:
        config_dict: Configuration dictionary from pyproject.toml
        project_root: Project root path
        include_globs_override: Optional list of paths to override include_globs.
                               If provided, only these paths are analyzed instead of
                               using the configured include_globs patterns.

    Returns:
        PluginValidationRunner with results
    """
    from vibelint.config import Config

    runner = PluginValidationRunner(config_dict, project_root)

    # Create a fake config object for discovery
    fake_config = Config(project_root, config_dict)

    # Choose discovery method based on whether include_globs are overridden
    if include_globs_override:
        # Use custom path discovery (include_globs override)
        files = discover_files_from_paths(
            custom_paths=include_globs_override, config=fake_config, explicit_exclude_paths=set()
        )
    else:
        # Use original discovery method with configured include_globs
        files = discover_files(
            paths=[project_root], config=fake_config, explicit_exclude_paths=set()
        )

    # Run validation
    runner.run_validation(files)

    return runner
```

---
### File: src/vibelint/validators/__init__.py

```python
"""
vibelint validators sub-package.

Modular validator system with centralized registry and discovery.

Responsibility: Validator module organization and re-exports only.
Individual validation logic belongs in specific validator modules.

vibelint/src/vibelint/validators/__init__.py
"""

# Import validator categories for direct access
from . import architecture, project_wide, single_file
# Import registry system
from .registry import (get_all_validators, get_validator, register_validator,
                       validator_registry)
# Re-export specific validators for backward compatibility
from .single_file.absolute_imports import AbsoluteImportValidator
from .single_file.docstring import (DocstringPathValidator,
                                    MissingDocstringValidator)
from .single_file.emoji import EmojiUsageValidator
from .single_file.exports import InitAllValidator, MissingAllValidator
from .single_file.line_count import LineCountValidator
from .single_file.logger_names import LoggerNameValidator
from .single_file.print_statements import PrintStatementValidator
from .single_file.self_validation import VibelintSelfValidator

try:
    from .architecture.basic_patterns import ArchitectureValidator
    from .project_wide.dead_code import DeadCodeValidator
except ImportError:
    # These might not exist yet or have import issues
    pass

__all__ = [
    # Registry system
    "validator_registry",
    "register_validator",
    "get_validator",
    "get_all_validators",
    # Category modules
    "single_file",
    "project_wide",
    "architecture",
    # Individual validators (backward compatibility)
    "AbsoluteImportValidator",
    "MissingDocstringValidator",
    "DocstringPathValidator",
    "EmojiUsageValidator",
    "MissingAllValidator",
    "InitAllValidator",
    "LoggerNameValidator",
    "PrintStatementValidator",
    "VibelintSelfValidator",
    "LineCountValidator",
]
```

---
### File: src/vibelint/validators/architecture/__init__.py

```python
"""
Architecture analysis module for vibelint.

This module consolidates all architecture-related validators:
- Basic architectural patterns (consistency, naming)
- LLM-powered semantic analysis
- Embedding-based similarity detection
- Fallback pattern analysis for silent failures

vibelint/src/vibelint/validators/architecture/__init__.py
"""

from .basic_patterns import ArchitectureValidator
from .fallback_patterns import FallbackAnalyzer
from .llm_analysis import LLMAnalysisValidator
from .semantic_similarity import SemanticSimilarityValidator

__all__ = [
    "ArchitectureValidator",
    "LLMAnalysisValidator",
    "SemanticSimilarityValidator",
    "FallbackAnalyzer",
]
```

---
### File: src/vibelint/validators/architecture/basic_patterns.py

```python
"""
Architecture consistency validator.

Detects architectural inconsistencies like competing systems,
mixed patterns, and violations of established conventions.

vibelint/validators/architecture.py
"""

import logging
from pathlib import Path
from typing import Dict, Iterator, List

from ...plugin_system import BaseValidator, Finding, Severity

logger = logging.getLogger(__name__)

__all__ = ["ArchitectureValidator"]


class ArchitectureValidator(BaseValidator):
    """Detects architectural inconsistencies and competing systems."""

    rule_id = "ARCHITECTURE-INCONSISTENT"
    name = "Architecture Consistency Checker"
    description = "Identifies competing systems, mixed patterns, and architectural violations"
    default_severity = Severity.WARN

    def validate(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        """Analyze file for architectural inconsistencies."""
        yield from self._check_competing_systems(file_path, content)
        yield from self._check_mixed_patterns(file_path, content)
        yield from self._check_import_inconsistencies(file_path, content)

    def _check_competing_systems(self, file_path: Path, content: str) -> Iterator[Finding]:
        """Check for architectural consistency - simplified since no legacy patterns exist."""
        # Since vibelint hasn't been released and we removed all backward compatibility,
        # there are no legacy patterns to detect anywhere. This method intentionally simplified.
        return
        yield  # Unreachable - just for type checking

    def _check_mixed_patterns(self, file_path: Path, content: str) -> Iterator[Finding]:
        """Check for mixed architectural patterns within same file."""
        lines = content.splitlines()

        # Track different console usage patterns
        console_patterns = []
        for line_num, line in enumerate(lines, 1):
            if "console" in line.lower():
                if "= Console()" in line:
                    console_patterns.append(("manual_instantiation", line_num))
                elif "from .utils import console" in line:
                    console_patterns.append(("shared_import", line_num))
                elif "from rich.console import Console" in line:
                    console_patterns.append(("rich_import", line_num))

        # If file uses multiple console patterns, flag it (except utils.py and validators that detect patterns)
        pattern_types = set(pattern[0] for pattern in console_patterns)
        if len(pattern_types) > 1 and file_path.name not in [
            "utils.py",
            "architecture.py",
            "dead_code.py",
            "basic_patterns.py",  # This file itself mentions multiple console patterns for detection
        ]:
            yield self.create_finding(
                message="File uses multiple console patterns inconsistently",
                file_path=file_path,
                line=console_patterns[0][1],
                suggestion="Use consistent console pattern throughout file",
            )

        # Check for mixed error handling patterns
        error_patterns = []
        for line_num, line in enumerate(lines, 1):
            if "raise " in line or "except " in line:
                if "ValidationError" in line:
                    error_patterns.append(("custom_exception", line_num))
                elif "ValueError" in line or "TypeError" in line:
                    error_patterns.append(("builtin_exception", line_num))

        # Detect inconsistent error handling
        if len(set(pattern[0] for pattern in error_patterns)) > 1:
            yield self.create_finding(
                message="Inconsistent exception types used in error handling",
                file_path=file_path,
                line=error_patterns[0][1],
                suggestion="Use consistent exception handling patterns",
            )

    def _check_import_inconsistencies(self, file_path: Path, content: str) -> Iterator[Finding]:
        """Check for inconsistent import patterns."""
        lines = content.splitlines()

        # Track different import styles for same module
        import_styles = {}  # module -> list of (style, line_num)

        for line_num, line in enumerate(lines, 1):
            line = line.strip()
            if line.startswith("from ") and " import " in line:
                # Parse "from X import Y" style
                parts = line.split(" import ", 1)
                if len(parts) == 2:
                    module = parts[0].replace("from ", "")
                    if module not in import_styles:
                        import_styles[module] = []
                    import_styles[module].append(("from_import", line_num, line))
            elif line.startswith("import "):
                # Parse "import X" style
                module = line.replace("import ", "").split(".")[0]
                if module not in import_styles:
                    import_styles[module] = []
                import_styles[module].append(("direct_import", line_num, line))

        # Flag modules imported in multiple ways
        for module, imports in import_styles.items():
            if len(set(imp[0] for imp in imports)) > 1:
                first_import = imports[0]
                yield self.create_finding(
                    message=f"Module '{module}' imported using different styles",
                    file_path=file_path,
                    line=first_import[1],
                    suggestion=f"Use consistent import style for {module}",
                )


class ProjectArchitectureAnalyzer:
    """
    Cross-file architecture analysis.

    This can be used by vibelint to analyze architectural patterns
    across the entire project, not just individual files.
    """

    def __init__(self, project_files: List[Path]):
        self.project_files = project_files
        self.findings: List[Dict] = []

    def analyze_competing_systems(self) -> List[Dict]:
        """Analyze project for competing validation systems."""
        legacy_files = []
        plugin_files = []

        for file_path in self.project_files:
            if not file_path.suffix == ".py":
                continue

            try:
                content = file_path.read_text(encoding="utf-8")
            except (OSError, UnicodeDecodeError) as e:
                logger.debug(f"Could not read file {file_path}: {e}")
                continue

            # Check validation patterns
            has_legacy = any(
                [
                    "ValidationResult" in content,
                    "def validate_" in content and "def validate(" not in content,
                ]
            )

            has_plugin = any(["BaseValidator" in content, "self.create_finding" in content])

            if has_legacy:
                legacy_files.append(file_path)
            if has_plugin:
                plugin_files.append(file_path)

        findings = []
        if legacy_files and plugin_files:
            findings.append(
                {
                    "issue": "Competing validation systems detected",
                    "legacy_files": [str(f) for f in legacy_files],
                    "plugin_files": [str(f) for f in plugin_files],
                    "suggestion": "Migrate all validators to plugin system for consistency",
                }
            )

        return findings

    def analyze_duplicate_functionality(self) -> List[Dict]:
        """Find duplicate functionality across files."""
        # This could analyze similar function names, similar AST patterns, etc.
        # For now, just check for validation duplicates
        validation_files = {}  # functionality -> list of files

        for file_path in self.project_files:
            if not file_path.suffix == ".py" or "validator" not in str(file_path):
                continue

            try:
                content = file_path.read_text(encoding="utf-8")
            except (OSError, UnicodeDecodeError) as e:
                logger.debug(f"Could not read file {file_path}: {e}")
                continue

            # Simple heuristic: check for similar validation functionality
            if "print" in content.lower() and "statement" in content.lower():
                if "print_validation" not in validation_files:
                    validation_files["print_validation"] = []
                validation_files["print_validation"].append(file_path)

            if "docstring" in content.lower():
                if "docstring_validation" not in validation_files:
                    validation_files["docstring_validation"] = []
                validation_files["docstring_validation"].append(file_path)

        findings = []
        for functionality, files in validation_files.items():
            if len(files) > 1:
                findings.append(
                    {
                        "issue": f"Duplicate {functionality} functionality",
                        "files": [str(f) for f in files],
                        "suggestion": f"Consolidate {functionality} into single implementation",
                    }
                )

        return findings
```

---
### File: src/vibelint/validators/architecture/fallback_patterns.py

```python
"""
Fallback pattern analyzer for detecting silent failure patterns.

Identifies problematic fallback patterns that mask errors and cause silent failures:
1. Bare except blocks that return default values
2. Function calls with broad exception catching that return None/empty
3. Dictionary.get() chains that hide missing configuration
4. Try/except blocks that swallow important errors
5. Default parameter fallbacks that mask real issues

vibelint/validators/fallback_analyzer.py
"""

import ast
import logging
from pathlib import Path
from typing import Any, Dict, Iterator, List, Optional

from ...plugin_system import BaseValidator, Finding, Severity

logger = logging.getLogger(__name__)

__all__ = ["FallbackAnalyzer"]


class FallbackAnalyzer(BaseValidator, ast.NodeVisitor):
    """
    Detects problematic fallback patterns that cause silent failures.
    """

    rule_id = "FALLBACK-SILENT-FAILURE"
    default_severity = Severity.WARN

    def __init__(
        self, severity: Optional[Severity] = None, config: Optional[Dict[str, Any]] = None
    ) -> None:
        super().__init__(severity, config)
        ast.NodeVisitor.__init__(self)
        self.findings: List[Finding] = []
        self.current_file: Optional[Path] = None

    def validate(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        """Analyze file for problematic fallback patterns."""
        self.findings = []
        self.current_file = file_path

        try:
            tree = ast.parse(content)
            self.visit(tree)
        except SyntaxError as e:
            logger.debug(f"Syntax error in {file_path}: {e}")
            pass

        return iter(self.findings)

    def visit_Try(self, node: ast.Try) -> None:
        """Analyze try/except blocks for problematic fallback patterns."""

        # Pattern 1: Bare except that returns a default value
        for handler in node.handlers:
            if handler.type is None:  # bare except:
                if self._handler_returns_default(handler):
                    assert self.current_file is not None
                    self.findings.append(
                        self.create_finding(
                            message="Bare except block returns default value, potentially masking important errors. Consider catching specific exceptions or at least logging the error.",
                            file_path=self.current_file,
                            line=node.lineno,
                        )
                    )

            # Pattern 2: Exception handler that swallows exceptions silently
            elif self._handler_is_silent(handler):
                exception_type = "Exception" if handler.type is None else ast.unparse(handler.type)
                assert self.current_file is not None
                self.findings.append(
                    self.create_finding(
                        message=f"Exception handler for {exception_type} swallows errors silently. Consider logging the error or re-raising if appropriate.",
                        file_path=self.current_file,
                        line=node.lineno,
                    )
                )

            # Pattern 3: Too broad exception catching
            elif self._handler_too_broad(handler):
                assert self.current_file is not None
                self.findings.append(
                    self.create_finding(
                        message="Catching 'Exception' or 'BaseException' is too broad and may hide programming errors. Catch specific exception types instead.",
                        file_path=self.current_file,
                        line=node.lineno,
                    )
                )

        self.generic_visit(node)

    def visit_Call(self, node: ast.Call) -> None:
        """Analyze function calls for problematic patterns."""

        # Pattern 4: dict.get() chains that might hide config issues
        if self._is_chained_dict_get(node):
            assert self.current_file is not None
            self.findings.append(
                self.create_finding(
                    message="Chained dict.get() calls with defaults may hide missing configuration. Consider validating required configuration explicitly.",
                    file_path=self.current_file,
                    line=node.lineno,
                )
            )

        # Pattern 5: getattr with default that might hide attribute errors
        if self._is_problematic_getattr(node):
            assert self.current_file is not None
            self.findings.append(
                self.create_finding(
                    message="getattr() with default value may hide missing attributes. Consider checking if attribute should exist before accessing.",
                    file_path=self.current_file,
                    line=node.lineno,
                )
            )

        self.generic_visit(node)

    def visit_FunctionDef(self, node: ast.FunctionDef) -> None:
        """Analyze function definitions for problematic default patterns."""

        # Pattern 6: Functions that return None by default in multiple paths
        return_statements = self._get_return_statements(node)
        none_returns = [r for r in return_statements if self._returns_none_or_empty(r)]

        if len(none_returns) >= 2 and len(return_statements) >= 3:
            assert self.current_file is not None
            self.findings.append(
                self.create_finding(
                    message=f"Function '{node.name}' has multiple return paths that return None/empty values. This may indicate error conditions being masked as normal returns.",
                    file_path=self.current_file,
                    line=node.lineno,
                )
            )

        self.generic_visit(node)

    def _handler_returns_default(self, handler: ast.ExceptHandler) -> bool:
        """Check if exception handler returns a default value."""
        if not handler.body:
            return False

        # Look for return statements with default values
        for stmt in handler.body:
            if isinstance(stmt, ast.Return):
                if stmt.value is None:  # return None
                    return True
                elif isinstance(stmt.value, (ast.Constant, ast.List, ast.Dict, ast.Set)):
                    return True  # return literal value
        return False

    def _handler_is_silent(self, handler: ast.ExceptHandler) -> bool:
        """Check if exception handler swallows exceptions without logging."""
        if not handler.body:
            return True  # Empty handler

        # Check if handler only has pass, return, or continue
        for stmt in handler.body:
            if isinstance(stmt, (ast.Pass, ast.Return, ast.Continue, ast.Break)):
                continue
            elif isinstance(stmt, ast.Expr) and isinstance(stmt.value, ast.Constant):
                continue  # String literal (probably a comment)
            else:
                return False  # Has some other statement (probably logging)

        return True  # Only has pass/return/continue statements

    def _handler_too_broad(self, handler: ast.ExceptHandler) -> bool:
        """Check if exception handler catches too broadly."""
        if handler.type is None:
            return True  # bare except

        if isinstance(handler.type, ast.Name):
            return handler.type.id in ["Exception", "BaseException"]

        return False

    def _is_chained_dict_get(self, node: ast.Call) -> bool:
        """Check for chained dict.get() calls like config.get('a', {}).get('b', default)."""
        if isinstance(node.func, ast.Attribute) and node.func.attr == "get" and len(node.args) >= 2:

            # Check if this is called on another .get() call
            if isinstance(node.func.value, ast.Call):
                inner_call = node.func.value
                if isinstance(inner_call.func, ast.Attribute) and inner_call.func.attr == "get":
                    return True

        return False

    def _is_problematic_getattr(self, node: ast.Call) -> bool:
        """Check for getattr calls with defaults that might hide real issues."""
        if (
            isinstance(node.func, ast.Name) and node.func.id == "getattr" and len(node.args) >= 3
        ):  # getattr(obj, name, default)

            # If the default is None or a simple literal, it might be problematic
            default_arg = node.args[2]
            return isinstance(default_arg, (ast.Constant, ast.NameConstant)) or (
                isinstance(default_arg, ast.Name) and default_arg.id == "None"
            )

        return False

    def _get_return_statements(self, function_node: ast.FunctionDef) -> List[ast.Return]:
        """Get all return statements in a function."""
        returns = []

        class ReturnVisitor(ast.NodeVisitor):
            """A visitor that returns a value from visiting nodes in a syntax tree."""

            def visit_Return(self, node: ast.Return):
                """Visit a Return node in the AST and process it."""
                returns.append(node)
                self.generic_visit(node)

        ReturnVisitor().visit(function_node)
        return returns

    def _returns_none_or_empty(self, return_node: ast.Return) -> bool:
        """Check if return statement returns None or empty container."""
        if return_node.value is None:
            return True

        if isinstance(return_node.value, ast.Constant):
            return return_node.value.value is None

        if isinstance(return_node.value, ast.Name):
            return return_node.value.id == "None"

        if isinstance(return_node.value, (ast.List, ast.Set, ast.Tuple)):
            return len(return_node.value.elts) == 0
        elif isinstance(return_node.value, ast.Dict):
            return len(return_node.value.keys) == 0 and len(return_node.value.values) == 0

        return False
```

---
### File: src/vibelint/validators/architecture/intelligent_llm_analysis.py

```python
"""
Intelligent LLM-powered architectural analysis validator for vibelint.

Uses a multi-phase approach:
1. Global project structure analysis to identify potentially problematic areas
2. Pairwise file comparison for identified files
3. Structured JSON output with specific architectural issues

vibelint/validators/architecture/intelligent_llm_analysis.py
"""

import json
import logging
from pathlib import Path
from typing import Any, Dict, Iterator, List, Optional

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from ...plugin_system import BaseValidator, Finding, Severity

__all__ = ["IntelligentLLMValidator"]
logger = logging.getLogger(__name__)


class IntelligentLLMValidator(BaseValidator):
    """
    Intelligent LLM-powered architectural analysis using multi-phase approach.

    Phase 1: Analyzes entire project structure to identify potentially problematic files
    Phase 2: Performs pairwise comparison of flagged files
    Phase 3: Generates specific architectural findings
    """

    rule_id = "ARCHITECTURE-INTELLIGENT-LLM"
    default_severity = Severity.INFO

    def __init__(
        self, severity: Optional[Severity] = None, config: Optional[Dict[str, Any]] = None
    ) -> None:
        super().__init__(severity, config)
        self._api_base_url: Optional[str] = None
        self._model_name: Optional[str] = None
        self._session: Optional[requests.Session] = None
        self._project_files: List[Path] = []
        self._analysis_cache: Dict[str, Any] = {}

    def _setup_llm_client(self, config: Dict[str, Any]) -> bool:
        """Initialize LLM client with configuration."""
        llm_config = config.get("llm_analysis", {})

        if not isinstance(llm_config, dict):
            logger.debug("Intelligent LLM analysis disabled: no llm_analysis config section found")
            return False

        self._api_base_url = llm_config.get("api_base_url")
        self._model_name = llm_config.get("model")
        self._api_key = llm_config.get("api_key")
        self._max_tokens = llm_config.get("max_tokens", 4096)
        self._temperature = llm_config.get("temperature", 0.3)

        if not self._api_base_url or not self._model_name:
            logger.debug("Intelligent LLM analysis disabled: missing api_base_url or model")
            return False

        # Setup requests session
        self._session = requests.Session()
        retry_strategy = Retry(
            total=2, status_forcelist=[429, 500, 502, 503, 504], backoff_factor=1
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self._session.mount("http://", adapter)
        self._session.mount("https://", adapter)

        if self._api_key:
            self._session.headers.update({"Authorization": f"Bearer {self._api_key}"})

        # Test connectivity
        try:
            response = self._session.get(f"{self._api_base_url}/v1/models", timeout=5)
            if response.status_code == 200:
                logger.info(f"Intelligent LLM analysis enabled using API at {self._api_base_url}")
                return True
            else:
                logger.warning(f"LLM API test failed with status {response.status_code}")
                return False
        except Exception as e:
            logger.debug(f"LLM API connectivity test failed: {e}")
            return False

    def _query_llm(self, prompt: str) -> Optional[Dict[str, Any]]:
        """Query the LLM API with a prompt and return JSON response."""
        if not self._session or not self._api_base_url:
            return None

        try:
            payload = {
                "model": self._model_name,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": self._temperature,
                "max_tokens": self._max_tokens,
                "response_format": {"type": "json_object"},
            }

            response = self._session.post(
                f"{self._api_base_url}/v1/chat/completions",
                json=payload,
                timeout=120,
            )

            if response.status_code == 200:
                result = response.json()
                response_text = result.get("choices", [{}])[0].get("message", {}).get("content", "")

                # Handle different response formats
                if "<|message|>" in response_text:
                    if "<|channel|>final<|message|>" in response_text:
                        final_start = response_text.find("<|channel|>final<|message|>") + len(
                            "<|channel|>final<|message|>"
                        )
                        content = response_text[final_start:].split("<|end|>")[0].strip()
                    else:
                        message_start = response_text.find("<|message|>") + len("<|message|>")
                        content = response_text[message_start:].split("<|end|>")[0].strip()
                else:
                    content = response_text.strip()

                try:
                    return json.loads(content)
                except json.JSONDecodeError:
                    logger.debug(f"Failed to parse JSON from LLM response: {content[:200]}...")
                    return None
            else:
                logger.warning(f"LLM API request failed with status {response.status_code}")
                return None

        except Exception as e:
            logger.debug(f"LLM analysis request failed: {e}")
            return None

    def _collect_project_structure(self, project_root: Path, file_paths: List[Path]) -> str:
        """Create a project structure overview for LLM analysis."""
        structure_lines = ["# Project Structure Overview\n"]

        # Group files by directory
        dirs_files: Dict[str, List[Path]] = {}
        for file_path in file_paths:
            try:
                rel_path = file_path.relative_to(project_root)
                dir_path = str(rel_path.parent)
                if dir_path not in dirs_files:
                    dirs_files[dir_path] = []
                dirs_files[dir_path].append(rel_path)
            except ValueError:
                continue

        # Create tree representation
        for dir_path in sorted(dirs_files.keys()):
            structure_lines.append(f"\n## Directory: {dir_path}")
            files = sorted(dirs_files[dir_path])

            for file_path in files:
                # Add basic file info
                try:
                    full_path = project_root / file_path
                    if full_path.exists():
                        size = full_path.stat().st_size
                        lines = len(
                            full_path.read_text(encoding="utf-8", errors="ignore").splitlines()
                        )
                        structure_lines.append(f"- {file_path.name} ({lines} lines, {size} bytes)")
                except Exception:
                    structure_lines.append(f"- {file_path.name}")

        return "\n".join(structure_lines)

    def _phase1_global_analysis(
        self, project_root: Path, file_paths: List[Path]
    ) -> Optional[List[str]]:
        """Phase 1: Global project structure analysis to identify potentially problematic files."""
        logger.info("Phase 1: Running global project structure analysis")

        structure_overview = self._collect_project_structure(project_root, file_paths)

        prompt = f"""Analyze this Python project structure for potential architectural issues.

{structure_overview}

Look for patterns that indicate:
1. Code duplication across files
2. Overly similar file names suggesting redundant functionality
3. Thin wrapper files that might be unnecessary abstractions
4. Files that appear to be doing too many things (poor separation of concerns)
5. Missing abstractions where there should be shared code

Respond with a JSON object containing:
{{
    "potentially_problematic_files": [
        "path/to/file1.py",
        "path/to/file2.py"
    ],
    "reasoning": "Brief explanation of why these files were flagged",
    "comparison_pairs": [
        ["file1.py", "file2.py"],
        ["file3.py", "file4.py"]
    ]
}}

Focus on files that likely contain architectural issues, not every file in the project."""

        response = self._query_llm(prompt)
        if response and "potentially_problematic_files" in response:
            flagged_files = response["potentially_problematic_files"]
            logger.info(
                f"Phase 1 complete: Flagged {len(flagged_files)} potentially problematic files"
            )
            logger.debug(f"Reasoning: {response.get('reasoning', 'No reasoning provided')}")

            # Store comparison pairs for phase 2
            self._analysis_cache["comparison_pairs"] = response.get("comparison_pairs", [])
            return flagged_files

        logger.warning("Phase 1 failed: No valid response from LLM")
        return None

    def _phase2_pairwise_comparison(
        self, project_root: Path, comparison_pairs: List[List[str]]
    ) -> List[Dict[str, Any]]:
        """Phase 2: Pairwise comparison of flagged files."""
        logger.info(f"Phase 2: Running pairwise comparison of {len(comparison_pairs)} file pairs")

        issues = []

        for pair in comparison_pairs:
            if len(pair) != 2:
                continue

            file1_path = project_root / pair[0]
            file2_path = project_root / pair[1]

            if not (file1_path.exists() and file2_path.exists()):
                continue

            try:
                file1_content = file1_path.read_text(encoding="utf-8", errors="ignore")
                file2_content = file2_path.read_text(encoding="utf-8", errors="ignore")

                # Limit content size for LLM
                if len(file1_content) > 3000:
                    file1_content = file1_content[:3000] + "\n... [truncated]"
                if len(file2_content) > 3000:
                    file2_content = file2_content[:3000] + "\n... [truncated]"

                prompt = f"""Compare these two Python files for architectural redundancy and issues:

## File 1: {pair[0]}
```python
{file1_content}
```

## File 2: {pair[1]}
```python
{file2_content}
```

Analyze for:
1. Duplicate or very similar functionality
2. One file being a thin wrapper around the other
3. Opportunities for consolidation or better abstraction
4. Poor separation of concerns

Respond with JSON:
{{
    "has_issues": true/false,
    "issue_type": "duplication|thin_wrapper|poor_separation|other",
    "severity": "low|medium|high",
    "description": "Specific description of the architectural issue",
    "recommendation": "Specific recommendation for improvement"
}}"""

                response = self._query_llm(prompt)
                if response and response.get("has_issues"):
                    issues.append(
                        {
                            "files": pair,
                            "issue_type": response.get("issue_type", "unknown"),
                            "severity": response.get("severity", "medium"),
                            "description": response.get("description", ""),
                            "recommendation": response.get("recommendation", ""),
                        }
                    )

            except Exception as e:
                logger.debug(f"Failed to compare {pair[0]} and {pair[1]}: {e}")
                continue

        logger.info(f"Phase 2 complete: Found {len(issues)} architectural issues")
        return issues

    def validate(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        """
        Run intelligent multi-phase LLM architectural analysis.

        Only runs on the first file encountered, then analyzes the entire project.
        """
        # Skip if LLM is not available or already analyzed
        if not hasattr(self, "_llm_setup_attempted"):
            self._llm_setup_attempted = True
            self._llm_available = config is not None and self._setup_llm_client(config)

        if not self._llm_available:
            return

        # Only run analysis once per project (on first file)
        if hasattr(self, "_analysis_completed"):
            return

        self._analysis_completed = True

        # Collect all project files
        project_root = config.get("project_root", file_path.parent) if config else file_path.parent
        self._project_files = list(project_root.rglob("*.py"))

        logger.info(
            f"Starting intelligent LLM architectural analysis on {len(self._project_files)} files"
        )

        # Phase 1: Global analysis
        flagged_files = self._phase1_global_analysis(project_root, self._project_files)
        if not flagged_files:
            return

        # Phase 2: Pairwise comparison
        comparison_pairs = self._analysis_cache.get("comparison_pairs", [])
        if comparison_pairs:
            issues = self._phase2_pairwise_comparison(project_root, comparison_pairs)

            # Generate findings
            for issue in issues:
                files_str = " and ".join(issue["files"])
                message = f"Architectural issue in {files_str}: {issue['description']}"

                yield self.create_finding(
                    message=message,
                    file_path=file_path,  # Report on current file
                    line=1,
                )
```

---
### File: src/vibelint/validators/architecture/llm_analysis.py

```python
"""
LLM-powered architectural analysis validator using OpenAI-compatible APIs.

Provides intelligent architectural analysis using Large Language Models
to detect design issues, inconsistencies, and improvement opportunities.

vibelint/validators/architecture/llm_analysis.py
"""

import logging
import re
from pathlib import Path
from typing import Any, Dict, Iterator, List, Optional, Tuple

from pydantic import BaseModel, Field

from ...plugin_system import BaseValidator, Finding, Severity

logger = logging.getLogger(__name__)

__all__ = ["LLMAnalysisValidator"]


class ArchitecturalFinding(BaseModel):
    """Schema for architectural analysis findings."""

    file: str = Field(description="File path where issue was found")
    line: int = Field(default=1, description="Line number of the issue")
    severity: str = Field(description="Severity level: error, warning, info")
    category: str = Field(description="Category of architectural issue")
    message: str = Field(description="Description of the architectural issue")
    suggestion: str = Field(default="", description="Suggested improvement")


class ArchitecturalAnalysis(BaseModel):
    """Schema for complete architectural analysis response."""

    findings: List[ArchitecturalFinding] = Field(description="List of architectural issues found")
    summary: str = Field(description="Overall assessment of the codebase architecture")


class LLMAnalysisValidator(BaseValidator):
    """LLM-powered architectural analysis using OpenAI-compatible API for structured workflow."""

    rule_id = "ARCHITECTURE-LLM"
    name = "LLM Architectural Analysis"
    description = "AI-powered analysis of code architecture and design patterns"
    default_severity = Severity.INFO

    def __init__(
        self, severity: Optional[Severity] = None, config: Optional[Dict[str, Any]] = None
    ):
        super().__init__(severity, config)

        # Track analysis state
        self._llm_setup_attempted = False
        self._llm_available = False
        self._analysis_completed = False
        self._analyzed_files: set[Path] = set()  # Track which files we've seen
        self._global_analysis_done: bool = False  # Track if global analysis has run

        # Multi-phase analysis state
        self._file_summaries: dict[Path, str] = {}  # File path -> summary
        self._file_embeddings: dict[Path, list[float]] = {}  # File path -> embedding vector
        self._similarity_clusters: list[list[Path]] = []  # Groups of similar files
        self._pairwise_analyses: dict[tuple[Path, Path], str] = {}  # Pair -> analysis

        # Use the new dual LLM system exclusively
        from ...llm import create_llm_manager

        # Get full vibelint config (not just llm_analysis section)
        full_config = (
            getattr(self.config, "_config_data", self.config)
            if hasattr(self.config, "_config_data")
            else self.config
        )
        self.llm_manager = create_llm_manager(full_config)

        # Generation settings from config with sensible defaults
        llm_config = self.config.get("llm", {})  # Use llm section, not llm_analysis
        self.max_tokens = llm_config.get("fast_max_tokens", 4096)
        self.temperature = llm_config.get("fast_temperature", 0.1)

        # Advanced settings with defaults (most users won't need to change these)
        self.max_context_tokens = llm_config.get("max_context_tokens", 32768)
        self.max_prompt_tokens = llm_config.get("max_prompt_tokens", 28672)
        self.max_file_lines = llm_config.get("max_file_lines", 100)
        self.max_files = llm_config.get("max_files", 10)
        self.remove_thinking_tokens = llm_config.get("remove_thinking_tokens", True)
        self.thinking_format = llm_config.get("thinking_format", "harmony")
        self.custom_thinking_patterns = llm_config.get("custom_thinking_patterns", [])

        # Diagnostics for optimal LLM utilization
        self.enable_token_diagnostics = llm_config.get("enable_token_diagnostics", True)
        self.token_usage_stats = {
            "total_input_tokens": 0,
            "total_output_tokens": 0,
            "calls": 0,
            "context_efficiency": [],
        }

        # Dynamic context discovery
        self.enable_context_probing = llm_config.get("enable_context_probing", True)
        self.discovered_context_limit = None
        self.context_probe_cache = {}

        # Enable all compression strategies by default
        self.enable_import_summary = True
        self.enable_hierarchy_extraction = True
        self.enable_pattern_detection = True
        self.enable_complexity_analysis = True
        self.max_signature_lines = 20
        self.max_hierarchy_depth = 15

        # Architecture analysis will use the dual LLM system directly
        # No need for langchain setup - using our unified LLM manager

    def _get_similarity_prioritized_pairs(
        self, analysis_files: List[Path]
    ) -> List[Tuple[Path, Path, float]]:
        """
        Use semantic similarity to prioritize file pairs for LLM analysis.

        Returns list of (file1, file2, similarity_score) tuples sorted by similarity.
        Focuses expensive LLM analysis on highest redundancy candidates first.
        """
        try:
            # Import semantic similarity validator
            from .semantic_similarity import SemanticSimilarityValidator

            # Create similarity validator
            similarity_validator = SemanticSimilarityValidator()

            # Check if embedding model is available
            if not similarity_validator._initialize_model():
                logger.info("Embedding model not available, using file order priority")
                # Fallback: return files in original order with fake similarity scores
                pairs = []
                for i, file1 in enumerate(analysis_files):
                    for file2 in analysis_files[i + 1 :]:
                        pairs.append((file1, file2, 0.5))  # Neutral score
                return pairs

            # Compute embeddings for all files
            file_embeddings = {}
            for file_path in analysis_files:
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        content = f.read()

                    # Extract meaningful content for embedding
                    embedding_text = similarity_validator._extract_embedding_text(content)
                    if embedding_text.strip():
                        embedding = similarity_validator.model.encode([embedding_text])[0]
                        file_embeddings[file_path] = embedding

                except Exception as e:
                    logger.debug(f"Failed to process {file_path} for similarity: {e}")
                    continue

            # Compute similarity scores for all pairs
            pairs = []
            files_with_embeddings = list(file_embeddings.keys())

            for i, file1 in enumerate(files_with_embeddings):
                for file2 in files_with_embeddings[i + 1 :]:
                    similarity = similarity_validator._compute_similarity(
                        file_embeddings[file1], file_embeddings[file2]
                    )
                    pairs.append((file1, file2, similarity))

            # Sort by similarity (highest first) - focus on most redundant pairs
            pairs.sort(key=lambda x: x[2], reverse=True)

            logger.info(f"Prioritized {len(pairs)} file pairs by semantic similarity")
            if pairs:
                logger.info(
                    f"Highest similarity: {pairs[0][2]:.3f} between {pairs[0][0].name} and {pairs[0][1].name}"
                )

            return pairs

        except Exception as e:
            logger.debug(f"Failed to compute semantic similarity for prioritization: {e}")
            # Fallback to original file order
            pairs = []
            for i, file1 in enumerate(analysis_files):
                for file2 in analysis_files[i + 1 :]:
                    pairs.append((file1, file2, 0.5))  # Neutral score
            return pairs

    def validate(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        """
        Run LLM architectural analysis prioritized by semantic similarity.

        Uses semantic similarity to identify high-redundancy areas and focuses
        expensive LLM analysis on the most problematic file pairs first.
        """
        # Skip if LLM setup hasn't been attempted yet
        if not self._llm_setup_attempted:
            self._llm_setup_attempted = True
            self._llm_available = self._test_connectivity()

        if not self._llm_available:
            logger.debug("LLM service not available, skipping analysis")
            return

        # Skip if we've already analyzed this file
        if file_path in self._analyzed_files:
            return

        self._analyzed_files.add(file_path)

        # Skip if global analysis has already run
        if self._global_analysis_done:
            return

        self._global_analysis_done = True

        # Get the actual files being analyzed from the validation engine
        analysis_files = config.get("_analysis_files", [file_path]) if config else [file_path]

        # Step 1: Run semantic similarity analysis to prioritize files
        similarity_pairs = self._get_similarity_prioritized_pairs(analysis_files)

        # Step 2: Focus on high-similarity pairs first (> 0.8 similarity)
        high_similarity_pairs = [pair for pair in similarity_pairs if pair[2] > 0.8]
        priority_files = set()

        if high_similarity_pairs:
            logger.info(
                f"Found {len(high_similarity_pairs)} high-similarity pairs (>0.8) for priority analysis"
            )
            for file1, file2, score in high_similarity_pairs[:5]:  # Top 5 most similar pairs
                priority_files.update([file1, file2])
                logger.info(f"Priority pair: {file1.name} ↔ {file2.name} (similarity: {score:.3f})")

        # Estimate analysis time - prioritize expensive LLM analysis
        priority_count = len(priority_files)
        remaining_count = len(analysis_files) - priority_count
        estimated_minutes = max(
            1, (priority_count * 60 + remaining_count * 30) // 60
        )  # More time for priority pairs
        estimated_range = f"{max(1, estimated_minutes - 1)}-{estimated_minutes + 2}"

        logger.info(
            f"Starting similarity-guided LLM analysis: {priority_count} priority + {remaining_count} regular files"
        )
        logger.info(f"ESTIMATED TIME: {estimated_range} minutes (depends on LLM response speed)")
        logger.info("Strategy: High-similarity pairs analyzed first for redundancy detection")

        if estimated_minutes > 5:
            logger.warning(f"TIMEOUT RISK: Analysis may take {estimated_range} minutes")
            logger.warning(
                "If using AI coding tools or CI systems, consider analyzing smaller chunks:"
            )
            logger.warning("   vibelint check src/module1/ --rule ARCHITECTURE-LLM")
            logger.warning("   vibelint check src/module2/ --rule ARCHITECTURE-LLM")

        # Run architectural analysis prioritized by semantic similarity
        yield from self._analyze_similarity_guided_structure(
            file_path, analysis_files, similarity_pairs
        )

    def _analyze_similarity_guided_structure(
        self,
        primary_file: Path,
        analysis_files: List[Path],
        similarity_pairs: List[Tuple[Path, Path, float]],
    ) -> Iterator[Finding]:
        """
        Perform similarity-guided architectural analysis.

        Focuses expensive LLM analysis on high-similarity pairs first to catch
        redundancy and architectural inconsistencies most efficiently.
        """
        try:
            # Step 1: Analyze high-similarity pairs first (likely redundancy)
            high_sim_pairs = [pair for pair in similarity_pairs if pair[2] > 0.8]

            if high_sim_pairs:
                logger.info(
                    f"Phase 1: Analyzing {len(high_sim_pairs)} high-similarity pairs for redundancy"
                )

                for file1, file2, similarity in high_sim_pairs[:3]:  # Limit to top 3 for time
                    logger.info(
                        f"Analyzing similar pair: {file1.name} ↔ {file2.name} (similarity: {similarity:.3f})"
                    )

                    # Read both files
                    try:
                        with open(file1, "r", encoding="utf-8") as f:
                            content1 = f.read()
                        with open(file2, "r", encoding="utf-8") as f:
                            content2 = f.read()
                    except Exception as e:
                        logger.debug(f"Failed to read files for similarity analysis: {e}")
                        continue

                    # Create focused prompt for pair analysis
                    pair_analysis_prompt = f"""Analyze these two similar Python files for architectural redundancy and inconsistencies:

FILE 1: {file1}
```python
{content1[:3000]}  # Truncate for context limits
```

FILE 2: {file2}
```python
{content2[:3000]}  # Truncate for context limits
```

SIMILARITY SCORE: {similarity:.3f} (high similarity detected)

Focus on:
1. Functional redundancy (duplicate logic)
2. Architectural inconsistencies (different patterns for same purpose)
3. Opportunities for refactoring/consolidation
4. Interface mismatches between similar components

Provide specific, actionable findings."""

                    # Analyze the pair
                    try:
                        response = self.llm.invoke(pair_analysis_prompt)
                        findings = self._parse_llm_response(response.content, file1)

                        for finding in findings:
                            # Mark as high-priority similarity-based finding
                            finding.message = f"[SIMILARITY-GUIDED] {finding.message}"
                            yield finding

                    except Exception as e:
                        logger.debug(f"LLM analysis failed for pair {file1.name}-{file2.name}: {e}")
                        continue

            # Step 2: Global analysis on remaining files
            remaining_files = [
                f
                for f in analysis_files
                if f
                not in {pair[0] for pair in high_sim_pairs} | {pair[1] for pair in high_sim_pairs}
            ]

            if remaining_files:
                logger.info(f"Phase 2: Global analysis on {len(remaining_files)} remaining files")
                yield from self._analyze_global_structure(primary_file, remaining_files)

        except Exception as e:
            logger.error(f"Similarity-guided analysis failed: {e}")
            # Fallback to regular analysis
            yield from self._analyze_global_structure(primary_file, analysis_files)

    def _test_connectivity(self) -> bool:
        """Test if LLM service is available using new dual LLM system."""
        try:
            if not self.llm_manager:
                logger.debug("No LLM manager available")
                return False

            # Test new dual LLM system
            from ...llm import LLMRequest, LLMRole

            # Try a simple request to test connectivity
            test_request = LLMRequest(
                content="Test connectivity - respond with 'OK'", task_type="connectivity_test"
            )

            # Test with available LLM (sync version for validator compatibility)
            import asyncio

            async def test_async():
                if self.llm_manager.is_llm_available(LLMRole.FAST):
                    response = await self.llm_manager._call_fast_llm(test_request)
                    logger.debug(
                        f"Fast LLM connectivity test successful: {response.get('content', '')[:50]}"
                    )
                    return True
                elif self.llm_manager.is_llm_available(LLMRole.ORCHESTRATOR):
                    response = await self.llm_manager._call_orchestrator_llm(test_request)
                    logger.debug(
                        f"Orchestrator LLM connectivity test successful: {response.get('content', '')[:50]}"
                    )
                    return True
                else:
                    logger.debug("No LLMs available in dual system")
                    return False

            # Run async test in sync context
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                result = loop.run_until_complete(test_async())
                return result
            finally:
                loop.close()

        except Exception as e:
            logger.debug(f"LLM connectivity test failed: {e}")
            return False

    def _discover_context_limits(self) -> None:
        """Dynamically discover the LLM's actual context window limits."""
        if self.discovered_context_limit is not None:
            logger.debug(f"Using cached context limit: {self.discovered_context_limit}")
            return

        logger.info("Probing LLM for actual context window limits...")

        # Binary search for maximum context size
        min_tokens = 1000
        max_tokens = 200000  # Start with a high upper bound
        working_limit = min_tokens

        # Quick test sizes to find rough range
        test_sizes = [4000, 8000, 16000, 32000, 64000, 128000]

        for test_size in test_sizes:
            if self._test_context_size(test_size):
                working_limit = test_size
                logger.debug(f"PASS - Context size {test_size} tokens: PASSED")
            else:
                logger.debug(f"FAIL - Context size {test_size} tokens: FAILED")
                max_tokens = test_size
                break

        # Binary search within the working range
        min_tokens = working_limit
        attempts = 0
        max_attempts = 8  # Limit binary search iterations

        while min_tokens < max_tokens - 1000 and attempts < max_attempts:
            mid_tokens = (min_tokens + max_tokens) // 2

            if self._test_context_size(mid_tokens):
                min_tokens = mid_tokens
                working_limit = mid_tokens
                logger.debug(f"PASS - Binary search {mid_tokens} tokens: PASSED")
            else:
                max_tokens = mid_tokens
                logger.debug(f"FAIL - Binary search {mid_tokens} tokens: FAILED")

            attempts += 1

        self.discovered_context_limit = working_limit
        efficiency_gain = (working_limit / self.max_context_tokens) * 100

        if working_limit > self.max_context_tokens:
            logger.info(
                f"SUCCESS - Discovered larger context window: {working_limit:,} tokens (configured: {self.max_context_tokens:,})"
            )
            logger.info(
                f"TIP - OPTIMIZATION: Increase max_context_tokens to {working_limit} for {efficiency_gain:.0f}% better utilization"
            )
        elif working_limit < self.max_context_tokens:
            logger.warning(
                f"WARNING - Actual context limit lower than configured: {working_limit:,} tokens vs {self.max_context_tokens:,}"
            )
            logger.warning("TIP - RECOMMENDATION: Reduce max_context_tokens to avoid errors")
        else:
            logger.info(f"OK - Context configuration optimal: {working_limit:,} tokens")

    def _test_context_size(self, token_count: int) -> bool:
        """Test if the LLM can handle a specific context size."""
        # Use cache to avoid repeated tests
        if token_count in self.context_probe_cache:
            return self.context_probe_cache[token_count]

        # Generate test content roughly matching token count
        # Approximate 4 characters per token for English text
        char_count = token_count * 4
        test_content = "def test_function():\n    pass\n\n" * (char_count // 30)

        test_prompt = f"""Analyze this code for basic issues. Respond with just "OK" if analysis completes successfully.

CODE TO ANALYZE:
{test_content[:char_count]}

Respond with exactly: OK"""

        try:
            response = self.llm.invoke(test_prompt)
            success = "OK" in str(response).upper()
            self.context_probe_cache[token_count] = success
            return success

        except Exception as e:
            error_msg = str(e).lower()
            # Check for context-related errors
            context_errors = [
                "context length",
                "token limit",
                "max tokens",
                "context window",
                "input too long",
                "maximum context",
                "context exceeded",
            ]

            is_context_error = any(err in error_msg for err in context_errors)
            if is_context_error:
                logger.debug(f"Context limit reached at {token_count} tokens: {e}")
            else:
                logger.debug(f"Non-context error at {token_count} tokens: {e}")

            self.context_probe_cache[token_count] = False
            return False

    def _get_effective_context_limit(self) -> int:
        """Get the actual usable context limit (discovered or configured)."""
        if self.enable_context_probing and self.discovered_context_limit:
            return self.discovered_context_limit
        return self.max_context_tokens

    def _estimate_tokens(self, text: str) -> int:
        """Rough token estimation (4 chars per token for English)."""
        return max(1, len(text) // 4)

    def _optimize_context_usage(self, prompt: str) -> tuple[str, dict]:
        """Optimize prompt for maximum context utilization while tracking metrics."""
        estimated_input_tokens = self._estimate_tokens(prompt)

        # Use discovered context limit if available
        effective_context_limit = self._get_effective_context_limit()

        # Calculate optimal generation tokens based on actual context limits
        available_tokens = effective_context_limit - estimated_input_tokens
        optimal_generation_tokens = min(
            self.max_tokens, available_tokens - 500
        )  # 500 token safety buffer

        context_efficiency = estimated_input_tokens / effective_context_limit

        metrics = {
            "estimated_input_tokens": estimated_input_tokens,
            "optimal_generation_tokens": optimal_generation_tokens,
            "context_efficiency": context_efficiency,
            "context_utilization_percent": context_efficiency * 100,
            "effective_context_limit": effective_context_limit,
            "using_discovered_limit": self.discovered_context_limit is not None,
        }

        if self.enable_token_diagnostics:
            limit_source = "discovered" if self.discovered_context_limit else "configured"
            logger.info(
                f"Context utilization: {context_efficiency:.1%} ({estimated_input_tokens:,}/{effective_context_limit:,} tokens, {limit_source})"
            )
            if context_efficiency > 0.8:
                logger.warning(
                    f"High context usage ({context_efficiency:.1%}) - consider reducing batch size"
                )
            elif context_efficiency < 0.3:
                logger.info(
                    f"Low context usage ({context_efficiency:.1%}) - could increase batch size for efficiency"
                )

        return prompt, metrics

    def _invoke_with_diagnostics(self, prompt: str) -> str:
        """Invoke LLM with optimal context usage and diagnostics."""
        optimized_prompt, metrics = self._optimize_context_usage(prompt)

        try:
            # Use new dual LLM system for analysis
            import asyncio

            from ...llm import LLMRequest, LLMRole

            async def call_llm():
                # Use fast LLM for architectural analysis (orchestrator is too slow for interactive use)
                request = LLMRequest(
                    content=optimized_prompt,
                    task_type="architectural_analysis",
                    max_tokens=self.max_tokens,
                    temperature=self.temperature,
                )

                if self.llm_manager.is_llm_available(LLMRole.FAST):
                    response = await self.llm_manager._call_fast_llm(request)
                elif self.llm_manager.is_llm_available(LLMRole.ORCHESTRATOR):
                    response = await self.llm_manager._call_orchestrator_llm(request)
                else:
                    raise Exception("No LLM available")

                return response.get("content", "")

            # Run async in sync context
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                response_content = loop.run_until_complete(call_llm())
            finally:
                loop.close()

            cleaned_response = self._clean_response(response_content)

            # Track usage statistics
            if self.enable_token_diagnostics:
                estimated_output_tokens = self._estimate_tokens(cleaned_response)
                self.token_usage_stats["total_input_tokens"] += metrics["estimated_input_tokens"]
                self.token_usage_stats["total_output_tokens"] += estimated_output_tokens
                self.token_usage_stats["calls"] += 1
                self.token_usage_stats["context_efficiency"].append(metrics["context_efficiency"])

                logger.debug(
                    f"Call {self.token_usage_stats['calls']}: {metrics['estimated_input_tokens']}→{estimated_output_tokens} tokens"
                )

            return cleaned_response

        except Exception as e:
            logger.warning(f"LLM call failed: {e}")
            if self.enable_token_diagnostics:
                self.token_usage_stats["calls"] += 1
            return f"Error: LLM analysis failed - {e}"

    def _log_final_diagnostics(self):
        """Log final token usage diagnostics."""
        if not self.enable_token_diagnostics or self.token_usage_stats["calls"] == 0:
            return

        stats = self.token_usage_stats
        avg_efficiency = (
            sum(stats["context_efficiency"]) / len(stats["context_efficiency"])
            if stats["context_efficiency"]
            else 0
        )

        effective_limit = self._get_effective_context_limit()
        limit_source = "discovered" if self.discovered_context_limit else "configured"

        logger.info("=== LLM Usage Diagnostics ===")
        logger.info(f"Context window: {effective_limit:,} tokens ({limit_source})")
        logger.info(f"Total LLM calls: {stats['calls']}")
        logger.info(f"Total input tokens: {stats['total_input_tokens']:,}")
        logger.info(f"Total output tokens: {stats['total_output_tokens']:,}")
        logger.info(f"Average context efficiency: {avg_efficiency:.1%}")
        logger.info(
            f"Estimated cost efficiency: {(stats['total_input_tokens'] + stats['total_output_tokens']) / (stats['calls'] * effective_limit):.1%}"
        )

        if (
            self.discovered_context_limit
            and self.discovered_context_limit != self.max_context_tokens
        ):
            improvement = (self.discovered_context_limit / self.max_context_tokens - 1) * 100
            if improvement > 0:
                logger.info(
                    f"SUCCESS - Context discovery found {improvement:.0f}% larger window than configured!"
                )
            else:
                logger.warning(
                    f"WARNING - Context discovery found {abs(improvement):.0f}% smaller window than configured"
                )

        if avg_efficiency < 0.5:
            logger.info(
                "TIP: Context usage is low - consider increasing batch sizes for better LLM utilization"
            )
        elif avg_efficiency > 0.9:
            logger.warning(
                "WARNING: Context usage is very high - consider reducing batch sizes or file content"
            )
        else:
            logger.info("OK - Context usage is well-optimized")

    def _remove_thinking_tokens(self, text: str) -> str:
        """Remove thinking tokens from LLM response based on configured format."""
        if not self.remove_thinking_tokens:
            return text

        # Built-in format patterns
        format_patterns = {
            "harmony": [
                r"<\|channel\|>analysis<\|message\|>.*?(?=<\|channel\|>|<\|end\|>|$)",
                r"<\|channel\|>commentary<\|message\|>.*?(?=<\|channel\|>|<\|end\|>|$)",
                r"<\|[^|]*\|>",
                r"<think>.*?</think>",
                r"<thinking>.*?</thinking>",
                r"\[Thought:.*?\]",
                r"\[Internal:.*?\]",
            ],
            "qwen": [
                r"<think>.*?</think>",
                r"<思考>.*?</思考>",
                r"\[思考\].*?\[/思考\]",
                r"思考：.*?(?=\n|\r|\r\n|$)",
            ],
        }

        # Get patterns for the configured format
        if self.thinking_format in format_patterns:
            patterns = format_patterns[self.thinking_format]
        else:
            # Use custom patterns if format not recognized
            patterns = self.custom_thinking_patterns

        # Special handling for Harmony format - try to extract final channel first
        if self.thinking_format == "harmony":
            import re

            final_pattern = r"<\|channel\|>final<\|message\|>(.*?)(?:<\|end\|>|$)"
            final_matches = re.findall(final_pattern, text, re.DOTALL)

            if final_matches:
                # Found explicit final channel content
                result = final_matches[-1].strip()
                logger.debug(f"Extracted final channel content ({len(result)} chars)")
                return result

        # Remove all patterns
        import re

        cleaned_text = text
        for pattern in patterns:
            cleaned_text = re.sub(pattern, "", cleaned_text, flags=re.DOTALL | re.IGNORECASE)

        # Clean up extra whitespace
        cleaned_text = re.sub(r"\n\s*\n", "\n\n", cleaned_text)
        cleaned_text = cleaned_text.strip()

        # Log the cleaning if significant content was removed
        original_length = len(text)
        cleaned_length = len(cleaned_text)
        if original_length - cleaned_length > 50:
            logger.debug(
                f"Removed {original_length - cleaned_length} characters of thinking tokens"
            )

        return cleaned_text

    def _detect_unremoved_thinking_tokens(self, text: str) -> list[str]:
        """Detect potential thinking tokens that weren't removed."""
        common_patterns = {
            "<think>": r"<think>.*?</think>",
            "<reasoning>": r"<reasoning>.*?</reasoning>",
            "[THINKING]": r"\[THINKING\].*?\[/THINKING\]",
            "```thinking": r"```thinking.*?```",
            "<thought>": r"<thought>.*?</thought>",
            "# Thinking:": r"# Thinking:.*?(?=\n|\r|\r\n|$)",
            "## Analysis": r"## Analysis.*?(?=\n#|\n\n|$)",
        }

        detected = []
        for name, pattern in common_patterns.items():
            if re.search(pattern, text, re.DOTALL | re.IGNORECASE):
                detected.append(name)

        return detected

    def _warn_about_unremoved_tokens(self, text: str) -> None:
        """Warn user about potential unremoved thinking tokens and suggest configuration."""
        detected_patterns = self._detect_unremoved_thinking_tokens(text)

        if detected_patterns and len(detected_patterns) > 0:
            logger.warning(
                f"Detected potential unremoved thinking tokens: {', '.join(detected_patterns)}. "
                f"Consider updating your vibelint configuration:"
            )
            logger.warning(
                "Set thinking_format='custom' and add custom_thinking_patterns to your pyproject.toml:"
            )

            # Suggest specific patterns based on what was detected
            suggestions = []
            for pattern_name in detected_patterns:
                if pattern_name == "<think>":
                    suggestions.append("r'<think>.*?</think>'")
                elif pattern_name == "<reasoning>":
                    suggestions.append("r'<reasoning>.*?</reasoning>'")
                elif pattern_name == "[THINKING]":
                    suggestions.append("r'\\[THINKING\\].*?\\[/THINKING\\]'")
                elif pattern_name == "```thinking":
                    suggestions.append("r'```thinking.*?```'")
                elif pattern_name == "<thought>":
                    suggestions.append("r'<thought>.*?</thought>'")
                elif pattern_name == "# Thinking:":
                    suggestions.append("r'# Thinking:.*?(?=\\n|\\r|\\r\\n|$)'")
                elif pattern_name == "## Analysis":
                    suggestions.append("r'## Analysis.*?(?=\\n#|\\n\\n|$)'")

            if suggestions:
                logger.warning(
                    f"Add to [tool.vibelint.llm_analysis]: custom_thinking_patterns = {suggestions}"
                )

    def _analyze_global_structure(
        self, current_file: Path, analysis_files: list[Path]
    ) -> Iterator[Finding]:
        """Multi-phase intelligent analysis: file summaries → embedding similarity → pairwise comparison → global synthesis."""

        if not analysis_files:
            logger.warning("No analysis files provided")
            return

        logger.info(f"Starting multi-phase architectural analysis on {len(analysis_files)} files")

        # Phase 1: Generate individual file summaries using DFS traversal
        yield from self._phase1_generate_summaries(analysis_files)

        # Phase 2: Compute embeddings and find semantic similarities
        yield from self._phase2_semantic_clustering(analysis_files)

        # Phase 3: Pairwise analysis of similar files
        yield from self._phase3_pairwise_analysis()

        # Phase 4: Global synthesis of all findings
        yield from self._phase4_global_synthesis(current_file)

        # Log final token usage diagnostics
        self._log_final_diagnostics()

        logger.info(f"Multi-phase architectural analysis COMPLETED on {len(analysis_files)} files")

    def _phase1_generate_summaries(self, analysis_files: list[Path]) -> Iterator[Finding]:
        """Phase 1: Generate file summaries in efficient batches."""
        logger.info(f"Phase 1: Generating summaries for {len(analysis_files)} files")

        # Sort files by depth for DFS-like processing (deeper files first)
        sorted_files = sorted(analysis_files, key=lambda p: len(p.parts), reverse=True)

        # Batch files for efficiency (process 8 files per LLM call)
        batch_size = 8
        batches = [
            sorted_files[i : i + batch_size] for i in range(0, len(sorted_files), batch_size)
        ]

        for batch_idx, batch in enumerate(batches):
            try:
                # Create batch prompt with multiple files
                batch_content = []
                for file_path in batch:
                    try:
                        content = file_path.read_text(encoding="utf-8")
                        batch_content.append(f"FILE: {file_path}\n{content[:1500]}")
                    except Exception as e:
                        batch_content.append(f"FILE: {file_path}\nERROR: Could not read file - {e}")

                batch_prompt = f"""Analyze these Python files and provide concise summaries for each:

{chr(10).join(batch_content)}

For each file, provide a summary in this format:
FILE: [filename]
PURPOSE: What this file does
EXPORTS: Key classes/functions it exports
DEPENDENCIES: What it imports/depends on
CONCERNS: Any potential issues
---
"""

                batch_summaries = self._invoke_with_diagnostics(batch_prompt)

                # Parse batch response using generalized retry pattern
                self._process_batch_with_retry(batch, batch_summaries)
                logger.debug(f"Processed batch {batch_idx + 1}/{len(batches)} ({len(batch)} files)")

            except Exception as e:
                logger.warning(f"Failed to process batch {batch_idx + 1}: {e}")
                # Fall back to individual error summaries
                for file_path in batch:
                    self._file_summaries[file_path] = f"ERROR: Batch processing failed - {e}"

        logger.info(f"Phase 1 complete: Generated {len(self._file_summaries)} summaries")
        return iter([])  # No findings yet, just building state

    def _parse_batch_summaries(self, batch_files: list[Path], batch_response: str) -> None:
        """Parse batch summary response and assign to individual files."""

        # Strategy 1: Try original format with FILE: markers
        file_sections = re.split(r"FILE:\s*([^\n]+)", batch_response)
        parsed_files = set()

        if len(file_sections) > 1:
            for i in range(1, len(file_sections), 2):
                if i + 1 < len(file_sections):
                    filename_part = file_sections[i].strip()
                    summary_content = file_sections[i + 1].split("---")[0].strip()

                    # Find matching file by name
                    for file_path in batch_files:
                        if (
                            file_path.name in filename_part
                            or str(file_path) in filename_part
                            or str(file_path).replace("src/", "") in filename_part
                        ):
                            self._file_summaries[file_path] = summary_content
                            parsed_files.add(file_path)
                            break

        # Strategy 2: For unparsed files, try alternative markers
        unparsed_files = [f for f in batch_files if f not in parsed_files]
        if unparsed_files and batch_response:
            # Look for filename patterns anywhere in the response
            for file_path in unparsed_files:
                file_name = file_path.name
                # Try to extract content around filename mentions
                pattern = rf"(?:{re.escape(file_name)}|{re.escape(str(file_path))})[:\s]*([^{chr(10)}]*(?:{chr(10)}[^{chr(10)}]*)*?)(?=(?:FILE:|{re.escape(file_name)}|\Z))"
                matches = re.finditer(pattern, batch_response, re.IGNORECASE | re.MULTILINE)
                for match in matches:
                    content = match.group(1).strip()
                    if content and len(content) > 10:  # Only accept substantial content
                        self._file_summaries[file_path] = content
                        parsed_files.add(file_path)
                        break

        # Strategy 3: Fallback - split response evenly among remaining files
        still_unparsed = [f for f in batch_files if f not in parsed_files]
        if still_unparsed and batch_response:
            # If we have a response but couldn't parse it properly,
            # try to split it among the files
            response_chunks = re.split(r"\n\s*\n", batch_response.strip())
            meaningful_chunks = [
                chunk.strip()
                for chunk in response_chunks
                if chunk.strip() and len(chunk.strip()) > 20
            ]

            if meaningful_chunks:
                for i, file_path in enumerate(still_unparsed):
                    if i < len(meaningful_chunks):
                        self._file_summaries[file_path] = meaningful_chunks[i]
                        parsed_files.add(file_path)

        # Final fallback - ensure all files have some content
        failed_files = []
        for file_path in batch_files:
            if file_path not in self._file_summaries:
                # Try to generate a basic summary from file name/path
                basic_summary = f"Python file: {file_path.name}. Analysis needed."
                self._file_summaries[file_path] = basic_summary
                failed_files.append(file_path)

        return failed_files

    def _process_batch_with_retry(self, batch_files: list[Path], initial_response: str):
        """Process batch using generalized retry pattern."""
        from ...llm.llm_retry import (FileAnalysisRetryHandler, RetryConfig,
                                      RetryStrategy)

        # Create retry handler with configuration
        retry_config = RetryConfig(
            max_retries=2, strategy=RetryStrategy.FEW_SHOT, retry_threshold=3, enable_logging=True
        )

        # Create handler with our LLM callable
        handler = FileAnalysisRetryHandler(
            llm_callable=self._invoke_with_diagnostics, config=retry_config
        )

        # Since we already have the initial response, we need a slightly different approach
        # First, try to parse the initial response
        initial_result = handler.parse_response(batch_files, initial_response)

        # Store successful results
        for file_path, summary in initial_result.parsed_data.items():
            self._file_summaries[file_path] = (
                f"PURPOSE: {summary.purpose}\nEXPORTS: {summary.exports}\nDEPENDENCIES: {summary.dependencies}\nCONCERNS: {summary.concerns}"
            )

        # If there are failures, use the retry mechanism
        if initial_result.failed_items:
            logger.info(f"Retrying analysis for {len(initial_result.failed_items)} failed files")

            # Process only the failed items with retry
            retry_result = handler.process_with_retry(initial_result.failed_items)

            # Store retry results
            for file_path, summary in retry_result.parsed_data.items():
                self._file_summaries[file_path] = (
                    f"PURPOSE: {summary.purpose}\nEXPORTS: {summary.exports}\nDEPENDENCIES: {summary.dependencies}\nCONCERNS: {summary.concerns}"
                )

            # Log final stats
            stats = handler.get_stats()
            logger.info(f"Retry processing complete: {stats}")

            # For any still-failed files, use fallback
            for file_path in retry_result.failed_items:
                self._file_summaries[file_path] = f"Python file: {file_path.name}. Analysis needed."

    def _parse_batch_summaries_with_retry(
        self, batch_files: list[Path], initial_response: str, original_prompt: str
    ) -> list[Path]:
        """Parse batch summaries with few-shot learning retry for failed parsing."""

        # First attempt with existing logic
        failed_files = self._parse_batch_summaries(batch_files, initial_response)

        # If parsing failed for some files, try few-shot learning retry
        if failed_files and len(failed_files) <= 3:  # Only retry for small numbers of failures
            logger.info(f"Retrying parsing for {len(failed_files)} files with few-shot examples")

            # Create few-shot learning prompt with examples
            retry_prompt = f"""The previous response didn't follow the expected format. Here are examples of the correct format:

EXAMPLE 1:
FILE: src/example/config.py
PURPOSE: Configuration loading and validation for the application
EXPORTS: ConfigLoader class, load_config() function
DEPENDENCIES: pathlib, yaml, logging
CONCERNS: No input validation on config file paths
---

EXAMPLE 2:
FILE: src/example/utils.py
PURPOSE: Utility functions for string manipulation and file operations
EXPORTS: sanitize_string(), read_file_safe(), write_file_atomic()
DEPENDENCIES: re, pathlib, tempfile
CONCERNS: Limited error handling in file operations
---

Now please re-analyze these specific files using the EXACT format shown above:

{chr(10).join(f"FILE: {fp}{chr(10)}Content preview: {str(fp.read_text(encoding='utf-8')[:500])[:200]}..." for fp in failed_files)}

Remember: Use the exact format with FILE:, PURPOSE:, EXPORTS:, DEPENDENCIES:, CONCERNS: and end each file with ---"""

            try:
                retry_response = self._invoke_with_diagnostics(retry_prompt)

                # Parse the retry response using the same logic
                retry_failed = self._parse_batch_summaries(failed_files, retry_response)

                # Log success/failure of retry
                retry_success_count = len(failed_files) - len(retry_failed)
                if retry_success_count > 0:
                    logger.info(
                        f"Few-shot retry succeeded for {retry_success_count}/{len(failed_files)} files"
                    )
                else:
                    logger.warning(f"Few-shot retry failed for all {len(failed_files)} files")

                return retry_failed

            except Exception as e:
                logger.warning(f"Few-shot retry failed with error: {e}")
                return failed_files

        return failed_files

    def _parse_batch_pairwise(
        self, batch_pairs: list[tuple[Path, Path]], batch_response: str
    ) -> None:
        """Parse batch pairwise response and assign to individual pairs."""
        # Split by comparison markers
        comparison_sections = re.split(r"COMPARISON\s*(\d+):", batch_response)

        # comparison_sections[0] is empty, then alternates between comparison number and content
        for i in range(1, len(comparison_sections), 2):
            if i + 1 < len(comparison_sections):
                comparison_num = int(comparison_sections[i].strip()) - 1  # Convert to 0-based index
                analysis_content = comparison_sections[i + 1].split("---")[0].strip()

                # Assign to corresponding pair
                if 0 <= comparison_num < len(batch_pairs):
                    file1, file2 = batch_pairs[comparison_num]
                    self._pairwise_analyses[(file1, file2)] = analysis_content

        # Ensure all pairs have analyses (fallback)
        for file1, file2 in batch_pairs:
            if (file1, file2) not in self._pairwise_analyses:
                self._pairwise_analyses[(file1, file2)] = (
                    "ERROR: Could not parse pairwise analysis from batch response"
                )

    def _phase2_semantic_clustering(self, analysis_files: list[Path]) -> Iterator[Finding]:
        """Phase 2: Compute embeddings and find semantically similar files."""
        logger.info(f"Phase 2: Computing semantic similarities for {len(analysis_files)} files")

        try:
            # Get the shared embedding model
            embedding_model = self._get_embedding_model()
            if not embedding_model:
                logger.warning("Embedding model not available, skipping semantic clustering")
                return iter([])

            # Extract embeddings from summaries
            for file_path, summary in self._file_summaries.items():
                if summary and not summary.startswith("ERROR:"):
                    # Use the embedding model to get vector representation
                    embedding = embedding_model.encode([summary])[0].tolist()
                    self._file_embeddings[file_path] = embedding

            # Find similar file pairs using cosine similarity
            from itertools import combinations

            import numpy as np

            similar_pairs = []
            files_with_embeddings = list(self._file_embeddings.keys())

            for file1, file2 in combinations(files_with_embeddings, 2):
                emb1 = np.array(self._file_embeddings[file1])
                emb2 = np.array(self._file_embeddings[file2])

                # Cosine similarity
                similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))

                # Group files with >70% similarity for pairwise analysis
                if similarity > 0.7:
                    similar_pairs.append((file1, file2, similarity))
                    logger.debug(
                        f"Found similar files: {file1.name} <-> {file2.name} ({similarity:.3f})"
                    )

            # Create clusters from similar pairs
            clusters = []
            for file1, file2, _sim in similar_pairs:
                # Find existing cluster or create new one
                placed = False
                for cluster in clusters:
                    if file1 in cluster or file2 in cluster:
                        cluster.update([file1, file2])
                        placed = True
                        break
                if not placed:
                    clusters.append({file1, file2})

            self._similarity_clusters = [list(cluster) for cluster in clusters]
            logger.info(
                f"Phase 2 complete: Found {len(self._similarity_clusters)} similarity clusters"
            )

        except Exception as e:
            logger.warning(f"Phase 2 semantic clustering failed: {e}")

        return iter([])  # No findings yet

    def _phase3_pairwise_analysis(self) -> Iterator[Finding]:
        """Phase 3: Deep pairwise analysis of semantically similar files (batched for efficiency)."""
        logger.info(f"Phase 3: Analyzing {len(self._similarity_clusters)} similarity clusters")

        # Collect all pairs from all clusters
        all_pairs = []
        for cluster in self._similarity_clusters:
            if len(cluster) < 2:
                continue

            from itertools import combinations

            cluster_pairs = list(combinations(cluster, 2))
            all_pairs.extend(cluster_pairs)

        if not all_pairs:
            logger.info("Phase 3 complete: No similarity pairs found")
            return iter([])

        logger.info(f"Phase 3: Analyzing {len(all_pairs)} similarity pairs")

        # Batch pairs for efficiency (process 4 pairs per LLM call)
        batch_size = 4
        pair_batches = [all_pairs[i : i + batch_size] for i in range(0, len(all_pairs), batch_size)]

        for batch_idx, batch in enumerate(pair_batches):
            try:
                # Create batch prompt with multiple pairs
                batch_comparisons = []
                for file1, file2 in batch:
                    summary1 = self._file_summaries.get(file1, "No summary")
                    summary2 = self._file_summaries.get(file2, "No summary")

                    comparison = f"""COMPARISON {len(batch_comparisons) + 1}:
FILE 1: {file1}
{summary1[:400]}

FILE 2: {file2}
{summary2[:400]}
"""
                    batch_comparisons.append(comparison)

                batch_prompt = f"""Analyze these file pairs for duplication and refactoring opportunities:

{chr(10).join(batch_comparisons)}

For each comparison, analyze:
- Code duplication opportunities
- Potential for shared abstractions
- Redundant functionality
- Refactoring suggestions

Format for each:
COMPARISON [N]:
DUPLICATION: What's duplicated
ABSTRACTION: What could be abstracted
REFACTOR: How to refactor
---
"""

                batch_analyses = self._invoke_with_diagnostics(batch_prompt)

                # Parse batch response and assign to individual pairs
                self._parse_batch_pairwise(batch, batch_analyses)
                logger.debug(
                    f"Processed pair batch {batch_idx + 1}/{len(pair_batches)} ({len(batch)} pairs)"
                )

            except Exception as e:
                logger.warning(f"Failed to process pair batch {batch_idx + 1}: {e}")
                # Fall back to individual error analyses
                for file1, file2 in batch:
                    self._pairwise_analyses[(file1, file2)] = f"Error analyzing pair: {e}"

        logger.info(f"Phase 3 complete: Analyzed {len(self._pairwise_analyses)} file pairs")
        return iter([])  # No findings yet

    def _phase4_global_synthesis(self, current_file: Path) -> Iterator[Finding]:
        """Phase 4: Synthesize all findings into global architectural insights."""
        logger.info("Phase 4: Synthesizing global architectural findings")

        # Combine all summaries and pairwise analyses
        all_summaries = "\n\n".join(
            [f"{path}: {summary}" for path, summary in self._file_summaries.items()]
        )
        all_pairwise = "\n\n".join(
            [f"{p1} <-> {p2}: {analysis}" for (p1, p2), analysis in self._pairwise_analyses.items()]
        )

        synthesis_prompt = f"""Based on individual file summaries and pairwise comparisons, identify architectural issues:

FILE SUMMARIES:
{all_summaries[:3000]}

PAIRWISE ANALYSES:
{all_pairwise[:2000]}

Identify systemic issues in this format:

ISSUE: Architectural problem description
SUGGESTION: How to fix it
---
ISSUE: Another architectural problem
SUGGESTION: Another solution
---

Focus on:
- Widespread code duplication patterns
- Missing architectural layers
- Inconsistent patterns across the codebase
- Opportunities for shared abstractions
"""

        try:
            cleaned_response = self._invoke_with_diagnostics(synthesis_prompt)

            # Parse findings using regex
            import re

            issue_pattern = r"ISSUE:\s*(.*?)\s*SUGGESTION:\s*(.*?)(?=---|\s*$)"
            findings = re.findall(issue_pattern, cleaned_response, re.DOTALL | re.IGNORECASE)

            if findings:
                for issue, suggestion in findings:
                    issue = issue.strip()
                    suggestion = suggestion.strip()
                    if issue and suggestion:
                        yield self.create_finding(
                            message=f"[ARCHITECTURAL] {issue} - {suggestion}",
                            file_path=current_file,
                            line=1,
                        )
            else:
                # Fallback
                yield self.create_finding(
                    message=f"Global architectural analysis: {cleaned_response[:400]}...",
                    file_path=current_file,
                    line=1,
                )

        except Exception as e:
            logger.warning(f"Phase 4 synthesis failed: {e}")

        logger.info("Phase 4 complete: Global synthesis finished")

    def _clean_response(self, response) -> str:
        """Clean and extract response content."""
        response_text = response.content if hasattr(response, "content") else str(response)
        if not isinstance(response_text, str):
            response_text = str(response_text)
        cleaned = self._remove_thinking_tokens(response_text)
        self._warn_about_unremoved_tokens(cleaned)
        return cleaned

    def _get_embedding_model(self):
        """Get the shared embedding model from the rules engine."""
        try:
            # Check if the model was passed in config
            if self.config and hasattr(self.config, "get"):
                shared_model = self.config.get("_shared_model")
                if shared_model:
                    return shared_model

            # Try to load the model directly
            from sentence_transformers import SentenceTransformer

            # Use the default EmbeddingGemma model
            model_name = "google/embeddinggemma-300m"
            logger.info(f"Loading embedding model: {model_name}")
            model = SentenceTransformer(model_name)
            return model

        except Exception as e:
            logger.warning(f"Could not access embedding model: {e}")
            return None

    def _create_structured_summary(self, file_paths: list[Path]) -> str:
        """Create a structured summary optimized for LLM analysis with advanced compression strategies."""
        summary_parts = []
        total_tokens_estimate = 0

        # Rough token estimation: ~4 chars per token
        max_content_tokens = self.max_prompt_tokens - 2000  # Reserve space for prompt template

        # Limit files to prevent token overflow
        limited_files = file_paths[: self.max_files]

        # First pass: collect and prioritize content
        file_contents = []
        for path in limited_files:
            try:
                content = path.read_text(encoding="utf-8")
                compressed_content = self._compress_file_content(path, content)
                if compressed_content:
                    file_contents.append((path, compressed_content))
            except Exception as e:
                logger.debug(f"Could not analyze {path}: {e}")

        # Second pass: fit content within token budget
        for path, compressed_content in file_contents:
            try:
                rel_path = path.relative_to(path.parents[2])
            except (ValueError, IndexError):
                rel_path = path

            # Create file summary with compression indicators
            file_summary = f"=== {rel_path} ===\n{compressed_content}"

            # Estimate tokens for this file summary
            file_tokens = len(file_summary) // 4

            # Check if adding this file would exceed context window
            if total_tokens_estimate + file_tokens > max_content_tokens:
                logger.info(
                    f"Context window limit reached, truncating at {len(summary_parts)} files"
                )
                break

            summary_parts.append(file_summary)
            total_tokens_estimate += file_tokens

        result = "\n\n".join(summary_parts)
        logger.info(
            f"Generated compressed summary with ~{total_tokens_estimate} tokens for {len(summary_parts)} files"
        )
        return result

    def _compress_file_content(self, file_path: Path, content: str) -> str:
        """Apply multiple compression strategies to file content."""
        lines = content.split("\n")

        # Strategy 1: Extract architectural signatures
        signatures = self._extract_architectural_signatures(lines)

        # Strategy 2: Summarize imports and dependencies
        import_summary = self._summarize_imports(lines) if self.enable_import_summary else None

        # Strategy 3: Extract class/function hierarchy
        hierarchy = (
            self._extract_code_hierarchy(lines) if self.enable_hierarchy_extraction else None
        )

        # Strategy 4: Find architectural patterns and anti-patterns
        patterns = self._identify_patterns(lines) if self.enable_pattern_detection else None

        # Strategy 5: Extract complexity indicators
        complexity = (
            self._analyze_complexity_indicators(lines, file_path)
            if self.enable_complexity_analysis
            else None
        )

        # Combine all compressed information
        compressed_parts = []

        if import_summary:
            compressed_parts.append(f"IMPORTS: {import_summary}")

        if hierarchy:
            compressed_parts.append(f"STRUCTURE:\n{hierarchy}")

        if signatures:
            compressed_parts.append(
                f"KEY_CODE:\n{chr(10).join(signatures[:self.max_signature_lines])}"
            )

        if patterns:
            compressed_parts.append(f"PATTERNS: {patterns}")

        if complexity:
            compressed_parts.append(f"COMPLEXITY: {complexity}")

        return "\n".join(compressed_parts)

    def _extract_architectural_signatures(self, lines: list[str]) -> list[str]:
        """Extract the most architecturally significant lines."""
        signatures = []

        for line in lines[: self.max_file_lines]:
            stripped = line.strip()

            # High priority: classes, functions, decorators
            if (
                stripped.startswith(("class ", "def ", "async def ", "@"))
                or
                # Medium priority: control flow and error handling
                any(keyword in stripped for keyword in ["raise", "except", "finally", "with"])
                or
                # Architecture indicators
                any(pattern in stripped for pattern in ["TODO", "FIXME", "XXX", "HACK", "NOTE"])
                or
                # Design patterns
                any(
                    pattern in stripped
                    for pattern in ["factory", "singleton", "observer", "strategy", "adapter"]
                )
                or
                # Type hints and contracts
                "->" in stripped
                or ": " in stripped
                and "=" not in stripped
            ):

                signatures.append(line)

        return signatures

    def _summarize_imports(self, lines: list[str]) -> str:
        """Summarize import structure to understand dependencies."""
        imports = {"stdlib": set(), "third_party": set(), "local": set()}

        for line in lines:
            stripped = line.strip()
            if stripped.startswith(("import ", "from ")):
                if stripped.startswith("from .") or stripped.startswith("from .."):
                    imports["local"].add(stripped)
                elif any(
                    stdlib in stripped
                    for stdlib in ["os", "sys", "json", "logging", "pathlib", "typing"]
                ):
                    imports["stdlib"].add(stripped)
                else:
                    imports["third_party"].add(stripped)

        summary_parts = []
        if imports["stdlib"]:
            summary_parts.append(f"stdlib({len(imports['stdlib'])})")
        if imports["third_party"]:
            summary_parts.append(f"3rd_party({len(imports['third_party'])})")
        if imports["local"]:
            summary_parts.append(f"local({len(imports['local'])})")

        return ", ".join(summary_parts)

    def _extract_code_hierarchy(self, lines: list[str]) -> str:
        """Extract class and function hierarchy structure."""
        hierarchy = []
        current_class = None
        indent_level = 0

        for line in lines:
            stripped = line.strip()
            if not stripped:
                continue

            # Calculate indentation
            leading_spaces = len(line) - len(line.lstrip())

            if stripped.startswith("class "):
                class_name = stripped.split("(")[0].replace("class ", "").strip(":")
                current_class = class_name
                hierarchy.append(f"class {class_name}")
                indent_level = leading_spaces

            elif stripped.startswith(("def ", "async def ")) and current_class:
                func_name = stripped.split("(")[0].replace("def ", "").replace("async ", "").strip()
                if leading_spaces > indent_level:  # Method inside class
                    hierarchy.append(f"  └─ {func_name}()")
                else:  # New top-level function
                    current_class = None
                    hierarchy.append(f"def {func_name}()")

            elif stripped.startswith(("def ", "async def ")):
                func_name = stripped.split("(")[0].replace("def ", "").replace("async ", "").strip()
                hierarchy.append(f"def {func_name}()")
                current_class = None

        return "\n".join(hierarchy[: self.max_hierarchy_depth])  # Use configured hierarchy depth

    def _identify_patterns(self, lines: list[str]) -> str:
        """Identify architectural patterns and anti-patterns."""
        patterns = []

        # Count pattern indicators
        class_count = sum(1 for line in lines if line.strip().startswith("class "))
        function_count = sum(1 for line in lines if line.strip().startswith(("def ", "async def ")))
        import_count = sum(1 for line in lines if line.strip().startswith(("import ", "from ")))

        # Identify specific patterns
        has_inheritance = any("(" in line and line.strip().startswith("class ") for line in lines)
        has_decorators = any(line.strip().startswith("@") for line in lines)
        has_async = any("async " in line for line in lines)
        has_context_managers = any("with " in line for line in lines)
        has_exceptions = any(word in line for line in lines for word in ["raise", "except", "try:"])

        # Anti-pattern detection
        long_functions = []
        for i, line in enumerate(lines):
            if line.strip().startswith(("def ", "async def ")):
                # Count lines until next function/class or end
                func_lines = 0
                for j in range(i + 1, min(i + 100, len(lines))):
                    if lines[j].strip().startswith(("def ", "class ", "async def ")):
                        break
                    if lines[j].strip():  # Non-empty line
                        func_lines += 1
                if func_lines > 50:  # Arbitrary threshold
                    func_name = line.split("(")[0].replace("def ", "").replace("async ", "").strip()
                    long_functions.append(f"{func_name}({func_lines}L)")

        # Build pattern summary
        if class_count:
            patterns.append(f"classes({class_count})")
        if function_count:
            patterns.append(f"functions({function_count})")
        if import_count > 10:
            patterns.append(f"heavy_imports({import_count})")
        if has_inheritance:
            patterns.append("inheritance")
        if has_decorators:
            patterns.append("decorators")
        if has_async:
            patterns.append("async")
        if has_context_managers:
            patterns.append("context_mgmt")
        if has_exceptions:
            patterns.append("error_handling")
        if long_functions:
            patterns.append(f"long_funcs[{','.join(long_functions[:3])}]")

        return ", ".join(patterns)

    def _analyze_complexity_indicators(self, lines: list[str], file_path: Path) -> str:
        """Analyze complexity and quality indicators."""
        indicators = []

        # File size indicators
        total_lines = len(lines)
        code_lines = len(
            [line for line in lines if line.strip() and not line.strip().startswith("#")]
        )

        # Complexity indicators
        nested_blocks = 0
        max_nesting = 0
        current_nesting = 0

        for line in lines:
            stripped = line.strip()
            if any(keyword in stripped for keyword in ["if ", "for ", "while ", "with ", "try:"]):
                current_nesting += 1
                max_nesting = max(max_nesting, current_nesting)
                nested_blocks += 1
            # Simple heuristic for block end (not perfect but gives indication)
            elif stripped and not line.startswith(" ") and current_nesting > 0:
                current_nesting = max(0, current_nesting - 1)

        # Quality indicators
        todo_count = sum(
            1
            for line in lines
            if any(marker in line.upper() for marker in ["TODO", "FIXME", "XXX", "HACK"])
        )
        comment_lines = sum(1 for line in lines if line.strip().startswith("#"))

        # Build complexity summary
        if total_lines > 200:
            indicators.append(f"large_file({total_lines}L)")
        if max_nesting > 3:
            indicators.append(f"deep_nesting({max_nesting})")
        if todo_count > 0:
            indicators.append(f"todos({todo_count})")
        if comment_lines / max(code_lines, 1) < 0.1:
            indicators.append("low_comments")
        elif comment_lines / max(code_lines, 1) > 0.3:
            indicators.append("well_documented")

        return ", ".join(indicators) if indicators else "moderate_complexity"
```

---
### File: src/vibelint/validators/architecture/semantic_similarity.py

```python
"""
Semantic similarity discovery heuristic using EmbeddingGemma.

This validator serves as a DISCOVERY TOOL rather than a definitive problem indicator.
It identifies potentially redundant code patterns by analyzing semantic similarity of
docstrings, functions, and classes using local embedding models.

Primary use cases:
1. Human-driven redundancy exploration (INFO level findings)
2. Prioritization heuristic for expensive LLM analysis
3. Architectural refactoring planning

The findings are informational - high similarity doesn't always indicate problems,
but helps focus attention on areas that may benefit from consolidation or refactoring.

vibelint/validators/semantic_similarity.py
"""

import logging
from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict, Iterator, List, Optional, Tuple

try:
    import importlib.util

    SENTENCE_TRANSFORMERS_AVAILABLE = importlib.util.find_spec("numpy") is not None
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False

if TYPE_CHECKING or SENTENCE_TRANSFORMERS_AVAILABLE:
    try:
        from sentence_transformers import SentenceTransformer
    except ImportError:
        SentenceTransformer = None

from ...embedding_client import EmbeddingClient
from ...plugin_system import BaseValidator, Finding, Severity

__all__ = ["SemanticSimilarityValidator"]
logger = logging.getLogger(__name__)


class SemanticSimilarityValidator(BaseValidator):
    """
    Validates code for semantic similarity patterns that indicate potential redundancy.

    Uses EmbeddingGemma to find:
    - Functions with similar docstrings but different implementations
    - Classes that serve similar purposes
    - Code patterns that are semantically duplicate
    """

    rule_id = "SEMANTIC-SIMILARITY"
    default_severity = Severity.INFO

    def __init__(
        self,
        severity: Optional[Severity] = None,
        config: Optional[Dict[str, Any]] = None,
    ) -> None:
        super().__init__(severity, config)
        # Get shared model from config if available
        self._model: Optional[Any] = config.get("_shared_model") if config else None
        self._code_cache: Dict[str, List[Tuple[Path, str, str, Any]]] = {}
        self._setup_attempted = False

    def _safe_config_get(self, config: Any, key: str, default=None):
        """Safely get a value from config object."""
        if hasattr(config, "get"):
            return config.get(key, default)
        elif hasattr(config, "__getitem__"):
            try:
                return config[key]
            except (KeyError, TypeError):
                return default
        else:
            return default

    def _setup_embedding_model(self, config: Any) -> bool:
        """
        Initialize the embedding model if available and configured.

        Returns:
            True if model is available and ready, False otherwise.
        """
        if self._setup_attempted:
            return self._model is not None

        self._setup_attempted = True

        # If we already have a shared model, use it
        if self._model is not None:
            embedding_config = self._safe_config_get(config, "embedding_analysis", {})
            self._similarity_threshold = self._safe_config_get(
                embedding_config, "similarity_threshold", 0.85
            )
            logger.info("Using pre-loaded embedding model")
            return True

        if not SENTENCE_TRANSFORMERS_AVAILABLE:
            logger.debug(
                "Semantic similarity analysis disabled: sentence-transformers not available"
            )
            return False

        # Check configuration
        embedding_config = self._safe_config_get(config, "embedding_analysis", {})
        embeddings_config = self._safe_config_get(config, "embeddings", {})

        # Use new embeddings config if available, otherwise fall back to legacy config
        if embeddings_config:
            similarity_threshold = self._safe_config_get(
                embeddings_config, "similarity_threshold", 0.85
            )
            enabled = self._safe_config_get(embeddings_config, "use_specialized_embeddings", True)
        else:
            model_name = self._safe_config_get(
                embedding_config, "model", "google/embeddinggemma-300m"
            )
            similarity_threshold = self._safe_config_get(
                embedding_config, "similarity_threshold", 0.85
            )
            enabled = self._safe_config_get(embedding_config, "enabled", False)

        # Check if embedding analysis is enabled
        if not enabled:
            logger.debug("Semantic similarity analysis disabled in configuration")
            return False

        try:
            # Try to initialize the new EmbeddingClient for specialized endpoints
            try:
                self._embedding_client = EmbeddingClient(config)
                self._similarity_threshold = similarity_threshold
                logger.info(
                    f"Initialized EmbeddingClient with specialized endpoints (threshold: {similarity_threshold})"
                )

                # Keep legacy _model for backward compatibility
                if hasattr(self._embedding_client, "_local_model"):
                    self._model = self._embedding_client._local_model
                return True
            except Exception as client_error:
                logger.debug(f"EmbeddingClient initialization failed: {client_error}")

            # Fallback to legacy local model approach
            if embeddings_config:
                logger.debug("Falling back to legacy embedding model approach")
                return False

            # Handle HF token from config, .env file, or environment
            hf_token = self._safe_config_get(embedding_config, "hf_token")
            if not hf_token:
                import os

                # Try to load from .env file
                env_file = getattr(config, "project_root", None)
                if env_file:
                    env_file = env_file / ".env"
                if env_file and env_file.exists():
                    for line in env_file.read_text().splitlines():
                        if line.startswith("HF_TOKEN="):
                            hf_token = line.split("=", 1)[1].strip().strip("\"'")
                            break
                # Fallback to environment variable
                if not hf_token:
                    hf_token = os.getenv("HF_TOKEN")

            if hf_token:
                # Set HF token for this session
                os.environ["HF_TOKEN"] = hf_token

            logger.info(f"Loading embedding model: {model_name}")
            if SentenceTransformer is not None:
                self._model = SentenceTransformer(model_name)
            self._similarity_threshold = similarity_threshold
            logger.info(f"Semantic similarity analysis enabled (threshold: {similarity_threshold})")
            return True
        except Exception as e:
            logger.warning(f"Failed to initialize embedding system: {e}")
            return False

    def _extract_code_elements(self, file_path: Path, content: str) -> List[Tuple[str, str, str]]:
        """
        Extract functions and classes with ONLY their docstrings for analysis.

        This focuses on semantic intent rather than implementation details,
        making it much more effective at finding truly redundant code.

        Returns:
            List of (element_type, name, docstring_content) tuples
        """
        elements = []
        lines = content.splitlines()

        i = 0
        while i < len(lines):
            line = lines[i].strip()

            # Check for function or class definition
            if line.startswith("def ") or line.startswith("class "):
                element_type = "function" if line.startswith("def ") else "class"
                name = (
                    line.split("(")[0]
                    .split(":")[0]
                    .replace("def ", "")
                    .replace("class ", "")
                    .strip()
                )

                # Look for docstring only - skip implementation entirely
                i += 1
                docstring_content = ""

                # Skip until we find the first non-empty, non-comment line
                while i < len(lines):
                    current_line = lines[i].strip()
                    if current_line and not current_line.startswith("#"):
                        break
                    i += 1

                # Check if it's a docstring
                if i < len(lines):
                    current_line = lines[i].strip()
                    if current_line.startswith('"""') or current_line.startswith("'''"):
                        docstring_delimiter = '"""' if current_line.startswith('"""') else "'''"

                        # Handle single-line docstring
                        if current_line.count(docstring_delimiter) >= 2:
                            docstring_content = current_line.strip(docstring_delimiter).strip()
                        else:
                            # Multi-line docstring
                            docstring_lines = []
                            if len(current_line) > 3:  # Content on same line as opening quotes
                                docstring_lines.append(current_line[3:])

                            i += 1
                            while i < len(lines):
                                line_content = lines[i].rstrip()
                                if docstring_delimiter in line_content:
                                    # End of docstring
                                    final_content = line_content.split(docstring_delimiter)[0]
                                    if final_content.strip():
                                        docstring_lines.append(final_content)
                                    break
                                docstring_lines.append(line_content)
                                i += 1

                            docstring_content = "\n".join(docstring_lines).strip()

                # Only include elements that have substantial docstrings
                if (
                    docstring_content and len(docstring_content.strip()) > 20
                ):  # Meaningful docstring
                    elements.append((element_type, name, docstring_content))
                # Elements without docstrings will be caught by DOCSTRING-MISSING rule

            i += 1

        return elements

    def _get_embedding(self, text: str, task_type: str = "clustering") -> Optional[Any]:
        """Generate embedding for text using task-specific prompting."""
        try:
            # Try using the new EmbeddingClient first
            if hasattr(self, "_embedding_client"):
                # Determine content type for specialized routing
                if task_type == "code":
                    embeddings = self._embedding_client.get_code_embeddings([text])
                else:
                    embeddings = self._embedding_client.get_natural_embeddings([text])

                if embeddings:
                    return embeddings[0]

            # Fallback to legacy local model
            if not self._model:
                return None

            # Use EmbeddingGemma task-specific prompts for local model
            if task_type == "similarity":
                prompt = f"task: sentence similarity | query: {text}"
            elif task_type == "clustering":
                prompt = f"task: clustering | query: {text}"
            elif task_type == "code":
                prompt = f"task: code retrieval | query: {text}"
            else:
                prompt = text

            embedding = self._model.encode(prompt, normalize_embeddings=True)
            return embedding
        except Exception as e:
            logger.debug(f"Failed to generate embedding: {e}")
            return None

    def _get_embeddings_batch(
        self, texts: List[str], task_type: str = "clustering"
    ) -> Optional[Any]:
        """Generate embeddings for multiple texts using batch processing for efficiency."""
        if not self._model or not texts:
            return None

        try:
            # Use EmbeddingGemma task-specific prompts for all texts
            prompts = []
            for text in texts:
                if task_type == "similarity":
                    prompt = f"task: sentence similarity | query: {text}"
                elif task_type == "clustering":
                    prompt = f"task: clustering | query: {text}"
                elif task_type == "code":
                    prompt = f"task: code retrieval | query: {text}"
                else:
                    prompt = text
                prompts.append(prompt)

            # Batch encode for efficiency
            embeddings = self._model.encode(prompts, normalize_embeddings=True, batch_size=32)
            return embeddings
        except Exception as e:
            logger.debug(f"Failed to generate batch embeddings: {e}")
            return None

    def _compute_similarity(self, embedding1: Any, embedding2: Any) -> float:
        """Compute cosine similarity between two embeddings."""
        if SENTENCE_TRANSFORMERS_AVAILABLE:
            import numpy as np

            return float(np.dot(embedding1, embedding2))
        else:
            return 0.0

    def validate(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        """
        Perform semantic similarity analysis on the current file.

        Compares functions and classes in this file against previously seen ones
        to detect semantic redundancy.
        """
        if config is None or not self._setup_embedding_model(config):
            return

        # Extract code elements from current file
        elements = self._extract_code_elements(file_path, content)
        if not elements:
            return

        # Batch encode all elements at once for efficiency
        element_contents = [element_content for _, _, element_content in elements]
        embeddings = self._get_embeddings_batch(element_contents, "code")

        if embeddings is None or len(embeddings) != len(elements):
            return

        for (element_type, name, element_content), embedding in zip(elements, embeddings):
            # Check against cached elements of the same type
            cache_key = element_type
            if cache_key not in self._code_cache:
                self._code_cache[cache_key] = []

            # Compare against existing elements
            for cached_file, cached_name, _, cached_embedding in self._code_cache[cache_key]:
                # Skip if same file and same name
                if cached_file == file_path and cached_name == name:
                    continue

                similarity = self._compute_similarity(embedding, cached_embedding)

                threshold = getattr(self, "_similarity_threshold", 0.85)
                if similarity >= threshold:
                    # Create descriptive message
                    if cached_file == file_path:
                        message = f"Similar {element_type}s '{name}' and '{cached_name}' found in same file (similarity: {similarity:.3f})"
                        recommendation = f"Consider consolidating similar {element_type}s or renaming if they serve different purposes"
                    else:
                        relative_cached = cached_file.name
                        message = f"{element_type.title()} '{name}' is very similar to '{cached_name}' in {relative_cached} (similarity: {similarity:.3f})"
                        recommendation = f"Consider consolidating duplicate {element_type}s across files or documenting the differences"

                    yield self.create_finding(
                        message=f"{message}. Recommendation: {recommendation}",
                        file_path=file_path,
                        line=1,  # Could be enhanced to find actual line number
                    )

            # Add current element to cache for future comparisons
            self._code_cache[cache_key].append((file_path, name, element_content, embedding))

    def _cleanup_cache(self):
        """Clean up cache to prevent memory issues."""
        # Keep only the most recent 100 elements per type
        for cache_key in self._code_cache:
            if len(self._code_cache[cache_key]) > 100:
                self._code_cache[cache_key] = self._code_cache[cache_key][-100:]
```

---
### File: src/vibelint/validators/project_wide/__init__.py

```python
"""
Project-wide validators for vibelint.

These validators analyze entire projects and require knowledge
of multiple files to identify issues like:
- Dead code across modules
- API consistency violations
- Architecture pattern violations
- Semantic similarity between modules

vibelint/src/vibelint/validators/project_wide/__init__.py
"""

from pathlib import Path
from typing import Any, Dict, Iterator, List

from ...plugin_system import BaseValidator, Finding


class ProjectWideValidator(BaseValidator):
    """Base class for validators that analyze entire projects."""

    def validate_project(self, project_files: Dict[Path, str], config=None) -> Iterator[Finding]:
        """
        Validate entire project with knowledge of all files.

        Args:
            project_files: Dictionary mapping file paths to their content
            config: Configuration object

        Yields:
            Finding objects for any issues found
        """
        # Default implementation - subclasses should override
        raise NotImplementedError("Project-wide validators must implement validate_project")

    def validate(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        """
        Single-file validate method for project-wide validators.

        Project-wide validators should not be called on individual files.
        This method raises an error to prevent misuse.
        """
        raise NotImplementedError(
            f"{self.__class__.__name__} is a project-wide validator. "
            "Use validate_project() instead of validate()."
        )

    def requires_project_context(self) -> bool:
        """Project-wide validators require full project context."""
        return True


def get_project_wide_validators() -> List[str]:
    """Get list of project-wide validator names."""
    return [
        "DEAD-CODE-FOUND",
        "ARCHITECTURE-INCONSISTENT",
        "ARCHITECTURE-LLM",
        "SEMANTIC-SIMILARITY",
        "FALLBACK-SILENT-FAILURE",
        "API-CONSISTENCY",
        "CODE-SMELLS",
        "MODULE-COHESION",
    ]
```

---
### File: src/vibelint/validators/project_wide/api_consistency.py

```python
"""
API consistency validator for vibelint.

Detects inconsistent API usage patterns, missing required parameters,
and architectural violations that lead to runtime failures.

vibelint/src/vibelint/validators/api_consistency.py
"""

import ast
import logging
from pathlib import Path
from typing import Iterator

from ...plugin_system import BaseValidator, Finding, Severity

logger = logging.getLogger(__name__)

__all__ = ["APIConsistencyValidator"]


def _get_function_name(node: ast.Call) -> str:
    """Extract function name from call node."""
    if isinstance(node.func, ast.Name):
        return node.func.id
    elif isinstance(node.func, ast.Attribute):
        return node.func.attr
    return ""


class APIConsistencyValidator(BaseValidator):
    """Validator for API consistency and usage patterns."""

    rule_id = "API-CONSISTENCY"
    name = "API Consistency Checker"
    description = "Detects inconsistent API usage, missing parameters, and architectural violations"
    default_severity = Severity.WARN

    def __init__(self, severity=None, config=None):
        super().__init__(severity, config)
        # Known API signatures and their requirements
        self.known_apis = {
            "load_config": {
                "required_args": ["start_path"],
                "module": "config",
                "common_mistakes": [
                    "Called without required start_path parameter",
                    "Often needs Path('.') or similar as argument",
                ],
            },
            "create_llm_manager": {
                "required_args": ["config"],
                "module": "llm_manager",
                "common_mistakes": ["Requires config dict with llm section"],
            },
        }

    def validate(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        """Check for API consistency issues."""
        try:
            tree = ast.parse(content)
        except SyntaxError:
            return

        # Check function calls for API misuse
        for node in ast.walk(tree):
            if isinstance(node, ast.Call):
                yield from self._check_function_call(node, file_path)

            elif isinstance(node, ast.Import) or isinstance(node, ast.ImportFrom):
                yield from self._check_import_usage(node, file_path, tree)

    def _check_function_call(self, node: ast.Call, file_path: Path) -> Iterator[Finding]:
        """Check individual function calls for API consistency."""
        func_name = _get_function_name(node)

        if func_name in self.known_apis:
            api_info = self.known_apis[func_name]

            # Check required arguments
            provided_args = len(node.args)
            required_args = len(api_info["required_args"])

            if provided_args < required_args:
                missing_args = api_info["required_args"][provided_args:]
                yield self.create_finding(
                    message=f"API misuse: {func_name}() missing required arguments: {', '.join(missing_args)}",
                    file_path=file_path,
                    line=node.lineno,
                    suggestion=f"Add required arguments: {func_name}({', '.join(api_info['required_args'])})",
                )

    def _check_import_usage(
        self, node: ast.AST, file_path: Path, tree: ast.AST
    ) -> Iterator[Finding]:
        """Check for inconsistent import and usage patterns."""
        if isinstance(node, ast.ImportFrom):
            if node.module == "config" and any(
                alias.name == "load_config" for alias in (node.names or [])
            ):
                # Check if load_config is used correctly in this file
                for call_node in ast.walk(tree):
                    if (
                        isinstance(call_node, ast.Call)
                        and isinstance(call_node.func, ast.Name)
                        and call_node.func.id == "load_config"
                    ):

                        if not call_node.args:
                            yield self.create_finding(
                                message="Configuration anti-pattern: load_config() called without start_path",
                                file_path=file_path,
                                line=call_node.lineno,
                                suggestion="Use load_config(Path('.')) or pass explicit path for config discovery",
                            )


class ConfigurationPatternValidator(BaseValidator):
    """Validator for configuration pattern consistency."""

    rule_id = "CONFIG-PATTERN"
    name = "Configuration Pattern Checker"
    description = "Ensures consistent configuration loading and usage patterns"
    default_severity = Severity.INFO

    def validate(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        """Check for configuration pattern issues."""
        try:
            tree = ast.parse(content)
        except SyntaxError:
            return

        # Track how config is loaded and used
        config_loading_patterns = []
        config_usage_patterns = []

        for node in ast.walk(tree):
            if isinstance(node, ast.Call):
                func_name = _get_function_name(node)

                if func_name == "load_config":
                    config_loading_patterns.append(node.lineno)

                elif "config" in str(node).lower():
                    config_usage_patterns.append(node.lineno)

        # Check for multiple config loading approaches in same file
        if len(config_loading_patterns) > 1:
            yield self.create_finding(
                message="Configuration inconsistency: Multiple config loading patterns detected",
                file_path=file_path,
                line=config_loading_patterns[0],
                suggestion="Consolidate to single source of truth for configuration",
            )

        # Check for config dict creation vs proper loading
        for node in ast.walk(tree):
            if isinstance(node, ast.Dict) and any(
                isinstance(key, ast.Constant) and key.value == "llm" for key in node.keys if key
            ):

                yield self.create_finding(
                    message="Configuration anti-pattern: Manual config dict creation detected",
                    file_path=file_path,
                    line=node.lineno,
                    suggestion="Use load_config() for single source of truth instead of manual dict creation",
                )
```

---
### File: src/vibelint/validators/project_wide/code_smells.py

```python
"""
Code smell detection validator implementing Martin Fowler's taxonomy.

Detects common code smells like long methods, large classes, magic numbers,
and other patterns that indicate design issues.

vibelint/src/vibelint/validators/code_smells.py
"""

import ast
import logging
from pathlib import Path
from typing import Any, Dict, Iterator, Optional

from ...plugin_system import BaseValidator, Finding, Severity

logger = logging.getLogger(__name__)

__all__ = ["CodeSmellValidator"]


class CodeSmellValidator(BaseValidator):
    """Detects common code smells based on Martin Fowler's taxonomy."""

    rule_id = "CODE-SMELLS"
    name = "Code Smell Detector"
    description = "Detects long methods, large classes, magic numbers, and other code smells"
    default_severity = Severity.INFO

    def validate(
        self, file_path: Path, content: str, config: Optional[Dict[str, Any]] = None
    ) -> Iterator[Finding]:
        """Single-pass AST analysis for code smell detection."""
        try:
            tree = ast.parse(content)
        except SyntaxError as e:
            logger.debug(f"Syntax error in {file_path}: {e}")
            return

        # Single AST walk detecting all smell categories
        for node in ast.walk(tree):
            yield from self._check_bloaters(node, file_path)
            yield from self._check_lexical_abusers(node, file_path)
            yield from self._check_couplers(node, file_path)
            yield from self._check_obfuscators(node, file_path)

    def _check_bloaters(self, node: ast.AST, file_path: Path) -> Iterator[Finding]:
        """Detect Bloater code smells: Large Class, Long Method, Long Parameter List."""
        # Long Method (>20 lines suspicious, >50 bad)
        if isinstance(node, ast.FunctionDef):
            method_length = self._count_logical_lines(node)
            if method_length > 50:
                yield self.create_finding(
                    message=f"Method '{node.name}' is too long ({method_length} lines)",
                    file_path=file_path,
                    line=node.lineno,
                    suggestion="Break method into smaller, focused functions",
                )
            elif method_length > 20:
                yield self.create_finding(
                    message=f"Method '{node.name}' is getting long ({method_length} lines)",
                    file_path=file_path,
                    line=node.lineno,
                    suggestion="Consider breaking into smaller functions",
                )

        # Large Class (>500 lines or >20 methods)
        elif isinstance(node, ast.ClassDef):
            class_length = self._count_logical_lines(node)
            method_count = len([n for n in node.body if isinstance(n, ast.FunctionDef)])
            if class_length > 500 or method_count > 20:
                yield self.create_finding(
                    message=f"Class '{node.name}' is too large ({class_length} lines, {method_count} methods)",
                    file_path=file_path,
                    line=node.lineno,
                    suggestion="Split class responsibilities using Single Responsibility Principle",
                )

        # Long Parameter List (>3 suspicious, >5 bad)
        if isinstance(node, ast.FunctionDef):
            param_count = len(node.args.args)
            if param_count > 5:
                yield self.create_finding(
                    message=f"Function '{node.name}' has too many parameters ({param_count})",
                    file_path=file_path,
                    line=node.lineno,
                    suggestion="Consider parameter object or builder pattern",
                )

    def _check_lexical_abusers(self, node: ast.AST, file_path: Path) -> Iterator[Finding]:
        """Detect Lexical Abuser code smells: Magic Numbers, Uncommunicative Names."""
        # Magic Numbers
        if isinstance(node, ast.Constant) and isinstance(node.value, (int, float)):
            if node.value not in [0, 1, -1, 2] and not self._is_in_test_context(node):
                yield self.create_finding(
                    message=f"Magic number '{node.value}' should be named constant",
                    file_path=file_path,
                    line=node.lineno,
                    suggestion=f"Replace with named constant: MEANINGFUL_NAME = {node.value}",
                )

        # Uncommunicative Names
        name_patterns = [
            (ast.FunctionDef, "function", "name"),
            (ast.ClassDef, "class", "name"),
            (ast.arg, "parameter", "arg"),
        ]
        for node_type, context, attr in name_patterns:
            if isinstance(node, node_type):
                if hasattr(node, attr):
                    name = getattr(node, attr)
                    if name and self._is_uncommunicative_name(name):
                        yield self.create_finding(
                            message=f"Uncommunicative {context} name '{name}'",
                            file_path=file_path,
                            line=node.lineno,
                            suggestion=f"Use descriptive name that explains {context} purpose",
                        )

    def _check_couplers(self, node: ast.AST, file_path: Path) -> Iterator[Finding]:
        """Detect Coupler code smells: Message Chain, Feature Envy."""
        # Message Chain (a.b.c.d.method())
        if isinstance(node, ast.Call) and isinstance(node.func, ast.Attribute):
            chain_length = self._count_attribute_chain(node.func)
            if chain_length > 3:
                yield self.create_finding(
                    message=f"Long message chain detected ({chain_length} levels)",
                    file_path=file_path,
                    line=node.lineno,
                    suggestion="Consider introducing intermediate methods to reduce coupling",
                )

    def _check_obfuscators(self, node: ast.AST, file_path: Path) -> Iterator[Finding]:
        """Detect Obfuscator code smells: Complicated Boolean Expression, Clever Code."""
        # Complicated Boolean Expression
        if isinstance(node, ast.BoolOp):
            complexity = self._calculate_boolean_complexity(node)
            if complexity > 4:
                yield self.create_finding(
                    message=f"Complex boolean expression (complexity: {complexity})",
                    file_path=file_path,
                    line=node.lineno,
                    suggestion="Break into intermediate boolean variables with descriptive names",
                )

        # Clever Code (nested comprehensions)
        if isinstance(node, ast.ListComp):
            nesting_level = self._count_comprehension_nesting(node)
            if nesting_level > 2:
                yield self.create_finding(
                    message=f"Overly complex list comprehension (nesting level: {nesting_level})",
                    file_path=file_path,
                    line=node.lineno,
                    suggestion="Break into multiple steps or use traditional loops for clarity",
                )

    # Helper methods
    def _count_logical_lines(self, node: ast.AST) -> int:
        """Count logical lines of code (excluding comments/blank lines)."""
        lines = set()
        for child in ast.walk(node):
            if hasattr(child, "lineno"):
                lines.add(child.lineno)
        return len(lines)

    def _is_uncommunicative_name(self, name: str) -> bool:
        """Check if name is uncommunicative."""
        if len(name) == 1 and name not in ["i", "j", "k", "x", "y", "z"]:
            return True
        if len(name) > 2 and not any(c in "aeiou" for c in name.lower()):
            return True
        return False

    def _count_attribute_chain(self, node: ast.Attribute) -> int:
        """Count depth of attribute chain."""
        count = 1
        current = node.value
        while isinstance(current, ast.Attribute):
            count += 1
            current = current.value
        return count

    def _calculate_boolean_complexity(self, node: ast.BoolOp) -> int:
        """Calculate complexity of boolean expression."""
        complexity = 1
        for value in node.values:
            if isinstance(value, ast.BoolOp):
                complexity += self._calculate_boolean_complexity(value)
            else:
                complexity += 1
        return complexity

    def _count_comprehension_nesting(self, node: ast.ListComp) -> int:
        """Count nesting level of comprehensions."""
        max_nesting = 1
        for generator in node.generators:
            for comp in ast.walk(generator.iter):
                if isinstance(comp, (ast.ListComp, ast.SetComp, ast.DictComp, ast.GeneratorExp)):
                    max_nesting = max(max_nesting, 1 + self._count_comprehension_nesting(comp))
        return max_nesting

    def _is_in_test_context(self, node: ast.AST) -> bool:
        """Check if node is in test context where magic numbers are more acceptable."""
        return False  # Simplified for now
```

---
### File: src/vibelint/validators/project_wide/dead_code.py

```python
"""
Dead code detection validator.

Identifies unused imports, unreferenced functions, duplicate implementations,
and other forms of dead code that can be safely removed.

vibelint/src/vibelint/validators/dead_code.py
"""

import ast
import re
from pathlib import Path
from typing import Dict, Iterator, List, Set

from ...plugin_system import BaseValidator, Finding, Severity
from ...utils import find_files_by_extension, find_project_root

__all__ = ["DeadCodeValidator"]


class DeadCodeValidator(BaseValidator):
    """Detects various forms of dead code."""

    rule_id = "DEAD-CODE-FOUND"
    name = "Dead Code Detector"
    description = "Identifies unused imports, unreferenced functions, and other dead code"
    default_severity = Severity.WARN

    def __init__(self, severity=None, config=None):
        super().__init__(severity, config)
        self._project_files_cache = None
        self._all_exports_cache = None

    def validate(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        """Analyze file for dead code patterns."""
        try:
            tree = ast.parse(content)
        except SyntaxError:
            return

        # Build project context for dynamic analysis
        project_root = find_project_root(file_path) or file_path.parent

        # Analyze the AST for various dead code patterns
        yield from self._check_unused_imports_dynamic(file_path, tree, content, project_root)
        yield from self._check_unreferenced_definitions_dynamic(file_path, tree, project_root)
        yield from self._check_duplicate_patterns(file_path, content)
        yield from self._check_legacy_patterns(file_path, content)

    def _get_project_files(self, project_root: Path) -> List[Path]:
        """Get all Python files in the project."""
        if self._project_files_cache is None:
            exclude_patterns = ["*/__pycache__/*", "*/.pytest_cache/*", "*/build/*", "*/dist/*"]
            self._project_files_cache = find_files_by_extension(
                project_root, extension=".py", exclude_globs=exclude_patterns
            )
        return self._project_files_cache

    def _get_all_exports(self, project_root: Path) -> Dict[str, Set[str]]:
        """Get all __all__ exports across the project."""
        if self._all_exports_cache is None:
            self._all_exports_cache = {}
            for py_file in self._get_project_files(project_root):
                try:
                    content = py_file.read_text(encoding="utf-8")
                    tree = ast.parse(content)
                    exports = self._extract_all_exports(tree)
                    if exports:
                        self._all_exports_cache[str(py_file)] = exports
                except (UnicodeDecodeError, SyntaxError):
                    continue
        return self._all_exports_cache

    def _extract_all_exports(self, tree: ast.AST) -> Set[str]:
        """Extract names from __all__ assignments."""
        exports = set()
        for node in ast.walk(tree):
            if isinstance(node, ast.Assign):
                for target in node.targets:
                    if isinstance(target, ast.Name) and target.id == "__all__":
                        if isinstance(node.value, (ast.List, ast.Tuple)):
                            for elt in node.value.elts:
                                if isinstance(elt, ast.Constant) and isinstance(elt.value, str):
                                    exports.add(elt.value)
        return exports

    def _scan_string_references(self, content: str, name: str) -> bool:
        """Check if name is referenced in strings (getattr, importlib, etc.)."""
        patterns = [
            rf"getattr\([^,]+,\s*['\"]({re.escape(name)})['\"]",
            rf"hasattr\([^,]+,\s*['\"]({re.escape(name)})['\"]",
            rf"importlib\.import_module\(['\"].*{re.escape(name)}.*['\"]",
            rf"__import__\(['\"].*{re.escape(name)}.*['\"]",
            rf"['\"]({re.escape(name)})['\"]",
        ]
        return any(re.search(pattern, content) for pattern in patterns)

    def _is_used_in_tests(self, name: str, project_root: Path) -> bool:
        """Check if name is used in test files."""
        test_files = [f for f in self._get_project_files(project_root) if "test" in str(f).lower()]
        for test_file in test_files:
            try:
                content = test_file.read_text(encoding="utf-8")
                if name in content:
                    return True
            except UnicodeDecodeError:
                continue
        return False

    def _check_unused_imports_dynamic(
        self, file_path: Path, tree: ast.AST, content: str, project_root: Path
    ) -> Iterator[Finding]:
        """Check for imported names that are never used with dynamic analysis."""
        imported_names: Dict[str, int] = {}  # name -> line number
        used_names: Set[str] = set()

        # Collect all imports
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    name = alias.asname if alias.asname else alias.name
                    imported_names[name] = node.lineno
            elif isinstance(node, ast.ImportFrom):
                for alias in node.names:
                    if alias.name == "*":
                        continue  # Skip wildcard imports
                    name = alias.asname if alias.asname else alias.name
                    imported_names[name] = node.lineno

        # Collect all used names
        for node in ast.walk(tree):
            if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Load):
                used_names.add(node.id)
            elif isinstance(node, ast.Attribute):
                # For attribute access like `os.path`, record `os` as used
                if isinstance(node.value, ast.Name):
                    used_names.add(node.value.id)

        # Get __all__ exports for this file
        all_exports = self._extract_all_exports(tree)

        # Find unused imports with dynamic checks
        for name, line_num in imported_names.items():
            if name not in used_names:
                # Dynamic analysis checks
                is_exported = name in all_exports
                is_string_referenced = self._scan_string_references(content, name)
                is_test_used = self._is_used_in_tests(name, project_root)

                # Skip if used dynamically
                if is_exported or is_string_referenced or is_test_used:
                    continue

                yield self.create_finding(
                    message=f"Imported '{name}' is never used",
                    file_path=file_path,
                    line=line_num,
                    suggestion=f"Remove unused import: {name}",
                )

    def _check_unreferenced_definitions_dynamic(
        self, file_path: Path, tree: ast.AST, project_root: Path
    ) -> Iterator[Finding]:
        """Check for functions/classes that are defined but never referenced with dynamic analysis."""
        # Skip this check for __init__.py files and test files
        if file_path.name == "__init__.py" or "test" in file_path.name.lower():
            return

        defined_names: Dict[str, int] = {}  # name -> line number
        referenced_names: Set[str] = set()

        # Collect all function and class definitions
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):
                # Skip private/dunder methods and main blocks
                if not node.name.startswith("_") and node.name != "main":
                    defined_names[node.name] = node.lineno

        # Collect all references
        for node in ast.walk(tree):
            if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Load):
                referenced_names.add(node.id)
            elif isinstance(node, ast.Attribute):
                referenced_names.add(node.attr)

        # Get __all__ exports for this file
        all_exports = self._extract_all_exports(tree)

        # Check cross-file usage
        def _is_used_in_other_files(name: str) -> bool:
            module_name = self._get_module_name(file_path, project_root)
            if not module_name:
                return False

            for py_file in self._get_project_files(project_root):
                if py_file == file_path:
                    continue
                try:
                    content = py_file.read_text(encoding="utf-8")
                    # Check for direct imports
                    if f"from {module_name} import" in content and name in content:
                        return True
                    # Check for module imports
                    if f"import {module_name}" in content and f"{module_name}.{name}" in content:
                        return True
                except UnicodeDecodeError:
                    continue
            return False

        # Find unreferenced definitions with dynamic checks
        for name, line_num in defined_names.items():
            if name not in referenced_names:
                # Dynamic analysis checks
                is_exported = name in all_exports
                is_string_referenced = self._scan_string_references(
                    file_path.read_text(encoding="utf-8"), name
                )
                is_test_used = self._is_used_in_tests(name, project_root)
                is_cross_file_used = _is_used_in_other_files(name)

                # Skip if used dynamically
                if is_exported or is_string_referenced or is_test_used or is_cross_file_used:
                    continue

                yield self.create_finding(
                    message=f"Function/class '{name}' is defined but never referenced",
                    file_path=file_path,
                    line=line_num,
                    suggestion="Consider removing unused definition or adding to __all__",
                )

    def _get_module_name(self, file_path: Path, project_root: Path) -> str:
        """Convert file path to Python module name."""
        try:
            rel_path = file_path.relative_to(project_root)
            if rel_path.name == "__init__.py":
                module_parts = rel_path.parent.parts
            else:
                module_parts = rel_path.with_suffix("").parts
            return ".".join(module_parts)
        except ValueError:
            return ""

    def _check_duplicate_patterns(self, file_path: Path, content: str) -> Iterator[Finding]:
        """Check for duplicate code patterns that suggest redundancy."""
        lines = content.splitlines()

        # Check for duplicate validation result classes
        validation_classes = []
        for line_num, line in enumerate(lines, 1):
            if "ValidationResult" in line and "class " in line:
                validation_classes.append((line_num, line.strip()))

        if len(validation_classes) > 1:
            for line_num, _ in validation_classes:
                yield self.create_finding(
                    message="Validation result class found - may be duplicating plugin system",
                    file_path=file_path,
                    line=line_num,
                    suggestion="Consider using plugin system's Finding class instead",
                )

        # Check for duplicate validation functions
        validation_functions = []
        for line_num, line in enumerate(lines, 1):
            if line.strip().startswith("def validate_") and not line.strip().startswith(
                "def validate("
            ):
                validation_functions.append((line_num, line.strip()))

        if len(validation_functions) > 0:
            for line_num, _ in validation_functions:
                yield self.create_finding(
                    message="Legacy validation function found - may duplicate BaseValidator",
                    file_path=file_path,
                    line=line_num,
                    suggestion="Consider migrating to BaseValidator plugin system",
                )

    def _check_legacy_patterns(self, file_path: Path, content: str) -> Iterator[Finding]:
        """Check for legacy code patterns that might be dead."""
        lines = content.splitlines()

        for line_num, line in enumerate(lines, 1):
            stripped = line.strip()

            # Legacy pattern detection removed - no legacy patterns exist to detect

            # Check for manual console instantiation (except in utils.py which creates the shared instance)
            if "= Console()" in stripped and not file_path.name == "utils.py":
                yield self.create_finding(
                    message="Manual Console instantiation - use shared utils instead",
                    file_path=file_path,
                    line=line_num,
                    suggestion="Replace with: from .utils import console",
                )
```

---
### File: src/vibelint/validators/project_wide/module_cohesion.py

```python
"""
Module cohesion validator for detecting scattered related modules.

Identifies when related modules should be grouped together in subpackages
based on naming patterns, import relationships, and functional cohesion.

vibelint/src/vibelint/validators/module_cohesion.py
"""

import ast
import logging
from collections import defaultdict
from pathlib import Path
from typing import Dict, Iterator, List, Set

from ...plugin_system import BaseValidator, Finding, Severity
from ...utils import find_project_root

logger = logging.getLogger(__name__)

__all__ = ["ModuleCohesionValidator"]


class ModuleCohesionValidator(BaseValidator):
    """Detects module organization issues and unjustified file existence."""

    rule_id = "MODULE-COHESION"
    name = "Module Cohesion & File Justification Analyzer"
    description = (
        "Identifies scattered related modules and files without clear purpose justification"
    )
    default_severity = Severity.INFO

    def __init__(self, severity=None, config=None):
        super().__init__(severity, config)
        # Common patterns that suggest related modules
        self.cohesion_patterns = [
            # Prefixed modules (llm_, api_, db_, etc.)
            r"^([a-z]+)_[a-z_]+\.py$",
            # Service/handler patterns
            r"^([a-z]+)_(service|handler|manager|client|adapter)\.py$",
            # Model/schema patterns
            r"^([a-z]+)_(model|schema|entity|dto)\.py$",
            # Test patterns
            r"^test_([a-z]+)_.*\.py$",
            # Utils patterns
            r"^([a-z]+)_(utils|helpers|tools)\.py$",
        ]

    def validate(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        """Analyze project structure for module cohesion issues."""
        # Only analyze from project root to avoid duplicates
        project_root = find_project_root(file_path) or file_path.parent
        if file_path.parent != project_root / "src" / "vibelint":
            return

        # Get all Python files in the project
        src_dir = project_root / "src" / "vibelint"
        python_files = list(src_dir.glob("*.py"))

        if len(python_files) < 3:  # Need multiple files to detect patterns
            return

        # Group files by naming patterns
        pattern_groups = self._group_files_by_patterns(python_files)

        # Check for scattered modules that should be grouped
        for pattern, files in pattern_groups.items():
            if len(files) >= 2:  # 2+ files with same prefix suggest a module group
                yield from self._suggest_module_grouping(pattern, files, src_dir)

        # Check for functional cohesion based on imports
        yield from self._analyze_import_cohesion(python_files, src_dir)

        # Check for unjustified files
        yield from self._check_file_justification(project_root)

    def _group_files_by_patterns(self, files: List[Path]) -> Dict[str, List[Path]]:
        """Group files by common naming patterns."""
        import re

        groups = defaultdict(list)

        for file_path in files:
            filename = file_path.name

            # Skip special files
            if filename in ["__init__.py", "cli.py", "main.py"]:
                continue

            # Check each pattern
            for pattern in self.cohesion_patterns:
                match = re.match(pattern, filename)
                if match:
                    prefix = match.group(1)
                    groups[prefix].append(file_path)
                    break

        return groups

    def _suggest_module_grouping(
        self, prefix: str, files: List[Path], src_dir: Path
    ) -> Iterator[Finding]:
        """Suggest grouping related files into a submodule."""
        if len(files) < 2:
            return

        # Check if they're already in a submodule
        if any(len(f.relative_to(src_dir).parts) > 1 for f in files):
            return  # Already organized

        file_names = [f.name for f in files]

        yield Finding(
            rule_id=self.rule_id,
            message=f"Related modules with '{prefix}_' prefix should be grouped: {', '.join(file_names)}",
            file_path=files[0],  # Report on first file
            line=1,
            severity=self.default_severity,
            suggestion=f"Create 'src/vibelint/{prefix}/' subpackage and move related modules:\n"
            f"  mkdir src/vibelint/{prefix}/\n"
            f"  mv {' '.join(file_names)} src/vibelint/{prefix}/\n"
            f"  # Rename files to remove prefix: {prefix}_manager.py -> manager.py",
        )

    def _analyze_import_cohesion(self, files: List[Path], src_dir: Path) -> Iterator[Finding]:
        """Analyze import patterns to suggest functional grouping."""
        # Build import graph
        import_graph = defaultdict(set)

        for file_path in files:
            try:
                content = file_path.read_text(encoding="utf-8")
                tree = ast.parse(content)

                imports = self._extract_local_imports(tree, src_dir)
                module_name = file_path.stem
                import_graph[module_name].update(imports)

            except (UnicodeDecodeError, SyntaxError):
                continue

        # Find tightly coupled modules (import each other frequently)
        coupled_groups = self._find_coupled_modules(import_graph)

        for group in coupled_groups:
            if len(group) >= 3:  # Suggest grouping for 3+ tightly coupled modules
                yield Finding(
                    rule_id=self.rule_id,
                    message=f"Tightly coupled modules should be grouped: {', '.join(group)}",
                    file_path=src_dir / f"{list(group)[0]}.py",
                    line=1,
                    severity=Severity.INFO,
                    suggestion="Consider grouping these functionally related modules into a subpackage",
                )

    def _extract_local_imports(self, tree: ast.AST, src_dir: Path) -> Set[str]:
        """Extract imports that reference local modules."""
        local_imports = set()

        for node in ast.walk(tree):
            if isinstance(node, ast.ImportFrom):
                if node.module and node.module.startswith("vibelint"):
                    # Extract module name
                    parts = node.module.split(".")
                    if len(parts) >= 2:
                        local_imports.add(parts[-1])

            elif isinstance(node, ast.Import):
                for alias in node.names:
                    if alias.name.startswith("vibelint"):
                        parts = alias.name.split(".")
                        if len(parts) >= 2:
                            local_imports.add(parts[-1])

        return local_imports

    def _find_coupled_modules(self, import_graph: Dict[str, Set[str]]) -> List[Set[str]]:
        """Find groups of modules that frequently import each other."""
        # Simple clustering based on mutual imports
        coupled_groups = []
        processed = set()

        for module, imports in import_graph.items():
            if module in processed:
                continue

            # Find modules that import this one and vice versa
            mutual_imports = set()
            for imported in imports:
                if imported in import_graph and module in import_graph[imported]:
                    mutual_imports.add(imported)

            if mutual_imports:
                group = {module} | mutual_imports
                coupled_groups.append(group)
                processed.update(group)

        return coupled_groups

    def _check_file_justification(self, project_root: Path) -> Iterator[Finding]:
        """Check that every file has clear justification for existence."""
        # Files that are automatically justified
        auto_justified = {
            "pyproject.toml",
            "setup.py",
            "setup.cfg",
            "requirements.txt",
            "requirements-dev.txt",
            "LICENSE",
            "LICENSE.txt",
            "LICENSE.md",
            "README.md",
            "README.rst",
            "CHANGELOG.md",
            "CONTRIBUTING.md",
            "CODE_OF_CONDUCT.md",
            ".gitignore",
            ".gitattributes",
            "Dockerfile",
            "docker-compose.yml",
            "Makefile",
            "tox.ini",
            ".pre-commit-config.yaml",
            "__init__.py",
            "conftest.py",
            "pytest.ini",
        }

        # Check all files in project
        for file_path in project_root.rglob("*"):
            if file_path.is_dir() or file_path.name.startswith("."):
                continue

            # Skip auto-justified files
            if file_path.name in auto_justified:
                continue

            # Skip files in build/cache directories
            if any(
                part
                in [
                    ".git",
                    "__pycache__",
                    ".pytest_cache",
                    "build",
                    "dist",
                    ".tox",
                    ".mypy_cache",
                    "node_modules",
                ]
                for part in file_path.parts
            ):
                continue

            # Check file justification based on type
            if file_path.suffix == ".py":
                yield from self._check_python_file_justification(file_path)
            elif file_path.suffix in [".md", ".rst", ".txt"]:
                yield from self._check_documentation_justification(file_path)
            elif file_path.suffix in [".json", ".yaml", ".yml", ".toml", ".ini", ".cfg"]:
                yield from self._check_config_file_justification(file_path)
            elif file_path.suffix in [".sh", ".bat", ".ps1"]:
                yield from self._check_script_justification(file_path)
            else:
                # Unknown file type - requires explicit justification
                yield Finding(
                    rule_id=self.rule_id,
                    message=f"Unknown file type without clear justification: {file_path.name}",
                    file_path=file_path,
                    line=1,
                    severity=Severity.INFO,
                    suggestion="Add comment/documentation explaining file purpose or remove if unnecessary",
                )

    def _check_python_file_justification(self, file_path: Path) -> Iterator[Finding]:
        """Check that Python files have module docstrings."""
        try:
            content = file_path.read_text(encoding="utf-8")
            tree = ast.parse(content)

            # Check for module docstring
            has_module_docstring = False
            if (
                tree.body
                and isinstance(tree.body[0], ast.Expr)
                and isinstance(tree.body[0].value, ast.Constant)
            ):
                if (
                    isinstance(tree.body[0].value.value, str)
                    and len(tree.body[0].value.value.strip()) > 10
                ):
                    has_module_docstring = True

            if not has_module_docstring:
                yield Finding(
                    rule_id=self.rule_id,
                    message=f"Python file lacks module docstring explaining its purpose: {file_path.name}",
                    file_path=file_path,
                    line=1,
                    severity=Severity.WARN,
                    suggestion='Add module docstring: """Brief description of module purpose."""',
                )

        except (UnicodeDecodeError, SyntaxError):
            yield Finding(
                rule_id=self.rule_id,
                message=f"Python file has syntax errors or encoding issues: {file_path.name}",
                file_path=file_path,
                line=1,
                severity=Severity.WARN,
                suggestion="Fix syntax errors or encoding issues, or remove if unused",
            )

    def _check_documentation_justification(self, file_path: Path) -> Iterator[Finding]:
        """Check that documentation files have meaningful content."""
        try:
            content = file_path.read_text(encoding="utf-8").strip()

            # Check for minimal content
            if len(content) < 50:  # Very short docs
                yield Finding(
                    rule_id=self.rule_id,
                    message=f"Documentation file is too short to be useful: {file_path.name}",
                    file_path=file_path,
                    line=1,
                    severity=Severity.INFO,
                    suggestion="Add meaningful content or remove if unnecessary",
                )

            # Check for placeholder/template content
            placeholder_indicators = ["TODO", "FIXME", "PLACEHOLDER", "Lorem ipsum", "Example text"]
            if any(indicator in content for indicator in placeholder_indicators):
                yield Finding(
                    rule_id=self.rule_id,
                    message=f"Documentation file contains placeholder content: {file_path.name}",
                    file_path=file_path,
                    line=1,
                    severity=Severity.INFO,
                    suggestion="Replace placeholder content with actual documentation or remove file",
                )

        except UnicodeDecodeError:
            yield Finding(
                rule_id=self.rule_id,
                message=f"Documentation file has encoding issues: {file_path.name}",
                file_path=file_path,
                line=1,
                severity=Severity.WARN,
                suggestion="Fix encoding issues or remove if unnecessary",
            )

    def _check_config_file_justification(self, file_path: Path) -> Iterator[Finding]:
        """Check that config files have clear purpose."""
        try:
            content = file_path.read_text(encoding="utf-8").strip()

            # Check for empty config files
            if len(content) < 10:
                yield Finding(
                    rule_id=self.rule_id,
                    message=f"Config file is nearly empty: {file_path.name}",
                    file_path=file_path,
                    line=1,
                    severity=Severity.INFO,
                    suggestion="Add configuration content or remove if unnecessary",
                )

            # Check for comments explaining purpose
            has_explanatory_comments = any(
                line.strip().startswith("#") and len(line.strip()) > 10
                for line in content.splitlines()[:5]  # Check first 5 lines
            )

            if not has_explanatory_comments and file_path.suffix in [".json", ".yaml", ".yml"]:
                yield Finding(
                    rule_id=self.rule_id,
                    message=f"Config file lacks explanatory comments: {file_path.name}",
                    file_path=file_path,
                    line=1,
                    severity=Severity.INFO,
                    suggestion="Add comments explaining configuration purpose and usage",
                )

        except UnicodeDecodeError:
            yield Finding(
                rule_id=self.rule_id,
                message=f"Config file has encoding issues: {file_path.name}",
                file_path=file_path,
                line=1,
                severity=Severity.WARN,
                suggestion="Fix encoding issues or remove if unnecessary",
            )

    def _check_script_justification(self, file_path: Path) -> Iterator[Finding]:
        """Check that script files have clear purpose."""
        try:
            content = file_path.read_text(encoding="utf-8")

            # Check for shebang and comments
            lines = content.splitlines()

            # Look for explanatory comments in first 10 lines
            has_explanation = any(
                line.strip().startswith("#") and len(line.strip()) > 20 for line in lines[:10]
            )

            if not has_explanation:
                yield Finding(
                    rule_id=self.rule_id,
                    message=f"Script file lacks explanatory comments: {file_path.name}",
                    file_path=file_path,
                    line=1,
                    severity=Severity.INFO,
                    suggestion="Add comments explaining script purpose, usage, and requirements",
                )

        except UnicodeDecodeError:
            yield Finding(
                rule_id=self.rule_id,
                message=f"Script file has encoding issues: {file_path.name}",
                file_path=file_path,
                line=1,
                severity=Severity.WARN,
                suggestion="Fix encoding issues or remove if unnecessary",
            )
```

---
### File: src/vibelint/validators/project_wide/namespace_collisions.py

```python
"""
Namespace representation & collision detection for Python code.

vibelint/src/vibelint/namespace.py
"""

import ast
import logging
from collections import defaultdict
from pathlib import Path

from ...config import Config
from ...discovery import discover_files
from ...utils import find_project_root, get_relative_path

__all__ = [
    "CollisionType",
    "NamespaceCollision",
    "NamespaceNode",
    "detect_hard_collisions",
    "detect_global_definition_collisions",
    "detect_local_export_collisions",
    "build_namespace_tree",
    "get_namespace_collisions_str",
]

logger = logging.getLogger(__name__)


class CollisionType:
    """
    Enum-like class for collision types.

    vibelint/src/vibelint/namespace.py
    """

    HARD = "hard"
    LOCAL_SOFT = "local_soft"
    GLOBAL_SOFT = "global_soft"


class NamespaceCollision:
    """
    Represents a collision between two or more same-named entities.

    vibelint/src/vibelint/namespace.py
    """

    def __init__(
        self,
        name: str,
        collision_type: str,
        paths: list[Path],
        linenos: list[int | None] | None = None,
    ) -> None:
        """
        Initializes a NamespaceCollision instance.

        Args:
        name: The name of the colliding entity.
        collision_type: The type of collision (HARD, LOCAL_SOFT, GLOBAL_SOFT).
        paths: A list of Path objects for all files involved in the collision.
        linenos: An optional list of line numbers corresponding to each path.

        vibelint/src/vibelint/namespace.py
        """

        if not paths:
            raise ValueError("At least one path must be provided for a collision.")

        self.name = name
        self.collision_type = collision_type

        self.paths = sorted(list(set(paths)), key=str)

        self.linenos = (
            linenos if linenos and len(linenos) == len(self.paths) else [None] * len(self.paths)
        )

        self.path1: Path = self.paths[0]
        self.path2: Path = self.paths[1] if len(self.paths) > 1 else self.paths[0]
        self.lineno1: int | None = self.linenos[0] if self.linenos else None
        self.lineno2: int | None = self.linenos[1] if len(self.linenos) > 1 else self.lineno1

        self.definition_paths: list[Path] = (
            self.paths
            if self.collision_type in [CollisionType.GLOBAL_SOFT, CollisionType.LOCAL_SOFT]
            else []
        )

    def __repr__(self) -> str:
        """
        Provides a detailed string representation for debugging.

        vibelint/src/vibelint/namespace.py
        """

        return (
            f"NamespaceCollision(name='{self.name}', type='{self.collision_type}', "
            f"paths={self.paths}, linenos={self.linenos})"
        )

    def __str__(self) -> str:
        """
        Provides a user-friendly string representation of the collision.

        vibelint/src/vibelint/namespace.py
        """

        proj_root = find_project_root(Path(".").resolve())
        base_path = proj_root if proj_root else Path(".")

        paths_str_list = []
        for i, p in enumerate(self.paths):
            loc = f":{self.linenos[i]}" if self.linenos and self.linenos[i] is not None else ""
            try:
                paths_str_list.append(f"{get_relative_path(p, base_path)}{loc}")
            except ValueError:
                paths_str_list.append(f"{p}{loc}")
        paths_str = ", ".join(paths_str_list)

        if self.collision_type == CollisionType.HARD:
            if len(self.paths) == 2 and self.paths[0] == self.paths[1]:

                line_info = ""
                if self.lineno1 is not None and self.lineno2 is not None:
                    line_info = f" (lines ~{self.lineno1} and ~{self.lineno2})"
                elif self.lineno1 is not None:
                    line_info = f" (line ~{self.lineno1})"

                return (
                    f"{self.collision_type.upper()} Collision: Duplicate definition/import of '{self.name}' in "
                    f"{paths_str_list[0]}{line_info}"
                )
            else:
                return f"{self.collision_type.upper()} Collision: Name '{self.name}' used by conflicting entities in: {paths_str}"
        elif self.collision_type == CollisionType.LOCAL_SOFT:
            return f"{self.collision_type.upper()} Collision: '{self.name}' exported via __all__ in multiple sibling modules: {paths_str}"
        elif self.collision_type == CollisionType.GLOBAL_SOFT:
            return f"{self.collision_type.upper()} Collision: '{self.name}' defined in multiple modules: {paths_str}"
        else:
            return f"Unknown Collision: '{self.name}' involving paths: {paths_str}"


def detect_hard_collisions(
    paths: list[Path],
    config: Config,
) -> list[NamespaceCollision]:
    """
    Detect HARD collisions: member vs. submodule, or duplicate definitions within a file.

    Args:
    paths: List of target paths (files or directories).
    config: The loaded vibelint configuration object.

    Returns:
    A list of detected HARD NamespaceCollision objects.

    vibelint/src/vibelint/namespace.py
    """

    root_node, intra_file_collisions = build_namespace_tree(paths, config)

    inter_file_collisions = root_node.get_hard_collisions()

    all_collisions = intra_file_collisions + inter_file_collisions
    for c in all_collisions:
        c.collision_type = CollisionType.HARD
    return all_collisions


def detect_global_definition_collisions(
    paths: list[Path],
    config: Config,
) -> list[NamespaceCollision]:
    """
    Detect GLOBAL SOFT collisions: the same name defined/assigned at the top level
    in multiple different modules across the project.

    Args:
    paths: List of target paths (files or directories).
    config: The loaded vibelint configuration object.

    Returns:
    A list of detected GLOBAL_SOFT NamespaceCollision objects.

    vibelint/src/vibelint/namespace.py
    """

    root_node, _ = build_namespace_tree(paths, config)

    definition_collisions = root_node.detect_global_definition_collisions()

    return definition_collisions


def detect_local_export_collisions(
    paths: list[Path],
    config: Config,
) -> list[NamespaceCollision]:
    """
    Detect LOCAL SOFT collisions: the same name exported via __all__ by multiple
    sibling modules within the same package.

    Args:
    paths: List of target paths (files or directories).
    config: The loaded vibelint configuration object.

    Returns:
    A list of detected LOCAL_SOFT NamespaceCollision objects.

    vibelint/src/vibelint/namespace.py
    """

    root_node, _ = build_namespace_tree(paths, config)
    collisions: list[NamespaceCollision] = []
    root_node.find_local_export_collisions(collisions)
    return collisions


def get_namespace_collisions_str(
    paths: list[Path],
    config: Config,
    console=None,
) -> str:
    """
    Return a string representation of all collision types for quick debugging.

    Args:
    paths: List of target paths (files or directories).
    config: The loaded vibelint configuration object.
    console: Optional console object (unused).

    Returns:
    A string summarizing all detected collisions.

    vibelint/src/vibelint/namespace.py
    """

    from io import StringIO

    buf = StringIO()

    hard_collisions = detect_hard_collisions(paths, config)
    global_soft_collisions = detect_global_definition_collisions(paths, config)
    local_soft_collisions = detect_local_export_collisions(paths, config)

    proj_root = find_project_root(Path(".").resolve())
    base_path = proj_root if proj_root else Path(".")

    if hard_collisions:
        buf.write("Hard Collisions:\n")
        for c in sorted(hard_collisions, key=lambda x: (x.name, str(x.paths[0]))):
            buf.write(f"- {str(c)}\n")

    if local_soft_collisions:
        buf.write("\nLocal Soft Collisions (__all__):\n")

        grouped = defaultdict(list)
        for c in local_soft_collisions:
            grouped[c.name].extend(c.paths)
        for name, involved_paths in sorted(grouped.items()):
            try:
                paths_str = ", ".join(
                    sorted(str(get_relative_path(p, base_path)) for p in set(involved_paths))
                )
            except ValueError:
                paths_str = ", ".join(sorted(str(p) for p in set(involved_paths)))
            buf.write(f"- '{name}': exported by {paths_str}\n")

    if global_soft_collisions:
        buf.write("\nGlobal Soft Collisions (Definitions):\n")

        grouped = defaultdict(list)
        for c in global_soft_collisions:
            grouped[c.name].extend(c.paths)
        for name, involved_paths in sorted(grouped.items()):
            try:
                paths_str = ", ".join(
                    sorted(str(get_relative_path(p, base_path)) for p in set(involved_paths))
                )
            except ValueError:
                paths_str = ", ".join(sorted(str(p) for p in set(involved_paths)))
            buf.write(f"- '{name}': defined in {paths_str}\n")

    return buf.getvalue()


class NamespaceNode:
    """
    A node in the "module" hierarchy (like package/subpackage, or file-level).
    Holds child nodes and top-level members (functions/classes).

    vibelint/src/vibelint/namespace.py
    """

    def __init__(self, name: str, path: Path | None = None, is_package: bool = False) -> None:
        """
        Initializes a NamespaceNode.

        Args:
        name: The name of the node (e.g., module name, package name).
        path: The filesystem path associated with this node (optional).
        is_package: True if this node represents a package (directory).

        vibelint/src/vibelint/namespace.py
        """

        self.name = name
        self.path = path
        self.is_package = is_package
        self.children: dict[str, NamespaceNode] = {}

        self.members: dict[str, tuple[Path, int | None]] = {}

        self.member_collisions: list[NamespaceCollision] = []

        self.exported_names: list[str] | None = None

    def set_exported_names(self, names: list[str]) -> None:
        """
        Sets the list of names found in __all__.

        vibelint/src/vibelint/namespace.py
        """

        self.exported_names = names

    def add_child(self, name: str, path: Path, is_package: bool = False) -> "NamespaceNode":
        """
        Adds a child node, creating if necessary.

        vibelint/src/vibelint/namespace.py
        """

        if name not in self.children:
            self.children[name] = NamespaceNode(name, path, is_package)

        elif path:

            if not (self.children[name].is_package and not is_package):
                self.children[name].path = path
            self.children[name].is_package = is_package or self.children[name].is_package
        return self.children[name]

    def get_hard_collisions(self) -> list[NamespaceCollision]:
        """
        Detect HARD collisions recursively: members vs. child modules.

        vibelint/src/vibelint/namespace.py
        """

        collisions: list[NamespaceCollision] = []

        member_names_with_info = {}
        if self.is_package and self.path:
            init_path = (self.path / "__init__.py").resolve()
            member_names_with_info = {
                name: (def_path, lineno)
                for name, (def_path, lineno) in self.members.items()
                if def_path.resolve() == init_path
            }

        child_names = set(self.children.keys())
        common_names = set(member_names_with_info.keys()).intersection(child_names)

        for name in common_names:

            member_def_path, member_lineno = member_names_with_info.get(name, (None, None))
            cnode = self.children[name]
            child_path = cnode.path

            if member_def_path and child_path:

                collisions.append(
                    NamespaceCollision(
                        name=name,
                        collision_type=CollisionType.HARD,
                        paths=[member_def_path, child_path],
                        linenos=[member_lineno, None],
                    )
                )

        for cnode in self.children.values():
            collisions.extend(cnode.get_hard_collisions())
        return collisions

    def collect_defined_members(self, all_dict: dict[str, list[tuple[Path, int | None]]]) -> None:
        """
        Recursively collects defined members (path, lineno) for global definition collision check.

        vibelint/src/vibelint/namespace.py
        """

        if self.path and self.members:

            for mname, (mpath, mlineno) in self.members.items():
                all_dict.setdefault(mname, []).append((mpath, mlineno))

        for cnode in self.children.values():
            cnode.collect_defined_members(all_dict)

    def detect_global_definition_collisions(self) -> list[NamespaceCollision]:
        """
        Detects GLOBAL SOFT collisions across the whole tree starting from this node.

        vibelint/src/vibelint/namespace.py
        """

        all_defined_members: dict[str, list[tuple[Path, int | None]]] = defaultdict(list)
        self.collect_defined_members(all_defined_members)

        collisions: list[NamespaceCollision] = []
        for name, path_lineno_list in all_defined_members.items():

            unique_paths_map: dict[Path, int | None] = {}
            for path, lineno in path_lineno_list:
                resolved_p = path.resolve()

                if resolved_p not in unique_paths_map:
                    unique_paths_map[resolved_p] = lineno

            if len(unique_paths_map) > 1:

                sorted_paths = sorted(unique_paths_map.keys(), key=str)

                sorted_linenos = [unique_paths_map[p] for p in sorted_paths]

                collisions.append(
                    NamespaceCollision(
                        name=name,
                        collision_type=CollisionType.GLOBAL_SOFT,
                        paths=sorted_paths,
                        linenos=sorted_linenos,
                    )
                )
        return collisions

    def find_local_export_collisions(self, collisions_list: list[NamespaceCollision]) -> None:
        """
        Recursively finds LOCAL SOFT collisions (__all__) within packages.

        Args:
        collisions_list: A list to append found collisions to.

        vibelint/src/vibelint/namespace.py
        """

        if self.is_package:
            exports_in_package: dict[str, list[Path]] = defaultdict(list)

            if self.path and self.path.is_dir() and self.exported_names:

                init_path = (self.path / "__init__.py").resolve()

                if init_path.exists() and any(
                    p.resolve() == init_path for p, _ in self.members.values()
                ):
                    for name in self.exported_names:
                        exports_in_package[name].append(init_path)

            for child in self.children.values():

                if (
                    child.path
                    and child.path.is_file()
                    and not child.is_package
                    and child.name != "__init__"
                    and child.exported_names
                ):
                    for name in child.exported_names:
                        exports_in_package[name].append(child.path.resolve())

            for name, paths in exports_in_package.items():
                unique_paths = sorted(list(set(paths)), key=str)
                if len(unique_paths) > 1:
                    collisions_list.append(
                        NamespaceCollision(
                            name=name,
                            collision_type=CollisionType.LOCAL_SOFT,
                            paths=unique_paths,
                            linenos=[None for _ in unique_paths],
                        )
                    )

        for child in self.children.values():
            if child.is_package:
                child.find_local_export_collisions(collisions_list)

    def __str__(self) -> str:
        """
        Provides a string representation of the node and its subtree, including members.
        Uses a revised formatting approach for better clarity relative to project root.

        vibelint/src/vibelint/namespace.py
        """

        lines = []

        proj_root = find_project_root(Path(".").resolve())
        base_path_for_display = proj_root if proj_root else Path(".")

        def build_tree_lines(
            node: "NamespaceNode", prefix: str = "", base: Path = Path(".")
        ) -> list[str]:
            """
            Docstring for function 'build_tree_lines'.

            vibelint/src/vibelint/namespace.py
            """

            child_items = sorted(node.children.items())

            direct_members = []
            if node.path and node.members:

                expected_def_path = None
                node_path_resolved = node.path.resolve()
                if node.is_package and node_path_resolved.is_dir():
                    expected_def_path = (node_path_resolved / "__init__.py").resolve()
                elif node_path_resolved.is_file():
                    expected_def_path = node_path_resolved

                if expected_def_path:
                    direct_members = sorted(
                        [
                            name
                            for name, (def_path, _) in node.members.items()
                            if def_path.resolve() == expected_def_path
                        ]
                    )

            all_items = child_items + [(name, "member") for name in direct_members]
            total_items = len(all_items)

            for i, (name, item) in enumerate(all_items):
                is_last = i == total_items - 1
                connector = "└── " if is_last else "├── "
                next_level_prefix = prefix + ("    " if is_last else "│   ")

                if item == "member":

                    lines.append(f"{prefix}{connector}{name} (member)")
                else:

                    child: NamespaceNode = item
                    child_path_str = ""
                    indicator = ""
                    if child.path:
                        try:
                            rel_p = get_relative_path(child.path, base)

                            if child.is_package:
                                indicator = " (P)"
                            elif child.name == "__init__":
                                indicator = " (I)"
                            else:
                                indicator = " (M)"
                            child_path_str = f"  [{rel_p}{indicator}]"
                        except ValueError:
                            indicator = (
                                " (P)"
                                if child.is_package
                                else (" (I)" if child.name == "__init__" else " (M)")
                            )
                            child_path_str = f"  [{child.path.resolve()}{indicator}]"
                    else:
                        child_path_str = "  [No Path]"

                    lines.append(f"{prefix}{connector}{name}{child_path_str}")

                    if child.children or (
                        child.members
                        and any(
                            m_path.resolve() == (child.path.resolve() if child.path else None)
                            for _, (m_path, _) in child.members.items()
                        )
                    ):
                        lines.extend(build_tree_lines(child, next_level_prefix, base))

            return lines

        root_path_str = ""
        root_indicator = ""

        if self.path:
            root_path_resolved = self.path.resolve()
            try:

                rel_p = get_relative_path(root_path_resolved, base_path_for_display.parent)

                if rel_p == Path("."):
                    rel_p = Path(self.name)

                root_indicator = (
                    " (P)" if self.is_package else (" (M)" if root_path_resolved.is_file() else "")
                )
                root_path_str = f"  [{rel_p}{root_indicator}]"
            except ValueError:
                root_indicator = (
                    " (P)" if self.is_package else (" (M)" if root_path_resolved.is_file() else "")
                )
                root_path_str = f"  [{root_path_resolved}{root_indicator}]"
        else:
            root_path_str = "  [No Path]"

        lines.append(f"{self.name}{root_path_str}")
        lines.extend(build_tree_lines(self, prefix="", base=base_path_for_display))
        return "\n".join(lines)


def _extract_module_members(
    file_path: Path,
) -> tuple[dict[str, tuple[Path, int | None]], list[NamespaceCollision], list[str] | None]:
    """
    Parses a Python file and extracts top-level member definitions/assignments,
    intra-file hard collisions, and the contents of __all__ if present.

    Returns:
    - A dictionary mapping defined/assigned names to a tuple of (file path, line number).
    - A list of intra-file hard collisions (NamespaceCollision objects).
    - A list of names in __all__, or None if __all__ is not found or invalid.

    vibelint/src/vibelint/namespace.py
    """

    try:
        source = file_path.read_text(encoding="utf-8")

        tree = ast.parse(source, filename=str(file_path))
    except (OSError, UnicodeDecodeError) as e:
        logger.warning(f"Could not read file {file_path} for namespace analysis: {e}")
        return {}, [], None
    except (SyntaxError, ValueError) as e:
        logger.warning(f"Could not parse file {file_path} for namespace analysis: {e}")
        return {}, [], None

    defined_members_map: dict[str, tuple[Path, int | None]] = {}
    collisions: list[NamespaceCollision] = []
    exported_names: list[str] | None = None

    defined_names_nodes: dict[str, ast.AST] = {}

    for node in tree.body:
        current_node = node
        name: str | None = None
        is_definition = False
        is_all_assignment = False
        lineno = getattr(current_node, "lineno", None)

        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
            name = node.name
            is_definition = True
        elif isinstance(node, ast.Assign):

            if (
                len(node.targets) == 1
                and isinstance(node.targets[0], ast.Name)
                and node.targets[0].id == "__all__"
            ):
                is_all_assignment = True

                if isinstance(node.value, (ast.List, ast.Tuple)):
                    exported_names = []
                    for elt in node.value.elts:

                        if isinstance(elt, ast.Constant) and isinstance(elt.value, str):
                            exported_names.append(elt.value)

                if "__all__" not in defined_names_nodes:
                    defined_names_nodes["__all__"] = current_node
                else:
                    first_node = defined_names_nodes["__all__"]
                    collisions.append(
                        NamespaceCollision(
                            name="__all__",
                            collision_type=CollisionType.HARD,
                            paths=[file_path, file_path],
                            linenos=[getattr(first_node, "lineno", None), lineno],
                        )
                    )

            else:

                for target in node.targets:
                    if isinstance(target, ast.Name):
                        name = target.id
                        is_definition = True

                        if name:
                            if name in defined_names_nodes:

                                first_node = defined_names_nodes[name]
                                collisions.append(
                                    NamespaceCollision(
                                        name=name,
                                        collision_type=CollisionType.HARD,
                                        paths=[file_path, file_path],
                                        linenos=[
                                            getattr(first_node, "lineno", None),
                                            lineno,
                                        ],
                                    )
                                )
                            else:

                                defined_names_nodes[name] = current_node
                                defined_members_map[name] = (
                                    file_path,
                                    lineno,
                                )
                            name = None

        if name and is_definition and not is_all_assignment:
            if name in defined_names_nodes:

                first_node = defined_names_nodes[name]
                collisions.append(
                    NamespaceCollision(
                        name=name,
                        collision_type=CollisionType.HARD,
                        paths=[file_path, file_path],
                        linenos=[getattr(first_node, "lineno", None), lineno],
                    )
                )
            else:

                defined_names_nodes[name] = current_node
                defined_members_map[name] = (file_path, lineno)

    return defined_members_map, collisions, exported_names


def build_namespace_tree(
    paths: list[Path], config: Config
) -> tuple[NamespaceNode, list[NamespaceCollision]]:
    """
    Builds the namespace tree, collects intra-file collisions, and stores members/__all__.

    Args:
    paths: List of target paths (files or directories).
    config: The loaded vibelint configuration object.

    Returns a tuple: (root_node, all_intra_file_collisions)

    vibelint/src/vibelint/namespace.py
    """

    project_root_found = config.project_root or find_project_root(
        paths[0].resolve() if paths else Path(".")
    )
    if not project_root_found:

        project_root_found = Path(".")
        root_node_name = "root"
        logger.warning(
            "Could not determine project root. Using '.' as root for namespace analysis."
        )
    else:
        root_node_name = project_root_found.name

    root = NamespaceNode(root_node_name, path=project_root_found.resolve(), is_package=True)
    root_path_for_rel = project_root_found.resolve()
    all_intra_file_collisions: list[NamespaceCollision] = []

    python_files = [
        f
        for f in discover_files(
            paths,
            config,
        )
        if f.suffix == ".py"
    ]

    if not python_files:
        logger.info("No Python files found for namespace analysis based on configuration.")
        return root, all_intra_file_collisions

    for f in python_files:
        try:

            rel_path = f.relative_to(root_path_for_rel)
            rel_parts = list(rel_path.parts)
        except ValueError:

            rel_parts = [f.name]
            logger.warning(
                f"File {f} is outside the determined project root {root_path_for_rel}. Adding directly under root."
            )

        current = root

        for i, part in enumerate(rel_parts[:-1]):

            dir_path = root_path_for_rel.joinpath(*rel_parts[: i + 1])
            current = current.add_child(part, dir_path, is_package=True)

        file_name = rel_parts[-1]
        mod_name = Path(file_name).stem
        file_abs_path = f

        members, intra_collisions, exported_names = _extract_module_members(file_abs_path)
        all_intra_file_collisions.extend(intra_collisions)

        if mod_name == "__init__":

            package_node = current
            package_node.is_package = True
            package_node.path = file_abs_path.parent

            for m_name, m_info in members.items():
                if m_name not in package_node.members:
                    package_node.members[m_name] = m_info

            if exported_names is not None:
                package_node.set_exported_names(exported_names)

        else:

            module_node = current.add_child(mod_name, file_abs_path, is_package=False)
            module_node.members = members
            if exported_names is not None:
                module_node.set_exported_names(exported_names)
            module_node.member_collisions.extend(intra_collisions)

    return root, all_intra_file_collisions
```

---
### File: src/vibelint/validators/project_wide/namespace_report.py

```python
"""
Report generation functionality for vibelint.

vibelint/src/vibelint/validators/namespace_report.py
"""

import logging
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import TextIO

from ...utils import get_relative_path
from ..config import Config
from .namespace_collisions import NamespaceCollision, NamespaceNode

__all__ = ["write_report_content"]
logger = logging.getLogger(__name__)


def _get_files_in_namespace_order(
    node: NamespaceNode, collected_files: set[Path], project_root: Path
) -> None:
    """
    Recursively collects file paths from the namespace tree in DFS order,
    including __init__.py files for packages. Populates the collected_files set.

    Args:
        node: The current NamespaceNode.
        collected_files: A set to store the absolute paths of collected files.
        project_root: The project root path for checking containment.

    vibelint/src/vibelint/validators/namespace_report.py
    """

    if node.is_package and node.path and node.path.is_dir():
        try:

            node.path.relative_to(project_root)
            init_file = node.path / "__init__.py"

            if init_file.is_file() and init_file not in collected_files:

                init_file.relative_to(project_root)
                logger.debug(f"Report: Adding package init file: {init_file}")
                collected_files.add(init_file)
        except ValueError:
            logger.warning(f"Report: Skipping package node outside project root: {node.path}")
        except (OSError, TypeError) as e:
            logger.error(f"Report: Error checking package init file for {node.path}: {e}")

    for child_name in sorted(node.children.keys()):
        child_node = node.children[child_name]

        if child_node.path and child_node.path.is_file() and not child_node.is_package:
            try:

                child_node.path.relative_to(project_root)
                if child_node.path not in collected_files:
                    logger.debug(f"Report: Adding module file: {child_node.path}")
                    collected_files.add(child_node.path)
            except ValueError:
                logger.warning(
                    f"Report: Skipping module file outside project root: {child_node.path}"
                )
            except (OSError, TypeError) as e:
                logger.error(f"Report: Error checking module file {child_node.path}: {e}")

        _get_files_in_namespace_order(child_node, collected_files, project_root)

    if not node.children and node.path and node.path.is_file():
        try:
            node.path.relative_to(project_root)
            if node.path not in collected_files:
                logger.debug(f"Report: Adding root file node: {node.path}")
                collected_files.add(node.path)
        except ValueError:
            logger.warning(f"Report: Skipping root file node outside project root: {node.path}")
        except (OSError, TypeError) as e:
            logger.error(f"Report: Error checking root file node {node.path}: {e}")


def write_report_content(
    f: TextIO,
    project_root: Path,
    target_paths: list[Path],
    findings: list,  # List of Finding objects from plugin system
    hard_coll: list[NamespaceCollision],
    soft_coll: list[NamespaceCollision],
    root_node: NamespaceNode,
    config: Config,
) -> None:
    """
    Writes the comprehensive markdown report content to the given file handle.

    Args:
    f: The text file handle to write the report to.
    project_root: The root directory of the project.
    target_paths: List of paths that were analyzed.
    findings: List of Finding objects from the plugin validation phase.
    hard_coll: List of hard NamespaceCollision objects.
    soft_coll: List of definition/export (soft) NamespaceCollision objects.
    root_node: The root NamespaceNode of the project structure.
    config: Configuration object.

    vibelint/src/vibelint/validators/namespace_report.py
    """

    package_name = project_root.name if project_root else "Unknown"

    f.write("# vibelint Report\n\n")
    f.write(f"*Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n\n")
    f.write(f"**Project:** {package_name}\n")

    f.write(f"**Project Root:** `{str(project_root.resolve())}`\n\n")
    f.write(f"**Paths analyzed:** {', '.join(str(p) for p in target_paths)}\n\n")

    f.write("## Table of Contents\n\n")
    f.write("1. [Summary](#summary)\n")
    f.write("2. [Linting Results](#linting-results)\n")
    f.write("3. [Namespace Structure](#namespace-structure)\n")
    f.write("4. [Namespace Collisions](#namespace-collisions)\n")
    f.write("5. [File Contents](#file-contents)\n\n")

    f.write("## Summary\n\n")
    f.write("| Metric | Count |\n")
    f.write("|--------|-------|\n")

    # Get unique files from findings
    analyzed_files = set(f.file_path for f in findings)
    files_analyzed_count = len(analyzed_files)
    f.write(f"| Files analyzed | {files_analyzed_count} |\n")

    # Count findings by severity
    from .plugin_system import Severity

    error_findings = [f for f in findings if f.severity == Severity.BLOCK]
    warn_findings = [f for f in findings if f.severity == Severity.WARN]

    f.write(f"| Findings with errors | {len(error_findings)} |\n")
    f.write(f"| Findings with warnings | {len(warn_findings)} |\n")
    f.write(f"| Hard namespace collisions | {len(hard_coll)} |\n")
    total_soft_collisions = len(soft_coll)
    f.write(f"| Definition/Export namespace collisions | {total_soft_collisions} |\n\n")

    f.write("## Linting Results\n\n")

    if not findings:
        f.write("*No validation issues found.*\n\n")
    else:
        # Group findings by file for better reporting
        files_with_findings = defaultdict(list)
        for finding in findings:
            files_with_findings[finding.file_path].append(finding)

        f.write("| File | Rule | Severity | Message |\n")
        f.write("|------|------|----------|---------|\n")

        for file_path in sorted(files_with_findings.keys(), key=str):
            file_findings = files_with_findings[file_path]
            try:
                rel_path = get_relative_path(file_path.resolve(), project_root.resolve())
            except ValueError:
                rel_path = file_path

            for finding in sorted(file_findings, key=lambda f: f.line):
                location = f":{finding.line}" if finding.line > 0 else ""
                f.write(
                    f"| `{rel_path}{location}` | `{finding.rule_id}` | {finding.severity.value} | {finding.message} |\n"
                )
        f.write("\n")

    f.write("## Namespace Structure\n\n")
    f.write("```\n")
    try:

        tree_str = root_node.__str__()
        f.write(tree_str)
    except (ValueError, TypeError) as e:
        logger.error(f"Report: Error generating namespace tree string: {e}")
        f.write(f"[Error generating namespace tree: {e}]\n")
    f.write("\n```\n\n")

    f.write("## Namespace Collisions\n\n")
    f.write("### Hard Collisions\n\n")
    if not hard_coll:
        f.write("*No hard collisions detected.*\n\n")
    else:
        f.write("These collisions can break Python imports or indicate duplicate definitions:\n\n")
        f.write("| Name | Path 1 | Path 2 | Details |\n")
        f.write("|------|--------|--------|---------|\n")
        for collision in sorted(hard_coll, key=lambda c: (c.name, str(c.path1))):
            try:
                p1_rel = (
                    get_relative_path(collision.path1.resolve(), project_root.resolve())
                    if collision.path1
                    else "N/A"
                )
                p2_rel = (
                    get_relative_path(collision.path2.resolve(), project_root.resolve())
                    if collision.path2
                    else "N/A"
                )
            except ValueError:
                p1_rel = collision.path1 or "N/A"
                p2_rel = collision.path2 or "N/A"
            loc1 = f":{collision.lineno1}" if collision.lineno1 else ""
            loc2 = f":{collision.lineno2}" if collision.lineno2 else ""
            details = (
                "Intra-file duplicate" if str(p1_rel) == str(p2_rel) else "Module/Member clash"
            )
            f.write(f"| `{collision.name}` | `{p1_rel}{loc1}` | `{p2_rel}{loc2}` | {details} |\n")
        f.write("\n")

    f.write("### Definition & Export Collisions (Soft)\n\n")
    if not soft_coll:
        f.write("*No definition or export collisions detected.*\n\n")
    else:
        f.write(
            "These names are defined/exported in multiple files, which may confuse humans and LLMs:\n\n"
        )
        f.write("| Name | Type | Files Involved |\n")
        f.write("|------|------|----------------|\n")
        grouped_soft = defaultdict(lambda: {"paths": set(), "types": set()})
        for collision in soft_coll:
            all_paths = collision.definition_paths or [collision.path1, collision.path2]
            grouped_soft[collision.name]["paths"].update(p for p in all_paths if p)
            grouped_soft[collision.name]["types"].add(collision.collision_type)

        for name, data in sorted(grouped_soft.items()):
            paths_str_list = []
            for p in sorted(list(data["paths"]), key=str):
                try:
                    paths_str_list.append(
                        f"`{get_relative_path(p.resolve(), project_root.resolve())}`"
                    )
                except ValueError:
                    paths_str_list.append(f"`{p}`")
            type_str = (
                " & ".join(sorted([t.replace("_soft", "").upper() for t in data["types"]]))
                or "Unknown"
            )
            f.write(f"| `{name}` | {type_str} | {', '.join(paths_str_list)} |\n")
        f.write("\n")

    f.write("## File Contents\n\n")
    f.write("Files are ordered alphabetically by path.\n\n")

    collected_files_set: set[Path] = set()
    try:
        _get_files_in_namespace_order(root_node, collected_files_set, project_root.resolve())

        python_files_abs = sorted(list(collected_files_set), key=lambda p: str(p))
        logger.info(f"Report: Found {len(python_files_abs)} files for content section.")
    except (ValueError, TypeError, OSError) as e:
        logger.error(f"Report: Error collecting files for content section: {e}", exc_info=True)
        python_files_abs = []

    if not python_files_abs:
        f.write("*No Python files found in the namespace tree to display.*\n\n")
    else:
        for abs_file_path in python_files_abs:

            if abs_file_path and abs_file_path.is_file():
                try:

                    rel_path = get_relative_path(abs_file_path, project_root.resolve())
                    f.write(f"### {rel_path}\n\n")

                    try:
                        lang = "python"
                        content = abs_file_path.read_text(encoding="utf-8", errors="ignore")
                        f.write(f"```{lang}\n")
                        f.write(content)

                        if not content.endswith("\n"):
                            f.write("\n")
                        f.write("```\n\n")
                    except (OSError, UnicodeDecodeError) as read_e:
                        logger.warning(
                            f"Report: Error reading file content for {rel_path}: {read_e}"
                        )
                        f.write(f"*Error reading file content: {read_e}*\n\n")

                except ValueError:

                    logger.warning(
                        f"Report: Skipping file outside project root in content section: {abs_file_path}"
                    )
                    f.write(f"### {abs_file_path} (Outside Project Root)\n\n")
                    f.write("*Skipping content as file is outside the detected project root.*\n\n")
                except (OSError, ValueError, TypeError) as e_outer:
                    logger.error(
                        f"Report: Error processing file entry for {abs_file_path}: {e_outer}",
                        exc_info=True,
                    )
                    f.write(f"### Error Processing Entry for {abs_file_path}\n\n")
                    f.write(f"*An unexpected error occurred: {e_outer}*\n\n")
            elif abs_file_path:
                logger.warning(
                    f"Report: Skipping non-file path found during content writing: {abs_file_path}"
                )
                f.write(f"### {abs_file_path} (Not a File)\n\n")
                f.write("*Skipping entry as it is not a file.*\n\n")

            f.write("---\n\n")
```

---
### File: src/vibelint/validators/registry.py

```python
"""
Validator registry and discovery system.

Provides centralized registration and discovery of validators with
automatic loading from entry points and modular organization.

Responsibility: Validator discovery and registration only.
Validation logic belongs in individual validator modules.

vibelint/src/vibelint/validators/registry.py
"""

import importlib.metadata
import logging
from typing import Dict, List, Optional, Type

from ..plugin_system import BaseValidator

logger = logging.getLogger(__name__)


class ValidatorRegistry:
    """Registry for managing available validators."""

    def __init__(self):
        self._validators: Dict[str, Type[BaseValidator]] = {}
        self._loaded = False

    def register_validator(self, validator_class: Type[BaseValidator]) -> None:
        """Register a validator class."""
        if not issubclass(validator_class, BaseValidator):
            raise ValueError(f"Validator {validator_class} must inherit from BaseValidator")

        rule_id = validator_class.rule_id
        if rule_id in self._validators:
            logger.warning(f"Overriding existing validator: {rule_id}")

        self._validators[rule_id] = validator_class
        logger.debug(f"Registered validator: {rule_id}")

    def get_validator(self, rule_id: str) -> Optional[Type[BaseValidator]]:
        """Get a validator by rule ID."""
        if not self._loaded:
            self._load_all_validators()
        return self._validators.get(rule_id)

    def get_all_validators(self) -> Dict[str, Type[BaseValidator]]:
        """Get all registered validators."""
        if not self._loaded:
            self._load_all_validators()
        return self._validators.copy()

    def get_validators_by_category(self, category: str) -> Dict[str, Type[BaseValidator]]:
        """Get validators by category (single_file, project_wide, architecture)."""
        if not self._loaded:
            self._load_all_validators()

        filtered = {}
        for rule_id, validator_class in self._validators.items():
            # Determine category from module path
            module_path = validator_class.__module__
            if f".{category}." in module_path:
                filtered[rule_id] = validator_class
        return filtered

    def list_rule_ids(self) -> List[str]:
        """List all available rule IDs."""
        if not self._loaded:
            self._load_all_validators()
        return list(self._validators.keys())

    def _load_all_validators(self) -> None:
        """Load all validators from entry points and built-ins."""
        if self._loaded:
            return

        # Load from entry points
        self._load_entry_point_validators()

        # Load built-in validators
        self._load_builtin_validators()

        self._loaded = True
        logger.info(f"Loaded {len(self._validators)} validators")

    def _load_entry_point_validators(self) -> None:
        """Load validators from entry points."""
        try:
            for entry_point in importlib.metadata.entry_points(group="vibelint.validators"):
                try:
                    validator_class = entry_point.load()
                    self.register_validator(validator_class)
                except Exception as e:
                    logger.warning(f"Failed to load validator {entry_point.name}: {e}")
        except Exception as e:
            logger.error(f"Failed to load entry point validators: {e}")

    def _load_builtin_validators(self) -> None:
        """Load built-in validators from modules."""
        builtin_modules = [
            # Single-file validators
            "vibelint.validators.single_file.docstring",
            "vibelint.validators.single_file.emoji",
            "vibelint.validators.single_file.exports",
            "vibelint.validators.single_file.logger_names",
            "vibelint.validators.single_file.print_statements",
            "vibelint.validators.single_file.typing_quality",
            "vibelint.validators.single_file.self_validation",
            "vibelint.validators.single_file.line_count",
            # Project-wide validators
            "vibelint.validators.project_wide.dead_code",
            "vibelint.validators.project_wide.namespace_collisions",
            "vibelint.validators.project_wide.code_smells",
            "vibelint.validators.project_wide.module_cohesion",
            # Architecture validators
            "vibelint.validators.architecture.basic_patterns",
        ]

        for module_name in builtin_modules:
            try:
                module = importlib.import_module(module_name)

                # Look for get_validators function
                if hasattr(module, "get_validators"):
                    validators = module.get_validators()
                    for validator_class in validators:
                        self.register_validator(validator_class)

                # Look for individual validator classes
                for attr_name in dir(module):
                    attr = getattr(module, attr_name)
                    if (
                        isinstance(attr, type)
                        and issubclass(attr, BaseValidator)
                        and attr != BaseValidator
                        and hasattr(attr, "rule_id")
                    ):
                        self.register_validator(attr)

            except ImportError as e:
                logger.debug(f"Could not import builtin validator module {module_name}: {e}")
            except Exception as e:
                logger.warning(f"Error loading validators from {module_name}: {e}")


# Global registry instance
validator_registry = ValidatorRegistry()


# Convenience functions
def register_validator(validator_class: Type[BaseValidator]) -> None:
    """Register a validator class with the global registry."""
    validator_registry.register_validator(validator_class)


def get_validator(rule_id: str) -> Optional[Type[BaseValidator]]:
    """Get a validator by rule ID from the global registry."""
    return validator_registry.get_validator(rule_id)


def get_all_validators() -> Dict[str, Type[BaseValidator]]:
    """Get all validators from the global registry."""
    return validator_registry.get_all_validators()
```

---
### File: src/vibelint/validators/single_file/__init__.py

```python
"""
Single-file validators for vibelint.

These validators analyze individual Python files in isolation.
They should not require knowledge of other files in the project.

vibelint/src/vibelint/validators/single_file/__init__.py
"""

from pathlib import Path
from typing import Iterator, List

from ...plugin_system import BaseValidator, Finding


class SingleFileValidator(BaseValidator):
    """Base class for validators that analyze individual files."""

    def validate_file(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        """
        Validate a single file in isolation.

        Args:
            file_path: Path to the file being validated
            content: File content as string
            config: Configuration object

        Yields:
            Finding objects for any issues found
        """
        # Default implementation delegates to validate method
        yield from self.validate(file_path, content, config)

    def requires_project_context(self) -> bool:
        """Single-file validators do not require project context."""
        return False


def get_single_file_validators() -> List[str]:
    """Get list of single-file validator names."""
    return [
        "DOCSTRING-MISSING",
        "DOCSTRING-PATH-REFERENCE",
        "PRINT-STATEMENT",
        "EMOJI-IN-STRING",
        "TYPING-POOR-PRACTICE",
        "EXPORTS-MISSING-ALL",
        "EXPORTS-MISSING-ALL-INIT",
    ]
```

---
### File: src/vibelint/validators/single_file/absolute_imports.py

```python
"""
Absolute import enforcer validator.

Detects and can automatically fix relative imports to use absolute imports instead.
This prevents import spaghetti and circular dependency issues.

vibelint/src/vibelint/validators/single_file/absolute_imports.py
"""

import re
from pathlib import Path
from typing import Iterator

from vibelint.plugin_system import BaseValidator, Finding, Severity

__all__ = ["AbsoluteImportValidator"]


class AbsoluteImportValidator(BaseValidator):
    """Enforces absolute imports over relative imports."""

    rule_id = "ABSOLUTE-IMPORTS"
    name = "Absolute Import Enforcer"
    description = "Enforces absolute imports to prevent import spaghetti and circular dependencies"
    default_severity = Severity.WARN

    def validate(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        """Check for relative imports that should be absolute."""

        # Get the module path relative to the vibelint source
        try:
            # Find the vibelint source root
            vibelint_root = self._find_vibelint_root(file_path)
            if not vibelint_root:
                # Not in a vibelint project, skip validation
                return

            # Calculate module path
            relative_path = file_path.relative_to(vibelint_root)
            module_parts = list(relative_path.parts[:-1])  # Remove .py file

        except (ValueError, AttributeError):
            # Can't determine module structure, skip
            return

        lines = content.splitlines()
        for line_num, line in enumerate(lines, 1):
            # Match relative imports
            rel_import_match = re.match(r'^(\s*)from (\.+)([^.\s]*) import (.+)$', line.strip())
            if rel_import_match:
                _, dots, module_name, imports = rel_import_match.groups()

                # Calculate what the absolute import should be
                absolute_import = self._calculate_absolute_import(
                    module_parts, dots, module_name
                )

                if absolute_import:
                    yield self.create_finding(
                        message=f"Relative import detected: {line.strip()}",
                        file_path=file_path,
                        line=line_num,
                        suggestion=f"Replace with: from {absolute_import} import {imports}",
                    )

    def can_fix(self, finding: "Finding") -> bool:
        """Check if this finding can be automatically fixed."""
        return finding.rule_id == self.rule_id

    def apply_fix(self, content: str, finding: "Finding") -> str:
        """Automatically convert relative import to absolute import."""
        lines = content.splitlines(True)  # Keep line endings
        if finding.line <= len(lines):
            line = lines[finding.line - 1]

            # Extract the suggestion from the finding
            suggestion = finding.suggestion or ""
            if "Replace with:" in suggestion:
                replacement = suggestion.split("Replace with:", 1)[1].strip()

                # Get the indentation from the original line
                original_stripped = line.lstrip()
                indentation = line[:len(line) - len(original_stripped)]

                # Apply the replacement with original indentation
                lines[finding.line - 1] = indentation + replacement + '\n'

        return "".join(lines)

    def _find_vibelint_root(self, file_path: Path) -> Path:
        """Find the vibelint source root directory."""
        current = file_path

        # Walk up until we find the vibelint package root
        while current.parent != current:
            if current.name == "vibelint" and (current / "__init__.py").exists():
                return current
            # Also check if we're in a src/vibelint structure
            if (current / "src" / "vibelint" / "__init__.py").exists():
                return current / "src" / "vibelint"
            current = current.parent
        return None

    def _calculate_absolute_import(self, module_parts: list, dots: str, module_name: str) -> str:
        """Calculate the absolute import path."""
        levels_up = len(dots) - 1  # Number of parent directories to go up

        if levels_up == 0:
            # Same directory: from .module import something
            if module_name:
                absolute_module = "vibelint." + ".".join(module_parts + [module_name])
            else:
                # from . import something (importing from __init__.py)
                absolute_module = "vibelint." + ".".join(module_parts)
        else:
            # Parent directories: from ..parent.module import something
            if len(module_parts) >= levels_up:
                base_parts = module_parts[:-levels_up] if levels_up > 0 else module_parts
                if module_name:
                    absolute_module = "vibelint." + ".".join(base_parts + [module_name])
                else:
                    absolute_module = "vibelint." + ".".join(base_parts)
            else:
                # Too many levels up, can't determine
                return None

        return absolute_module
```

---
### File: src/vibelint/validators/single_file/docstring.py

```python
"""
Docstring validator using BaseValidator plugin system.

Checks for missing docstrings and proper path references in modules,
classes, and functions.

vibelint/src/vibelint/validators/docstring.py
"""

import ast
from pathlib import Path
from typing import Iterator

from ...plugin_system import BaseValidator, Finding, Severity

__all__ = ["MissingDocstringValidator", "DocstringPathValidator"]


class MissingDocstringValidator(BaseValidator):
    """Validator for missing docstrings."""

    rule_id = "DOCSTRING-MISSING"
    name = "Missing Docstring Checker"
    description = "Checks for missing docstrings in modules, classes, and functions"
    default_severity = Severity.INFO

    def validate(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        """Check for missing docstrings based on configuration."""
        try:
            tree = ast.parse(content)
        except SyntaxError:
            return

        # Get docstring configuration
        docstring_config = (config and config.get("docstring", {})) or {}
        require_module = docstring_config.get("require_module_docstrings", True)
        require_class = docstring_config.get("require_class_docstrings", True)
        require_function = docstring_config.get("require_function_docstrings", False)
        include_private = docstring_config.get("include_private_functions", False)

        # Check module docstring
        if require_module and not ast.get_docstring(tree):
            yield self.create_finding(
                message="Module is missing docstring",
                file_path=file_path,
                line=1,
                suggestion="Add a module-level docstring explaining the module's purpose",
            )

        # Check classes and functions
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                # Check class docstrings
                if require_class and (include_private or not node.name.startswith("_")):
                    if not ast.get_docstring(node):
                        yield self.create_finding(
                            message=f"Class '{node.name}' is missing docstring",
                            file_path=file_path,
                            line=node.lineno,
                            suggestion=f"Add docstring to {node.name} explaining its purpose",
                        )
            elif isinstance(node, ast.FunctionDef):
                # Check function docstrings
                if require_function and (include_private or not node.name.startswith("_")):
                    if not ast.get_docstring(node):
                        yield self.create_finding(
                            message=f"Function '{node.name}' is missing docstring",
                            file_path=file_path,
                            line=node.lineno,
                            suggestion=f"Add docstring to {node.name}() explaining its purpose",
                        )


class DocstringPathValidator(BaseValidator):
    """Validator for missing path references in docstrings."""

    rule_id = "DOCSTRING-PATH-REFERENCE"
    name = "Missing Path Reference in Docstring"
    description = "Checks that docstrings end with the expected relative file path reference"
    default_severity = Severity.INFO

    def validate(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        """Check for missing path references in docstrings based on configuration."""
        # Get docstring configuration
        docstring_config = (config and config.get("docstring", {})) or {}
        require_path_references = docstring_config.get("require_path_references", False)

        # Skip validation if path references are not required
        if not require_path_references:
            return

        try:
            tree = ast.parse(content)
        except SyntaxError:
            return

        # Get expected path reference based on format configuration
        path_format = docstring_config.get("path_reference_format", "relative")
        expected_path = self._get_expected_path(file_path, path_format)

        # Check module docstring
        module_docstring = ast.get_docstring(tree)
        if module_docstring and not module_docstring.strip().endswith(expected_path):
            yield self.create_finding(
                message=f"Module docstring missing/incorrect path reference (expected '{expected_path}')",
                file_path=file_path,
                line=1,
                suggestion=f"Add '{expected_path}' at the end of the module docstring for LLM context",
            )

        # Check function and class docstrings
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
                docstring = ast.get_docstring(node)
                if docstring and not docstring.strip().endswith(expected_path):
                    node_type = "Class" if isinstance(node, ast.ClassDef) else "Function"
                    yield self.create_finding(
                        message=f"{node_type} '{node.name}' docstring missing/incorrect path reference (expected '{expected_path}')",
                        file_path=file_path,
                        line=node.lineno,
                        suggestion=f"Add '{expected_path}' at the end of the docstring for LLM context",
                    )

    def _get_expected_path(self, file_path: Path, path_format: str) -> str:
        """Get expected path reference based on format configuration."""
        if path_format == "absolute":
            return str(file_path)
        elif path_format == "module_path":
            # Convert to Python module path (e.g., vibelint.validators.docstring)
            parts = file_path.parts
            if "src" in parts:
                src_idx = parts.index("src")
                module_parts = parts[src_idx + 1 :]
            else:
                module_parts = parts

            # Remove .py extension and convert to module path
            if module_parts and module_parts[-1].endswith(".py"):
                module_parts = module_parts[:-1] + (module_parts[-1][:-3],)

            return ".".join(module_parts)
        else:  # relative format (default)
            # Get relative path, removing project root and src/ prefix
            relative_path = str(file_path)
            try:
                # Try to find project root by looking for common markers
                current = file_path.parent
                while current.parent != current:
                    if any(
                        (current / marker).exists()
                        for marker in ["pyproject.toml", "setup.py", ".git"]
                    ):
                        relative_path = str(file_path.relative_to(current))
                        break
                    current = current.parent
            except ValueError:
                pass

            # Remove src/ prefix if present
            if relative_path.startswith("src/"):
                relative_path = relative_path[4:]

            return relative_path
```

---
### File: src/vibelint/validators/single_file/emoji.py

```python
"""
Emoji usage validator using BaseValidator plugin system.

Detects emoji usage that can cause encoding issues, reduce readability,
and create compatibility problems across different terminals and systems.

vibelint/src/vibelint/validators/emoji.py
"""

import re
from pathlib import Path
from typing import Iterator

from ...plugin_system import BaseValidator, Finding, Severity

__all__ = ["EmojiUsageValidator"]


class EmojiUsageValidator(BaseValidator):
    """Detects emoji usage that can cause encoding issues."""

    rule_id = "EMOJI-IN-STRING"
    name = "Emoji Usage Detector"
    description = "Detects emojis that can cause MCP and Windows shell issues"
    default_severity = Severity.WARN

    def validate(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        """Check for emoji usage in code."""
        # Comprehensive emoji regex pattern - matches ALL possible emojis
        emoji_pattern = re.compile(
            r"[\U0001F600-\U0001F64F"  # Emoticons
            r"\U0001F300-\U0001F5FF"  # Misc Symbols and Pictographs
            r"\U0001F680-\U0001F6FF"  # Transport and Map Symbols
            r"\U0001F1E0-\U0001F1FF"  # Regional Indicator Symbols
            r"\U0001F700-\U0001F77F"  # Alchemical Symbols
            r"\U0001F780-\U0001F7FF"  # Geometric Shapes Extended
            r"\U0001F800-\U0001F8FF"  # Supplemental Arrows-C
            r"\U0001F900-\U0001F9FF"  # Supplemental Symbols and Pictographs
            r"\U0001FA00-\U0001FA6F"  # Chess Symbols
            r"\U0001FA70-\U0001FAFF"  # Symbols and Pictographs Extended-A
            r"\U00002600-\U000026FF"  # Miscellaneous Symbols
            r"\U00002700-\U000027BF"  # Dingbats
            r"\U0000FE00-\U0000FE0F"  # Variation Selectors
            r"\U0001F018-\U0001F270"  # Various Asian characters
            r"\U0000238C-\U00002454"  # Misc technical (fixed with leading zeros)
            r"\U000020D0-\U000020FF"  # Combining Diacritical Marks for Symbols (fixed)
            r"]+"
        )

        lines = content.splitlines()
        for line_num, line in enumerate(lines, 1):
            emoji_matches = emoji_pattern.findall(line)
            if emoji_matches:
                emojis_found = "".join(emoji_matches)

                # Check if emoji is in code vs strings/comments
                if self._is_emoji_in_code_context(line):
                    # More severe for emojis in actual code
                    yield self.create_finding(
                        message=f"Emoji in code: {emojis_found}",
                        file_path=file_path,
                        line=line_num,
                        suggestion="Replace emoji in code with ASCII alternatives immediately",
                    )
                else:
                    # Less severe for emojis in strings/comments
                    yield self.create_finding(
                        message=f"Emoji usage detected: {emojis_found}",
                        file_path=file_path,
                        line=line_num,
                        suggestion="Replace emojis with text descriptions to avoid encoding issues in MCP and Windows shells",
                    )

    def can_fix(self, finding: "Finding") -> bool:
        """Check if this finding can be automatically fixed."""
        return finding.rule_id == self.rule_id

    def apply_fix(self, content: str, finding: "Finding") -> str:
        """Automatically remove emojis from content."""
        lines = content.splitlines(True)  # Keep line endings
        if finding.line <= len(lines):
            line = lines[finding.line - 1]

            # Comprehensive emoji pattern for removal
            emoji_pattern = re.compile(
                r"[\U0001F600-\U0001F64F"  # Emoticons
                r"\U0001F300-\U0001F5FF"  # Misc Symbols and Pictographs
                r"\U0001F680-\U0001F6FF"  # Transport and Map Symbols
                r"\U0001F1E0-\U0001F1FF"  # Regional Indicator Symbols
                r"\U0001F700-\U0001F77F"  # Alchemical Symbols
                r"\U0001F780-\U0001F7FF"  # Geometric Shapes Extended
                r"\U0001F800-\U0001F8FF"  # Supplemental Arrows-C
                r"\U0001F900-\U0001F9FF"  # Supplemental Symbols and Pictographs
                r"\U0001FA00-\U0001FA6F"  # Chess Symbols
                r"\U0001FA70-\U0001FAFF"  # Symbols and Pictographs Extended-A
                r"\U00002600-\U000026FF"  # Miscellaneous Symbols
                r"\U00002700-\U000027BF"  # Dingbats
                r"\U0000FE00-\U0000FE0F"  # Variation Selectors
                r"\U0001F018-\U0001F270"  # Various Asian characters
                r"\U0000238C-\U00002454"  # Misc technical (fixed with leading zeros)
                r"\U000020D0-\U000020FF"  # Combining Diacritical Marks for Symbols (fixed)
                r"]+"
            )

            # Remove emojis from the line
            fixed_line = emoji_pattern.sub("", line)

            # Clean up any double spaces that might result, but preserve indentation and newlines
            # Extract leading whitespace (indentation)
            leading_whitespace = ""
            content_start = 0
            for char in fixed_line:
                if char in [' ', '\t']:
                    leading_whitespace += char
                    content_start += 1
                else:
                    break

            # Split into content and line ending
            if fixed_line.endswith('\n'):
                content = fixed_line[content_start:-1]  # Remove leading whitespace and newline
                line_ending = '\n'
            elif fixed_line.endswith('\r\n'):
                content = fixed_line[content_start:-2]  # Remove leading whitespace and CRLF
                line_ending = '\r\n'
            else:
                content = fixed_line[content_start:]  # Remove leading whitespace
                line_ending = ''

            # Clean up only multiple consecutive spaces in content
            content = re.sub(r" {2,}", " ", content)  # Multiple spaces to single space
            content = content.rstrip()  # Remove trailing spaces only

            # Reconstruct the line with original indentation
            fixed_line = leading_whitespace + content + line_ending

            lines[finding.line - 1] = fixed_line

        return "".join(lines)

    def _is_emoji_in_code_context(self, line: str) -> bool:
        """
        Check if emoji appears to be in code rather than in strings or comments.

        This is a heuristic - emojis in strings/comments are less problematic
        than emojis used as identifiers or in code structure.
        """
        # Remove string literals and comments to see if emoji remains
        line_without_strings = re.sub(r'["\'].*?["\']', "", line)
        line_without_comments = re.sub(r"#.*$", "", line_without_strings)

        # If emoji still exists after removing strings/comments, it's likely in code
        emoji_pattern = re.compile(
            r"[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U00002600-\U000026FF\U00002700-\U000027BF]"
        )
        return bool(emoji_pattern.search(line_without_comments))
```

---
### File: src/vibelint/validators/single_file/exports.py

```python
"""
__all__ exports validator using BaseValidator plugin system.

Checks for presence and correct format of __all__ definitions in Python modules.

vibelint/src/vibelint/validators/exports.py
"""

import ast
from pathlib import Path
from typing import Iterator

from ...plugin_system import BaseValidator, Finding, Severity

__all__ = ["MissingAllValidator"]


class MissingAllValidator(BaseValidator):
    """Validator for missing __all__ definitions."""

    rule_id = "EXPORTS-MISSING-ALL"
    name = "Missing __all__ Checker"
    description = "Checks for missing __all__ definitions in modules"
    default_severity = Severity.INFO

    def validate(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        """Check for missing __all__ definition."""
        # Skip if it's __init__.py or private module
        if file_path.name.startswith("_"):
            return

        try:
            tree = ast.parse(content)
        except SyntaxError:
            yield self.create_finding(
                message="SyntaxError parsing file during __all__ validation",
                file_path=file_path,
                line=1,
                suggestion="Fix Python syntax errors in the file",
            )
            return

        # Look for __all__ definition
        has_all = False
        has_exports = False

        for node in tree.body:
            if isinstance(node, ast.Assign):
                for target in node.targets:
                    if isinstance(target, ast.Name) and target.id == "__all__":
                        has_all = True
                        # Check if __all__ is properly formatted
                        if not self._is_valid_all_format(node.value):
                            yield self.create_finding(
                                message="__all__ is not assigned a list or tuple value",
                                file_path=file_path,
                                line=node.lineno,
                                suggestion='Ensure __all__ = ["item1", "item2"] or __all__ = ("item1", "item2")',
                            )
                        break
            elif isinstance(node, (ast.FunctionDef, ast.ClassDef)) and not node.name.startswith(
                "_"
            ):
                has_exports = True

        if has_exports and not has_all:
            yield self.create_finding(
                message="Module has public functions/classes but no __all__ definition",
                file_path=file_path,
                line=1,
                suggestion="Add __all__ = [...] to explicitly define public API",
            )

    def _is_valid_all_format(self, node: ast.AST) -> bool:
        """Check if __all__ assignment value is a valid list or tuple."""
        if isinstance(node, (ast.List, ast.Tuple)):
            # Check that all elements are strings
            return all(
                isinstance(elt, ast.Constant) and isinstance(elt.value, str) for elt in node.elts
            )
        return False


class InitAllValidator(BaseValidator):
    """Validator for missing __all__ in __init__.py files."""

    rule_id = "EXPORTS-MISSING-ALL-INIT"
    name = "Missing __all__ in __init__.py"
    description = "__init__.py file is missing __all__ definition (optional based on config)"
    default_severity = Severity.INFO

    def validate(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        """Check for missing __all__ in __init__.py files."""
        # Only check __init__.py files
        if file_path.name != "__init__.py":
            return

        try:
            tree = ast.parse(content)
        except SyntaxError:
            return

        # Look for __all__ definition
        has_all = False
        for node in tree.body:
            if isinstance(node, ast.Assign):
                for target in node.targets:
                    if isinstance(target, ast.Name) and target.id == "__all__":
                        has_all = True
                        break

        if not has_all:
            yield self.create_finding(
                message="__init__.py file is missing __all__ definition",
                file_path=file_path,
                line=1,
                suggestion="Add __all__ = [...] to control package imports",
            )
```

---
### File: src/vibelint/validators/single_file/line_count.py

```python
"""
Line count validator for individual files.

Flags files that exceed configured line count thresholds to encourage
module decomposition and maintainability.

vibelint/src/vibelint/validators/single_file/line_count.py
"""

import ast
from pathlib import Path
from typing import Iterator

from ...plugin_system import BaseValidator, Finding, Severity

__all__ = ["LineCountValidator"]


class LineCountValidator(BaseValidator):
    """Validates that files don't exceed reasonable line count limits."""

    rule_id = "FILE-TOO-LONG"
    name = "Line Count Validator"
    description = "File exceeds recommended line count limit"
    default_severity = Severity.WARN

    def __init__(self, severity=None, config=None):
        super().__init__(severity, config)

        # Default thresholds can be overridden in project config
        self.warning_threshold = self.config.get("warning_threshold", 500)
        self.error_threshold = self.config.get("error_threshold", 1000)
        self.exclude_patterns = self.config.get(
            "exclude_patterns", ["test_*.py", "*_test.py", "conftest.py", "__init__.py"]
        )

    def validate(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        """Validate that file doesn't exceed line count thresholds."""

        # Skip excluded files
        filename = file_path.name
        for pattern in self.exclude_patterns:
            if (
                filename == pattern
                or (pattern.startswith("*") and filename.endswith(pattern[1:]))
                or (pattern.endswith("*") and filename.startswith(pattern[:-1]))
            ):
                return

        # Count non-empty lines (excluding pure whitespace)
        lines = content.splitlines()
        non_empty_lines = [line for line in lines if line.strip()]
        line_count = len(non_empty_lines)

        # Check thresholds
        if line_count >= self.error_threshold:
            yield self.create_finding(
                message=f"File has {line_count} lines, exceeding error threshold of {self.error_threshold}",
                file_path=file_path,
                line=1,
                column=1,
                suggestion=self._get_suggestion(file_path, line_count),
            )
        elif line_count >= self.warning_threshold:
            yield self.create_finding(
                message=f"File has {line_count} lines, exceeding warning threshold of {self.warning_threshold}",
                file_path=file_path,
                line=1,
                column=1,
                suggestion=self._get_suggestion(file_path, line_count),
            )

    def _get_suggestion(self, file_path: Path, line_count: int) -> str:
        """Generate context-appropriate refactoring suggestions."""

        # Try to analyze file structure for better suggestions
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                tree = ast.parse(f.read())

            # Count classes and functions
            classes = [node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
            functions = [node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]

            if len(classes) > 1:
                return f"Consider splitting {len(classes)} classes into separate files"
            elif len(functions) > 10:
                return (
                    f"Consider grouping {len(functions)} functions into classes or separate modules"
                )
            elif "cli" in str(file_path).lower():
                return "Consider breaking CLI into command modules (validation, analysis, etc.)"
            elif "config" in str(file_path).lower():
                return "Consider separating config loading, validation, and defaults"
            else:
                return "Consider extracting classes or functions into separate modules"

        except (SyntaxError, UnicodeDecodeError):
            # Fallback for unparseable files
            return f"Consider breaking this {line_count}-line file into smaller, focused modules"


# Entry point registration
def get_validators():
    """Return list of validators provided by this module."""
    return [LineCountValidator]
```

---
### File: src/vibelint/validators/single_file/logger_names.py

```python
"""
Logger name validator using BaseValidator plugin system.

Detects hardcoded logger names that should use __name__ instead
for proper module hierarchy and maintainability.

vibelint/src/vibelint/validators/logger_names.py
"""

import ast
from pathlib import Path
from typing import Iterator

from ...plugin_system import BaseValidator, Finding, Severity

__all__ = ["LoggerNameValidator"]


class LoggerNameValidator(BaseValidator):
    """Validator for detecting hardcoded logger names."""

    rule_id = "LOGGER-NAME"
    name = "Logger Name Checker"
    description = "Detects get_logger() calls with hardcoded strings instead of __name__"
    default_severity = Severity.BLOCK

    def validate(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        """Validate a single file for hardcoded logger names."""
        try:
            tree = ast.parse(content)
        except SyntaxError:
            return

        class LoggerNameVisitor(ast.NodeVisitor):
            """Visitor class that traverses an AST to collect all logger names used in the code."""

            def __init__(self, validator):
                """Initialize logger name visitor."""
                self.validator = validator
                self.logger_names = []
                self.findings = []

            def visit_Call(self, node: ast.Call):
                # Check for get_logger() calls
                if (
                    isinstance(node.func, ast.Name)
                    and node.func.id == "get_logger"
                    and len(node.args) == 1
                ):

                    arg = node.args[0]

                    # If argument is a string literal (not __name__)
                    if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                        self.findings.append(
                            Finding(
                                rule_id=self.validator.rule_id,
                                severity=self.validator.severity,
                                message=f"Use __name__ instead of hardcoded string '{arg.value}' for logger name",
                                file_path=file_path,
                                line=node.lineno,
                                suggestion="get_logger(__name__)",
                            )
                        )

                # Also check for attribute access like logging.getLogger()
                elif (
                    isinstance(node.func, ast.Attribute)
                    and node.func.attr in ["getLogger", "get_logger"]
                    and len(node.args) == 1
                ):

                    arg = node.args[0]

                    # If argument is a string literal (not __name__)
                    if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                        # Skip if this is in logging infrastructure getting specific loggers
                        if "logging.py" in str(file_path) and "kaia." in arg.value:
                            pass  # This is logging infrastructure, skip
                        else:
                            self.findings.append(
                                Finding(
                                    rule_id=self.validator.rule_id,
                                    severity=Severity.WARN,  # Less severe for standard logging
                                    message=f"Consider using __name__ instead of hardcoded string '{arg.value}' for logger name",
                                    file_path=file_path,
                                    line=node.lineno,
                                    suggestion="Use __name__ for consistent logger hierarchy",
                                )
                            )

                self.generic_visit(node)

        visitor = LoggerNameVisitor(self)
        visitor.visit(tree)
        yield from visitor.findings
```

---
### File: src/vibelint/validators/single_file/print_statements.py

```python
"""
Print statement validator using BaseValidator plugin system.

Detects print statements that should be replaced with proper logging
for better maintainability, configurability, and production readiness.

vibelint/src/vibelint/validators/print_statements.py
"""

import ast
import fnmatch
import re
from pathlib import Path
from typing import Iterator

from ...plugin_system import BaseValidator, Finding, Severity

__all__ = ["PrintStatementValidator"]


class PrintStatementValidator(BaseValidator):
    """Validator for detecting print statements."""

    rule_id = "PRINT-STATEMENT"
    name = "Print Statement Checker"
    description = "Detects print() calls that should be replaced with logging"
    default_severity = Severity.WARN

    def validate(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        """Validate print statement usage in a Python file."""
        # Check if file should be excluded based on configuration
        if self._should_exclude_file(file_path):
            return

        try:
            tree = ast.parse(content)
        except SyntaxError:
            return

        visitor = _PrintVisitor()
        visitor.visit(tree)

        # Split content into lines for suppression comment checking
        lines = content.split("\n")

        for line_num, context, print_content in visitor.print_calls:
            # Check for suppression comments on the same line
            if self._has_suppression_comment(lines, line_num):
                continue

            # Check if this looks like legitimate CLI output
            if self._is_legitimate_cli_print(print_content, context, content, line_num):
                continue

            message = (
                f"Print statement found{context}. Replace with logging for better maintainability."
            )
            suggestion = "Use logger.info(), logger.debug(), or logger.error() instead"

            yield self.create_finding(
                message=message, file_path=file_path, line=line_num, suggestion=suggestion
            )

    def _should_exclude_file(self, file_path: Path) -> bool:
        """Check if file should be excluded from print statement validation."""
        # Get exclude patterns from configuration
        print_config = self.config.get("print_validation", {})
        exclude_globs = print_config.get(
            "exclude_globs",
            [
                # Default patterns if no configuration is provided
                "test_*.py",
                "*_test.py",
                "conftest.py",
                "tests/**/*.py",
                "cli.py",
                "main.py",
                "__main__.py",
                "*_cli.py",
                "*_cmd.py",
            ],
        )

        # Check if file matches any exclude pattern
        for pattern in exclude_globs:
            # Check against file name
            if fnmatch.fnmatch(file_path.name, pattern):
                return True

            # Check against relative path pattern
            relative_path = str(file_path).replace("\\", "/")  # Normalize path separators
            if fnmatch.fnmatch(relative_path, pattern):
                return True

            # Check against path from parent directories
            for parent in file_path.parents:
                parent_relative = str(file_path.relative_to(parent)).replace("\\", "/")
                if fnmatch.fnmatch(parent_relative, pattern):
                    return True

        return False

    def _has_suppression_comment(self, lines: list[str], line_num: int) -> bool:
        """Check if the line has a suppression comment for print statements.

        Supports:
        - # vibelint: stdout  - Explicit stdout communication marker
        - # vibelint: ignore  - General vibelint suppression
        - # noqa: print       - Specific print suppression
        - # noqa              - General linting suppression
        """
        # Line numbers in AST are 1-indexed
        if line_num <= 0 or line_num > len(lines):
            return False

        line = lines[line_num - 1]

        # Check for suppression patterns in comments
        suppression_patterns = [
            r"#\s*vibelint:\s*stdout",  # Explicit stdout marker
            r"#\s*vibelint:\s*ignore",  # General vibelint ignore
            r"#\s*noqa:\s*print",  # Specific print suppression
            r"#\s*noqa(?:\s|$)",  # General noqa
            r"#\s*type:\s*ignore",  # Type ignore (sometimes used for prints)
            r"#\s*pragma:\s*no\s*cover",  # Coverage pragma
        ]

        for pattern in suppression_patterns:
            if re.search(pattern, line, re.IGNORECASE):
                return True

        return False

    def _is_legitimate_cli_print(
        self, print_content: str, context: str, file_content: str, line_num: int
    ) -> bool:
        """Check if a print statement appears to be legitimate CLI output."""
        # Patterns that suggest legitimate CLI usage
        cli_indicators = [
            # UI symbols and formatting
            r"[[EMOJI][TIP][EMOJI][ALERT]⭐[SUCCESS][ROCKET]]",  # Emoji indicators for user interface
            r"^[-=]{3,}",  # Headers/separators (----, ====)
            r"^\s*\*{2,}",  # Emphasis markers (***, etc.)
            r"^\s*#{2,}",  # Section headers (##, ###)
            # CLI instruction patterns
            r"(run|execute|visit|go to|open)",
            r"(http://|https://)",  # URLs
            r"(localhost|127\.0\.0\.1)",  # Local server addresses
            r"port\s+\d+",  # Port numbers
            # Status/progress indicators
            r"(starting|completed|finished|ready)",
            r"(success|error|warning|info).*:",
            r"^\s*\[.*\]",  # [INFO], [ERROR], etc.
            # Calibration/setup specific
            r"(calibration|configuration|setup)",
            r"(device|microphone|audio)",
            r"(instruction|step \d+)",
        ]

        # Function names that suggest CLI interface
        cli_function_names = [
            "show_",
            "display_",
            "print_",
            "output_",
            "start_",
            "run_",
            "main",
            "cli",
            "calibrat",
            "setup",
            "config",
            "instruction",
            "help",
            "usage",
        ]

        # Check print content against CLI patterns
        if print_content:
            for pattern in cli_indicators:
                if re.search(pattern, print_content, re.IGNORECASE | re.MULTILINE):
                    return True

        # Check if function name suggests CLI usage
        if context:
            func_name = context.replace(" in function ", "").lower()
            for cli_pattern in cli_function_names:
                if cli_pattern in func_name:
                    return True

        # Check file context - look for CLI-related imports or patterns
        file_lines = file_content.split("\n")

        # Look around the print statement for context clues
        start_line = max(0, line_num - 5)
        end_line = min(len(file_lines), line_num + 3)
        surrounding_context = "\n".join(file_lines[start_line:end_line])

        # Check for CLI-related context around the print
        context_patterns = [
            r"def\s+(show|display|print|output|start|run|main|cli)",
            r"(server|port|url|http)",
            r"(calibration|setup|config)",
            r"(instruction|help|usage)",
            r"input\s*\(",  # User input nearby
            r"argparse",  # Command line arguments
        ]

        for pattern in context_patterns:
            if re.search(pattern, surrounding_context, re.IGNORECASE):
                return True

        return False


class _PrintVisitor(ast.NodeVisitor):
    """AST visitor to detect print statements."""

    def __init__(self):
        self.print_calls = []
        self.current_function = None

    def visit_Call(self, node):
        """Visit Call nodes to detect print() function calls."""
        if isinstance(node.func, ast.Name) and node.func.id == "print":
            context = f" in function {self.current_function}" if self.current_function else ""

            # Extract print content for analysis
            print_content = ""
            if node.args:
                try:
                    # Try to extract string literals from print arguments
                    for arg in node.args:
                        if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                            print_content += arg.value + " "
                        elif isinstance(arg, ast.JoinedStr):  # f-strings
                            for value in arg.values:
                                if isinstance(value, ast.Constant) and isinstance(value.value, str):
                                    print_content += value.value
                except (AttributeError, TypeError):
                    # If we can't parse the content, just use empty string
                    pass

            self.print_calls.append((node.lineno, context, print_content.strip()))
        self.generic_visit(node)

    def visit_FunctionDef(self, node):
        """Visit FunctionDef nodes to track current function context for print detection."""
        old_function = self.current_function
        self.current_function = node.name
        self.generic_visit(node)
        self.current_function = old_function
```

---
### File: src/vibelint/validators/single_file/self_validation.py

```python
"""
Self-validation hooks for vibelint.

This module implements validation hooks that ensure vibelint follows
its own coding standards and architectural principles.

Key principles enforced:
- Single-file validators must not access other files
- Project-wide validators must implement validate_project()
- No emoji in code or comments (project rule)
- Proper validator categorization
- Adherence to killeraiagent project standards

vibelint/src/vibelint/self_validation.py
"""

import logging
import re
from pathlib import Path
from typing import Any, Dict, Iterator, List, Optional

from ...plugin_system import BaseValidator, Finding, Severity

logger = logging.getLogger(__name__)


class SelfValidationHook:
    """
    Hook that validates vibelint's own code against its standards.

    This runs automatically when vibelint analyzes its own codebase
    to ensure we follow our own rules.
    """

    def __init__(self):
        self.project_root = None
        self.violations_found = []

    def should_apply_self_validation(self, file_path: Path) -> bool:
        """Check if self-validation should apply to this file."""
        try:
            # Check if we're analyzing vibelint's own code
            path_str = str(file_path.absolute())
            return (
                "vibelint" in path_str
                and "/src/vibelint/" in path_str
                and file_path.suffix == ".py"
            )
        except Exception:
            return False

    def validate_single_file_validator(self, file_path: Path, content: str) -> Iterator[Finding]:
        """Validate that single-file validators don't violate isolation."""
        if not self._is_validator_file(file_path):
            return

        # Check if this is a single-file validator
        if self._is_single_file_validator(content):
            # Check for project context violations
            violations = self._check_project_context_violations(content)
            for violation in violations:
                yield Finding(
                    file_path=file_path,
                    line=violation["line"],
                    message=f"Single-file validator violates isolation: {violation['issue']}",
                    rule_id="VIBELINT-SINGLE-FILE-ISOLATION",
                    severity=Severity.BLOCK,
                    suggestion="Single-file validators should not access other files or require project context",
                )

    def validate_project_standards(self, file_path: Path, content: str) -> Iterator[Finding]:
        """Validate adherence to killeraiagent project standards."""
        if not self.should_apply_self_validation(file_path):
            return

        # Check for emoji violations (project rule: no emoji)
        emoji_violations = self._check_emoji_violations(content)
        for line_num, line_content in emoji_violations:
            yield Finding(
                file_path=file_path,
                line=line_num,
                message="Code contains emoji characters (violates project standards)",
                rule_id="VIBELINT-NO-EMOJI",
                severity=Severity.BLOCK,
                suggestion="Remove emoji characters from code and comments",
            )

        # Check for proper absolute path usage
        path_violations = self._check_path_violations(content)
        for line_num, issue in path_violations:
            yield Finding(
                file_path=file_path,
                line=line_num,
                message=f"Path usage issue: {issue}",
                rule_id="VIBELINT-ABSOLUTE-PATHS",
                severity=Severity.WARN,
                suggestion="Use absolute paths for file operations",
            )

    def validate_validator_categorization(self, file_path: Path, content: str) -> Iterator[Finding]:
        """Validate that validators are properly categorized."""
        if not self._is_validator_file(file_path):
            return

        # Check if validator is in correct directory
        is_single_file = self._is_single_file_validator(content)
        is_project_wide = self._is_project_wide_validator(content)

        path_str = str(file_path)
        in_single_file_dir = "/single_file/" in path_str
        in_project_wide_dir = "/project_wide/" in path_str
        in_architecture_dir = "/architecture/" in path_str

        if is_single_file and not in_single_file_dir and not in_architecture_dir:
            yield Finding(
                file_path=file_path,
                line=1,
                message="Single-file validator should be in validators/single_file/ directory",
                rule_id="VIBELINT-VALIDATOR-ORGANIZATION",
                severity=Severity.WARN,
                suggestion="Move to validators/single_file/ or implement project-wide validation",
            )

        if is_project_wide and not in_project_wide_dir and not in_architecture_dir:
            yield Finding(
                file_path=file_path,
                line=1,
                message="Project-wide validator should be in validators/project_wide/ directory",
                rule_id="VIBELINT-VALIDATOR-ORGANIZATION",
                severity=Severity.WARN,
                suggestion="Move to validators/project_wide/ or implement single-file validation",
            )

    def _is_validator_file(self, file_path: Path) -> bool:
        """Check if file is a validator."""
        return (
            "/validators/" in str(file_path)
            and file_path.name != "__init__.py"
            and file_path.suffix == ".py"
        )

    def _is_single_file_validator(self, content: str) -> bool:
        """Check if validator is designed for single-file analysis."""
        patterns = [
            r"class\s+\w+Validator.*BaseValidator",
            r"def validate\(self, file_path.*content.*\)",
            r"requires_project_context.*False",
        ]

        project_patterns = [
            r"validate_project",
            r"project_files",
            r"requires_project_context.*True",
        ]

        has_single_file_indicators = any(re.search(pattern, content) for pattern in patterns)
        has_project_indicators = any(re.search(pattern, content) for pattern in project_patterns)

        return has_single_file_indicators and not has_project_indicators

    def _is_project_wide_validator(self, content: str) -> bool:
        """Check if validator is designed for project-wide analysis."""
        patterns = [r"validate_project", r"project_files.*Dict", r"requires_project_context.*True"]
        return any(re.search(pattern, content) for pattern in patterns)

    def requires_project_context(self) -> bool:
        """Self-validation does not require project context."""
        return False

    def _check_project_context_violations(self, content: str) -> List[Dict[str, Any]]:
        """Check for violations of single-file validator isolation."""
        violations = []
        lines = content.split("\n")

        violation_patterns = [
            (r"import.*discovery", "Should not import discovery module"),
            (r"import.*project_map", "Should not import project mapping"),
            (r"glob\.glob", "Should not use glob to find other files"),
            (r"os\.walk", "Should not walk directory tree"),
            (r"Path.*glob", "Should not glob for other files"),
            (r"open\(.*\.py", "Should not open other Python files"),
        ]

        for line_num, line in enumerate(lines, 1):
            line = line.strip()
            if line.startswith("#"):  # Skip comments
                continue

            for pattern, issue in violation_patterns:
                if re.search(pattern, line):
                    violations.append({"line": line_num, "issue": issue, "content": line})

        return violations

    def _check_emoji_violations(self, content: str) -> List[tuple]:
        """Check for emoji characters in code."""
        violations = []
        lines = content.split("\n")

        # Unicode ranges for emoji
        emoji_pattern = re.compile(
            "["
            "\U0001f600-\U0001f64f"  # emoticons
            "\U0001f300-\U0001f5ff"  # symbols & pictographs
            "\U0001f680-\U0001f6ff"  # transport & map symbols
            "\U0001f1e0-\U0001f1ff"  # flags (iOS)
            "\U00002702-\U000027b0"  # dingbats
            "\U000024c2-\U0001f251"  # enclosed characters
            "]+",
            flags=re.UNICODE,
        )

        for line_num, line in enumerate(lines, 1):
            if emoji_pattern.search(line):
                violations.append((line_num, line.strip()))

        return violations

    def _check_path_violations(self, content: str) -> List[tuple]:
        """Check for improper path usage."""
        violations = []
        lines = content.split("\n")

        # Patterns that suggest relative path usage in file operations
        problematic_patterns = [
            (r'open\(["\'][^/]', "Relative path in open()"),
            (r'Path\(["\'][^/]', "Relative path in Path()"),
            (r'glob\(["\'][^/]', "Relative path in glob()"),
        ]

        for line_num, line in enumerate(lines, 1):
            line = line.strip()
            if line.startswith("#"):  # Skip comments
                continue

            for pattern, issue in problematic_patterns:
                if re.search(pattern, line):
                    violations.append((line_num, issue))

        return violations


class VibelintSelfValidator(BaseValidator):
    """
    Validator that applies vibelint's self-validation hooks.

    This ensures vibelint follows its own standards when analyzing
    its own codebase.
    """

    def __init__(self, severity: Severity = Severity.WARN, config: Optional[Dict[str, Any]] = None):
        super().__init__(severity)
        self.hook = SelfValidationHook()
        self.config = config or {}

    @property
    def rule_id(self) -> str:
        return "VIBELINT-SELF-VALIDATION"

    @property
    def name(self) -> str:
        return "Vibelint Self-Validation"

    @property
    def description(self) -> str:
        return "Ensures vibelint follows its own coding standards and architectural principles"

    def validate(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        """Apply all self-validation checks."""
        if not self.hook.should_apply_self_validation(file_path):
            return

        # Apply all self-validation checks
        yield from self.hook.validate_single_file_validator(file_path, content)
        yield from self.hook.validate_project_standards(file_path, content)
        yield from self.hook.validate_validator_categorization(file_path, content)


if __name__ == "__main__":
    import sys

    if len(sys.argv) != 2:
        print("Usage: python -m vibelint.validators.self_validation <file_path>")
        sys.exit(1)

    file_path = Path(sys.argv[1])
    if not file_path.exists():
        print(f"File not found: {file_path}")
        sys.exit(1)

    try:
        content = file_path.read_text(encoding="utf-8")
        validator = VibelintSelfValidator()
        findings = list(validator.validate(file_path, content))

        if findings:
            print(f"Self-validation violations found in {file_path}:")
            for finding in findings:
                print(f"  Line {finding.line}: {finding.message}")
                if finding.suggestion:
                    print(f"    Suggestion: {finding.suggestion}")
            sys.exit(1)
        else:
            print(f"Self-validation passed for {file_path}")
            sys.exit(0)

    except Exception as e:
        print(f"Error during self-validation: {e}")
        sys.exit(1)
```

---
### File: src/vibelint/validators/single_file/strict_config.py

```python
"""
Strict Configuration Validator

Enforces strict configuration management by detecting and flagging fallback patterns.
All configuration should go through the CM (Configuration Management) system without fallbacks.
"""

import ast
import re
from pathlib import Path
from typing import Dict, List, NamedTuple


# Standalone versions for CLI usage
class ValidationResult(NamedTuple):
    rule_name: str
    severity: str
    message: str
    line_number: int
    column: int
    suggestion: str
    fix_suggestion: str = ""
    category: str = "general"


class CodeContext(NamedTuple):
    file_path: Path
    content: str


class ValidationRule:
    def __init__(self, name: str, description: str, category: str, severity: str):
        self.name = name
        self.description = description
        self.category = category
        self.severity = severity


class StrictConfigRule(ValidationRule):
    """Detects configuration fallbacks and enforces strict config management."""

    def __init__(self):
        super().__init__(
            name="strict-config",
            description="Enforce strict configuration management - no fallbacks",
            category="configuration",
            severity="error",
        )

    def validate(self, context: CodeContext) -> List[ValidationResult]:
        """Check for configuration fallback patterns."""
        results = []

        # Check Python files for .get() patterns with fallbacks
        if context.file_path.suffix == ".py":
            results.extend(self._check_python_config_fallbacks(context))

        # Check for hardcoded workers.dev URLs
        results.extend(self._check_hardcoded_endpoints(context))

        # Check TOML/YAML config files for hardcoded fallbacks
        if context.file_path.suffix in [".toml", ".yaml", ".yml"]:
            results.extend(self._check_config_file_fallbacks(context))

        return results

    def _check_python_config_fallbacks(self, context: CodeContext) -> List[ValidationResult]:
        """Check Python code for config.get() patterns with fallbacks."""
        results = []

        try:
            tree = ast.parse(context.content)
        except SyntaxError:
            return results

        for node in ast.walk(tree):
            if isinstance(node, ast.Call):
                # Check for .get() calls with default values
                if (
                    isinstance(node.func, ast.Attribute)
                    and node.func.attr == "get"
                    and len(node.args) >= 2
                ):

                    # Get the object being called (e.g., 'config', 'embeddings_config')
                    if isinstance(node.func.value, ast.Name):
                        var_name = node.func.value.id
                    elif isinstance(node.func.value, ast.Attribute):
                        var_name = ast.unparse(node.func.value)
                    else:
                        continue

                    # Check if this looks like a config object
                    if self._is_config_variable(var_name):
                        # Get the key and default value
                        key_node = node.args[0]
                        default_node = node.args[1]

                        key = self._extract_string_value(key_node)
                        default_value = self._extract_node_value(default_node)

                        # Flag as error
                        results.append(
                            ValidationResult(
                                rule_name=self.name,
                                severity="error",
                                message=f"Configuration fallback detected: {var_name}.get('{key}', {default_value})",
                                line_number=node.lineno,
                                column=node.col_offset,
                                suggestion=f"Use strict config: {var_name}['{key}'] and ensure value exists in config",
                                fix_suggestion=f"{var_name}['{key}']  # STRICT: No fallbacks",
                                category="configuration",
                            )
                        )

        return results

    def _check_hardcoded_endpoints(self, context: CodeContext) -> List[ValidationResult]:
        """Check for hardcoded endpoints that bypass CM."""
        results = []

        # Patterns that indicate hardcoded endpoints
        dangerous_patterns = [
            (r"workers\.dev", "Cloudflare Workers endpoint"),
            (r"https?://[^/]*\.workers\.dev", "Cloudflare Workers URL"),
            (r"https?://\d+\.\d+\.\d+\.\d+:\d+", "Hardcoded IP endpoint"),
            (r"localhost:\d+", "Hardcoded localhost endpoint"),
            (r"127\.0\.0\.1:\d+", "Hardcoded localhost endpoint"),
        ]

        lines = context.content.splitlines()
        for line_num, line in enumerate(lines, 1):
            for pattern, description in dangerous_patterns:
                matches = re.finditer(pattern, line)
                for match in matches:
                    # Skip if it's in a comment explaining the pattern
                    if "#" in line and line.index("#") < match.start():
                        continue

                    results.append(
                        ValidationResult(
                            rule_name=self.name,
                            severity="error",
                            message=f"Hardcoded endpoint detected: {description}",
                            line_number=line_num,
                            column=match.start(),
                            suggestion="Move endpoint configuration to dev.pyproject.toml or pyproject.toml",
                            fix_suggestion="# FIXME: Move to configuration management",
                            category="configuration",
                        )
                    )

        return results

    def _check_config_file_fallbacks(self, context: CodeContext) -> List[ValidationResult]:
        """Check TOML/YAML files for fallback patterns."""
        results = []

        # Check for production URLs in config files
        production_patterns = [
            r"workers\.dev",
            r"\.vercel\.app",
            r"\.netlify\.app",
            r"\.herokuapp\.com",
        ]

        lines = context.content.splitlines()
        for line_num, line in enumerate(lines, 1):
            for pattern in production_patterns:
                if re.search(pattern, line):
                    results.append(
                        ValidationResult(
                            rule_name=self.name,
                            severity="warning",
                            message="Production URL in config file may need dev override",
                            line_number=line_num,
                            column=0,
                            suggestion="Ensure dev.pyproject.toml overrides production URLs",
                            category="configuration",
                        )
                    )

        return results

    def _is_config_variable(self, var_name: str) -> bool:
        """Check if variable name suggests it's a config object."""
        config_indicators = [
            "config",
            "settings",
            "cfg",
            "conf",
            "embedding_config",
            "embeddings_config",
            "llm_config",
            "kaia_config",
            "tool_config",
            "vibelint_config",
            "guardrails_config",
        ]

        var_lower = var_name.lower()
        return any(indicator in var_lower for indicator in config_indicators)

    def _extract_string_value(self, node: ast.AST) -> str:
        """Extract string value from AST node."""
        if isinstance(node, ast.Constant) and isinstance(node.value, str):
            return node.value
        elif isinstance(node, ast.Str):  # Python < 3.8 compatibility
            return node.s
        else:
            return ast.unparse(node) if hasattr(ast, "unparse") else "<complex>"

    def _extract_node_value(self, node: ast.AST) -> str:
        """Extract a readable representation of the node value."""
        if isinstance(node, ast.Constant):
            return repr(node.value)
        elif isinstance(node, ast.Str):
            return repr(node.s)
        elif isinstance(node, ast.Num):
            return str(node.n)
        elif isinstance(node, (ast.List, ast.Tuple, ast.Dict)):
            return ast.unparse(node) if hasattr(ast, "unparse") else "<collection>"
        else:
            return ast.unparse(node) if hasattr(ast, "unparse") else "<complex>"


class ConfigFallbackDetector:
    """Standalone utility for detecting configuration fallbacks."""

    def __init__(self):
        self.rule = StrictConfigRule()

    def scan_directory(self, directory: Path) -> Dict[str, List[ValidationResult]]:
        """Scan a directory for configuration fallbacks."""
        results = {}

        for file_path in directory.rglob("*.py"):
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()

                context = CodeContext(file_path=file_path, content=content)
                file_results = self.rule.validate(context)

                if file_results:
                    results[str(file_path)] = file_results

            except Exception as e:
                print(f"Error scanning {file_path}: {e}")

        return results

    def generate_report(self, results: Dict[str, List[ValidationResult]]) -> str:
        """Generate a human-readable report."""
        if not results:
            return "✅ No configuration fallbacks detected!"

        report = ["🚨 CONFIGURATION FALLBACKS DETECTED", "=" * 50, ""]

        total_issues = sum(len(issues) for issues in results.values())
        report.append(f"Total files with issues: {len(results)}")
        report.append(f"Total fallback patterns: {total_issues}")
        report.append("")

        for file_path, issues in results.items():
            report.append(f"📁 {file_path}")
            report.append("-" * 50)

            for issue in issues:
                report.append(f"  ❌ Line {issue.line_number}: {issue.message}")
                report.append(f"     💡 {issue.suggestion}")
                if issue.fix_suggestion:
                    report.append(f"     🔧 Fix: {issue.fix_suggestion}")
                report.append("")

        report.append("=" * 50)
        report.append("🎯 RECOMMENDATION: Move all configuration to CM system")
        report.append("   1. Add required config to dev.pyproject.toml")
        report.append("   2. Replace .get() calls with strict [] access")
        report.append("   3. Let configuration errors fail loudly")

        return "\n".join(report)


# CLI interface for standalone usage
if __name__ == "__main__":
    import sys
    from pathlib import Path

    if len(sys.argv) != 2:
        print("Usage: python strict_config.py <directory>")
        sys.exit(1)

    directory = Path(sys.argv[1])
    if not directory.exists():
        print(f"Directory not found: {directory}")
        sys.exit(1)

    detector = ConfigFallbackDetector()
    results = detector.scan_directory(directory)
    report = detector.generate_report(results)
    print(report)

    # Exit with error code if issues found
    if results:
        sys.exit(1)
```

---
### File: src/vibelint/validators/single_file/typing_quality.py

```python
"""
Type quality validator using BaseValidator plugin system.

Detects poor typing practices that reduce code clarity and type safety:
- Raw tuples instead of dataclasses/NamedTuples
- Untyped dictionaries instead of TypedDict
- Excessive use of Any
- Missing type annotations on public functions
- String literals that should be Enums

vibelint/src/vibelint/validators/typing_quality.py
"""

import ast
from pathlib import Path
from typing import Iterator

from ...plugin_system import BaseValidator, Finding, Severity

__all__ = ["TypingQualityValidator"]


class TypingQualityValidator(BaseValidator):
    """Validator for detecting poor typing practices."""

    rule_id = "TYPING-POOR-PRACTICE"
    name = "Type Quality Checker"
    description = "Detects poor typing practices that reduce code clarity and type safety"
    default_severity = Severity.WARN

    def validate(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        """Validate typing practices in a Python file."""
        try:
            tree = ast.parse(content)
        except SyntaxError:
            return

        # Check for tuple type annotations that should be dataclasses
        visitor = _TypingVisitor()
        visitor.visit(tree)

        # Report tuple type aliases
        for line_num, name, tuple_info in visitor.tuple_type_aliases:
            if self._looks_like_data_structure(name, tuple_info):
                yield self.create_finding(
                    message=f"Type alias '{name}' uses raw Tuple{tuple_info} - consider using a dataclass or NamedTuple for better clarity",
                    file_path=file_path,
                    line=line_num,
                    suggestion="Replace with: @dataclass class {name}: ...",
                )

        # Report untyped dictionaries
        for line_num, context in visitor.untyped_dicts:
            yield self.create_finding(
                message=f"Using untyped Dict{context} - consider using TypedDict for better type safety",
                file_path=file_path,
                line=line_num,
                suggestion="Define a TypedDict with explicit field types",
            )

        # Report excessive Any usage
        for line_num, context in visitor.any_usage:
            if not self._is_acceptable_any_usage(context):
                yield self.create_finding(
                    message=f"Using Any type{context} - specify a more precise type",
                    file_path=file_path,
                    line=line_num,
                    suggestion="Replace Any with a specific type or Union of types",
                )

        # Report missing type annotations on public functions
        for line_num, func_name in visitor.untyped_public_functions:
            yield self.create_finding(
                message=f"Public function '{func_name}' is missing type annotations",
                file_path=file_path,
                line=line_num,
                suggestion=f"Add type hints: def {func_name}(...) -> ReturnType:",
            )

        # Report string literals that look like enums
        enum_candidates = self._find_enum_candidates(visitor.string_constants)
        for pattern, locations in enum_candidates.items():
            if len(locations) >= 3:  # Same string pattern used 3+ times
                first_line = locations[0]
                yield self.create_finding(
                    message=f"String literal '{pattern}' used {len(locations)} times - consider using an Enum",
                    file_path=file_path,
                    line=first_line,
                    suggestion="Create an Enum for these related string constants",
                )

    def _looks_like_data_structure(self, name: str, tuple_info: str) -> bool:
        """Check if a tuple type alias looks like it should be a data structure."""
        # Skip if it's a simple pair like (bool, str) for return values
        if tuple_info.count(",") == 1 and "bool" in tuple_info.lower():
            return False

        # If the name suggests it's data (Issue, Result, Info, etc.)
        data_suffixes = ["Issue", "Result", "Info", "Data", "Record", "Entry", "Item"]
        return any(name.endswith(suffix) for suffix in data_suffixes)

    def _is_acceptable_any_usage(self, context: str) -> bool:
        """Check if Any usage is acceptable in this context."""
        # Any is acceptable for **kwargs, *args, or when interfacing with external libs
        acceptable_patterns = ["**kwargs", "*args", "json", "yaml", "config"]
        return any(pattern in context.lower() for pattern in acceptable_patterns)

    def _find_enum_candidates(self, string_constants: list) -> dict:
        """Find string literals that are used repeatedly and could be enums."""
        from collections import defaultdict

        # Group by string pattern (uppercase, prefix, etc.)
        patterns = defaultdict(list)

        for line_num, value in string_constants:
            # Skip short strings and file paths
            if len(value) < 3 or "/" in value or "\\" in value:
                continue

            # Look for patterns like "ERROR", "WARNING", "INFO"
            if value.isupper() and "_" in value:
                patterns[value].append(line_num)
            # Or prefixed patterns like "RULE101", "ERR102"
            elif any(value.startswith(prefix) for prefix in ["RULE", "ERR", "WARN"]):
                prefix = value[:3]
                patterns[f"{prefix}*"].append(line_num)

        return patterns


class _TypingVisitor(ast.NodeVisitor):
    """AST visitor to detect typing issues."""

    def __init__(self):
        self.tuple_type_aliases = []  # (line, name, tuple_info)
        self.untyped_dicts = []  # (line, context)
        self.any_usage = []  # (line, context)
        self.untyped_public_functions = []  # (line, func_name)
        self.string_constants = []  # (line, value)

    def visit_AnnAssign(self, node):
        """Visit annotated assignments to find type aliases."""
        if isinstance(node.target, ast.Name):
            name = node.target.id

            # Check for Tuple type annotations
            if self._is_tuple_annotation(node.annotation):
                tuple_info = (
                    ast.unparse(node.annotation)
                    if hasattr(ast, "unparse")
                    else str(node.annotation)
                )
                self.tuple_type_aliases.append((node.lineno, name, tuple_info))

        self.generic_visit(node)

    def visit_Assign(self, node):
        """Visit assignments to find type aliases using old syntax."""
        for target in node.targets:
            if isinstance(target, ast.Name) and target.id.endswith(("Issue", "Result", "Info")):
                # Check if it's a type alias assignment like: ValidationIssue = Tuple[str, str]
                if self._is_tuple_annotation(node.value):
                    tuple_info = (
                        ast.unparse(node.value) if hasattr(ast, "unparse") else str(node.value)
                    )
                    self.tuple_type_aliases.append((node.lineno, target.id, tuple_info))

        self.generic_visit(node)

    def visit_FunctionDef(self, node):
        """Visit function definitions to check for type annotations."""
        # Check if public function (doesn't start with _)
        if not node.name.startswith("_"):
            # Check if it has return type annotation
            if node.returns is None:
                self.untyped_public_functions.append((node.lineno, node.name))
            else:
                # Check for Any in return type
                if self._contains_any(node.returns):
                    context = f" in return type of {node.name}"
                    self.any_usage.append((node.lineno, context))

            # Check parameters
            for arg in node.args.args:
                if arg.annotation is None and arg.arg != "self":
                    self.untyped_public_functions.append((node.lineno, f"{node.name}({arg.arg})"))
                elif arg.annotation and self._contains_any(arg.annotation):
                    context = f" in parameter '{arg.arg}' of {node.name}"
                    self.any_usage.append((node.lineno, context))

        self.generic_visit(node)

    def visit_Constant(self, node):
        """Visit string constants to find potential enums."""
        if isinstance(node.value, str):
            self.string_constants.append((node.lineno, node.value))
        self.generic_visit(node)

    def _is_tuple_annotation(self, node) -> bool:
        """Check if a node is a Tuple type annotation."""
        if isinstance(node, ast.Subscript):
            if isinstance(node.value, ast.Name) and node.value.id == "Tuple":
                return True
            # Check for typing.Tuple
            if isinstance(node.value, ast.Attribute):
                if node.value.attr == "Tuple":
                    return True
        return False

    def _contains_any(self, node) -> bool:
        """Check if a type annotation contains Any."""
        if isinstance(node, ast.Name) and node.id == "Any":
            return True
        if isinstance(node, ast.Attribute) and node.attr == "Any":
            return True
        # Recursively check in subscripts (like Optional[Any], List[Any])
        if isinstance(node, ast.Subscript):
            return self._contains_any(node.value) or any(
                self._contains_any(arg)
                for arg in (node.slice.elts if isinstance(node.slice, ast.Tuple) else [node.slice])
            )
        return False
```

---
### File: src/vibelint/vector_store.py

```python
"""
Vector Database Adapter for vibelint.

Provides a unified interface for vector storage and similarity search
across multiple backends: in-memory, Qdrant, Pinecone.

Architecture:
- VectorStore: Abstract base class defining the interface
- InMemoryVectorStore: Fast local storage using numpy/faiss
- QdrantVectorStore: Production-ready local/cloud vector database
- PineconeVectorStore: Cloud vector database for enterprise deployments

Usage:
    from vibelint.vector_store import get_vector_store

    # Auto-configured based on environment/config
    store = get_vector_store(config)

    # Store embeddings
    store.upsert("file_path", embedding, metadata={"type": "function"})

    # Search similar
    results = store.search(query_embedding, top_k=5, filter={"type": "function"})

vibelint/src/vibelint/vector_store.py
"""

import logging
import os
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Union

import numpy as np

logger = logging.getLogger(__name__)


@dataclass
class VectorSearchResult:
    """Result from vector similarity search."""

    id: str
    score: float
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] = None


@dataclass
class VectorStoreConfig:
    """Configuration for vector store backends."""

    backend: str = "memory"  # "memory", "qdrant", "pinecone"

    # In-memory options (fallback only)
    cache_size: int = 10000

    # Qdrant options
    qdrant_url: str = "http://localhost:6333"
    qdrant_api_key: Optional[str] = None
    qdrant_collection: str = "vibelint_embeddings"

    # Pinecone options
    pinecone_api_key: Optional[str] = None
    pinecone_environment: str = "us-west1-gcp"
    pinecone_index: str = "vibelint-embeddings"

    # Cloudflare Vectorize options
    vectorize_api_token: Optional[str] = None
    vectorize_account_id: Optional[str] = None
    vectorize_index: str = "vibelint-embeddings"

    # Common options
    dimension: int = 768
    similarity_metric: str = "cosine"  # "cosine", "euclidean", "dot"


class VectorStore(ABC):
    """Abstract base class for vector storage backends."""

    def __init__(self, config: VectorStoreConfig):
        self.config = config
        self.dimension = config.dimension

    @abstractmethod
    def upsert(self, id: str, embedding: List[float], metadata: Dict[str, Any] = None) -> bool:
        """Store or update a vector with metadata."""
        pass

    @abstractmethod
    def search(
        self, query_embedding: List[float], top_k: int = 5, filter: Optional[Dict[str, Any]] = None
    ) -> List[VectorSearchResult]:
        """Search for similar vectors."""
        pass

    @abstractmethod
    def delete(self, id: str) -> bool:
        """Delete a vector by ID."""
        pass

    @abstractmethod
    def get(self, id: str) -> Optional[VectorSearchResult]:
        """Get a specific vector by ID."""
        pass

    @abstractmethod
    def list_ids(self, filter: Optional[Dict[str, Any]] = None) -> List[str]:
        """List all vector IDs, optionally filtered."""
        pass

    @abstractmethod
    def clear(self) -> bool:
        """Clear all vectors."""
        pass

    @abstractmethod
    def stats(self) -> Dict[str, Any]:
        """Get storage statistics."""
        pass


class InMemoryVectorStore(VectorStore):
    """In-memory vector store using FAISS for fast similarity search."""

    def __init__(self, config: VectorStoreConfig):
        super().__init__(config)

        try:
            import faiss

            self.faiss = faiss

            # Create FAISS index for fast similarity search
            if config.similarity_metric == "cosine":
                # Normalize vectors for cosine similarity
                self.index = faiss.IndexFlatIP(
                    self.dimension
                )  # Inner product on normalized vectors = cosine
                self.normalize_vectors = True
            elif config.similarity_metric == "euclidean":
                self.index = faiss.IndexFlatL2(self.dimension)  # L2 distance
                self.normalize_vectors = False
            else:  # dot product
                self.index = faiss.IndexFlatIP(self.dimension)  # Inner product
                self.normalize_vectors = False

            self.id_to_index: Dict[str, int] = {}  # Map IDs to FAISS indices
            self.index_to_id: Dict[int, str] = {}  # Map FAISS indices to IDs
            self.metadata: Dict[str, Dict[str, Any]] = {}
            self.next_index = 0

            logger.info(
                f"InMemoryVectorStore: FAISS-powered similarity search ({config.similarity_metric})"
            )

        except ImportError:
            logger.error("FAISS not available. Install with: pip install faiss-cpu")
            raise

    def upsert(self, id: str, embedding: List[float], metadata: Dict[str, Any] = None) -> bool:
        """Store or update a vector with metadata."""
        try:
            embedding_array = np.array(embedding, dtype=np.float32).reshape(1, -1)
            if embedding_array.shape[1] != self.dimension:
                logger.warning(
                    f"Embedding dimension mismatch: expected {self.dimension}, got {embedding_array.shape[1]}"
                )
                return False

            # Normalize if using cosine similarity
            if self.normalize_vectors:
                self.faiss.normalize_L2(embedding_array)

            # Remove existing vector if updating
            if id in self.id_to_index:
                # FAISS doesn't support direct updates, so we'll track this as a limitation
                logger.debug(f"Vector {id} already exists - FAISS doesn't support updates")
                return True

            # Add to FAISS index
            self.index.add(embedding_array)

            # Track mapping
            faiss_index = self.next_index
            self.id_to_index[id] = faiss_index
            self.index_to_id[faiss_index] = id
            self.metadata[id] = metadata or {}
            self.next_index += 1

            return True
        except Exception as e:
            logger.error(f"Failed to upsert vector {id}: {e}")
            return False

    def search(
        self, query_embedding: List[float], top_k: int = 5, filter: Optional[Dict[str, Any]] = None
    ) -> List[VectorSearchResult]:
        """Search for similar vectors using FAISS."""
        if self.index.ntotal == 0:
            return []

        try:
            query_array = np.array(query_embedding, dtype=np.float32).reshape(1, -1)

            # Normalize query if using cosine similarity
            if self.normalize_vectors:
                self.faiss.normalize_L2(query_array)

            # Perform FAISS search
            scores, indices = self.index.search(query_array, min(top_k, self.index.ntotal))

            results = []
            for i in range(len(scores[0])):
                faiss_index = indices[0][i]
                score = float(scores[0][i])

                # Skip invalid indices
                if faiss_index == -1:
                    continue

                # Get original ID
                if faiss_index not in self.index_to_id:
                    continue

                vec_id = self.index_to_id[faiss_index]

                # Apply metadata filter if specified
                if filter and not self._matches_filter(self.metadata.get(vec_id, {}), filter):
                    continue

                # Convert FAISS score based on similarity metric
                if self.config.similarity_metric == "cosine":
                    # FAISS inner product on normalized vectors = cosine similarity
                    final_score = score
                elif self.config.similarity_metric == "euclidean":
                    # FAISS returns squared L2 distance, convert to similarity
                    final_score = 1.0 / (1.0 + score)
                else:  # dot product
                    final_score = score

                results.append(
                    VectorSearchResult(
                        id=vec_id, score=final_score, metadata=self.metadata.get(vec_id, {})
                    )
                )

            # Sort by score descending and limit to top_k
            results.sort(key=lambda x: x.score, reverse=True)
            return results[:top_k]

        except Exception as e:
            logger.error(f"Failed to search vectors: {e}")
            return []

    def delete(self, id: str) -> bool:
        """Delete a vector by ID."""
        try:
            if id in self.id_to_index:
                # FAISS doesn't support deletion efficiently
                # We'll just remove from our tracking but keep in FAISS index
                faiss_index = self.id_to_index[id]
                del self.id_to_index[id]
                del self.index_to_id[faiss_index]
                del self.metadata[id]
                logger.debug(f"Logically deleted {id} (FAISS index entry remains)")
                return True
            return False
        except Exception as e:
            logger.error(f"Failed to delete vector {id}: {e}")
            return False

    def get(self, id: str) -> Optional[VectorSearchResult]:
        """Get a specific vector by ID."""
        if id not in self.id_to_index:
            return None

        return VectorSearchResult(
            id=id,
            score=1.0,  # Perfect match
            metadata=self.metadata.get(id, {}),
            # Note: FAISS doesn't easily retrieve vectors, so no embedding returned
        )

    def list_ids(self, filter: Optional[Dict[str, Any]] = None) -> List[str]:
        """List all vector IDs, optionally filtered."""
        if not filter:
            return list(self.id_to_index.keys())
        return self._apply_filter(filter)

    def clear(self) -> bool:
        """Clear all vectors."""
        try:
            # Reset FAISS index
            if self.config.similarity_metric == "cosine":
                self.index = self.faiss.IndexFlatIP(self.dimension)
            elif self.config.similarity_metric == "euclidean":
                self.index = self.faiss.IndexFlatL2(self.dimension)
            else:
                self.index = self.faiss.IndexFlatIP(self.dimension)

            self.id_to_index.clear()
            self.index_to_id.clear()
            self.metadata.clear()
            self.next_index = 0
            return True
        except Exception as e:
            logger.error(f"Failed to clear vectors: {e}")
            return False

    def stats(self) -> Dict[str, Any]:
        """Get storage statistics."""
        return {
            "backend": "memory_faiss",
            "total_vectors": len(self.id_to_index),
            "faiss_total": self.index.ntotal,
            "dimension": self.dimension,
            "similarity_metric": self.config.similarity_metric,
            "index_type": type(self.index).__name__,
        }

    # FAISS handles similarity computation internally

    def _apply_filter(self, filter: Dict[str, Any]) -> List[str]:
        """Apply metadata filter to get matching vector IDs."""
        matching_ids = []

        for vec_id, metadata in self.metadata.items():
            if self._matches_filter(metadata, filter):
                matching_ids.append(vec_id)

        return matching_ids

    def _matches_filter(self, metadata: Dict[str, Any], filter: Dict[str, Any]) -> bool:
        """Check if metadata matches filter criteria."""
        for key, value in filter.items():
            if key not in metadata or metadata[key] != value:
                return False
        return True

    # JSON persistence removed - use Qdrant for proper vector storage


class QdrantVectorStore(VectorStore):
    """Qdrant vector store adapter."""

    def __init__(self, config: VectorStoreConfig):
        super().__init__(config)
        self.client = None
        self.collection_name = config.qdrant_collection

        try:
            from qdrant_client import QdrantClient
            from qdrant_client.models import (Distance, PointStruct,
                                              VectorParams)

            self.client = QdrantClient(url=config.qdrant_url, api_key=config.qdrant_api_key)

            # Create collection if it doesn't exist
            try:
                self.client.get_collection(self.collection_name)
            except:
                distance_map = {
                    "cosine": Distance.COSINE,
                    "euclidean": Distance.EUCLID,
                    "dot": Distance.DOT,
                }

                self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=VectorParams(
                        size=self.dimension,
                        distance=distance_map.get(config.similarity_metric, Distance.COSINE),
                    ),
                )

            logger.info(f"Connected to Qdrant at {config.qdrant_url}")

        except ImportError:
            logger.error("qdrant-client not installed. Run: pip install qdrant-client")
            raise
        except Exception as e:
            logger.error(f"Failed to connect to Qdrant: {e}")
            raise

    def upsert(self, id: str, embedding: List[float], metadata: Dict[str, Any] = None) -> bool:
        """Store or update a vector with metadata."""
        try:
            import uuid

            from qdrant_client.models import PointStruct

            # Qdrant requires UUID or integer IDs, so convert string to UUID
            point_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, id))

            # Store original ID in metadata for retrieval
            payload = metadata or {}
            payload["original_id"] = id

            point = PointStruct(id=point_id, vector=embedding, payload=payload)

            self.client.upsert(collection_name=self.collection_name, points=[point])
            return True

        except Exception as e:
            logger.error(f"Failed to upsert vector {id}: {e}")
            return False

    def search(
        self, query_embedding: List[float], top_k: int = 5, filter: Optional[Dict[str, Any]] = None
    ) -> List[VectorSearchResult]:
        """Search for similar vectors."""
        try:
            from qdrant_client.models import FieldCondition, Filter, MatchValue

            # Convert filter to Qdrant format
            qdrant_filter = None
            if filter:
                conditions = []
                for key, value in filter.items():
                    conditions.append(FieldCondition(key=key, match=MatchValue(value=value)))
                qdrant_filter = Filter(must=conditions)

            results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                limit=top_k,
                query_filter=qdrant_filter,
                with_payload=True,
            )

            return [
                VectorSearchResult(
                    id=hit.payload.get("original_id", str(hit.id)),  # Return original ID
                    score=hit.score,
                    metadata={k: v for k, v in (hit.payload or {}).items() if k != "original_id"},
                )
                for hit in results
            ]

        except Exception as e:
            logger.error(f"Failed to search vectors: {e}")
            return []

    def delete(self, id: str) -> bool:
        """Delete a vector by ID."""
        try:
            import uuid

            # Convert string ID to UUID
            point_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, id))

            self.client.delete(collection_name=self.collection_name, points_selector=[point_id])
            return True
        except Exception as e:
            logger.error(f"Failed to delete vector {id}: {e}")
            return False

    def get(self, id: str) -> Optional[VectorSearchResult]:
        """Get a specific vector by ID."""
        try:
            import uuid

            # Convert string ID to UUID
            point_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, id))

            result = self.client.retrieve(
                collection_name=self.collection_name,
                ids=[point_id],
                with_payload=True,
                with_vectors=True,
            )

            if result:
                point = result[0]
                payload = point.payload or {}
                return VectorSearchResult(
                    id=payload.get("original_id", id),
                    score=1.0,
                    metadata={k: v for k, v in payload.items() if k != "original_id"},
                    embedding=point.vector,
                )
            return None

        except Exception as e:
            logger.error(f"Failed to get vector {id}: {e}")
            return None

    def list_ids(self, filter: Optional[Dict[str, Any]] = None) -> List[str]:
        """List all vector IDs, optionally filtered."""
        try:
            # Qdrant doesn't have a direct "list IDs" method, so we do a scroll
            from qdrant_client.models import FieldCondition, Filter, MatchValue

            qdrant_filter = None
            if filter:
                conditions = []
                for key, value in filter.items():
                    conditions.append(FieldCondition(key=key, match=MatchValue(value=value)))
                qdrant_filter = Filter(must=conditions)

            result, _ = self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter=qdrant_filter,
                with_payload=False,
                with_vectors=False,
            )

            return [str(point.id) for point in result]

        except Exception as e:
            logger.error(f"Failed to list IDs: {e}")
            return []

    def clear(self) -> bool:
        """Clear all vectors."""
        try:
            # Delete and recreate collection
            self.client.delete_collection(self.collection_name)

            from qdrant_client.models import Distance, VectorParams

            distance_map = {
                "cosine": Distance.COSINE,
                "euclidean": Distance.EUCLID,
                "dot": Distance.DOT,
            }

            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=self.dimension,
                    distance=distance_map.get(self.config.similarity_metric, Distance.COSINE),
                ),
            )
            return True

        except Exception as e:
            logger.error(f"Failed to clear vectors: {e}")
            return False

    def stats(self) -> Dict[str, Any]:
        """Get storage statistics."""
        try:
            info = self.client.get_collection(self.collection_name)
            return {
                "backend": "qdrant",
                "total_vectors": info.points_count,
                "dimension": self.dimension,
                "collection": self.collection_name,
                "url": self.config.qdrant_url,
                "similarity_metric": self.config.similarity_metric,
            }
        except Exception as e:
            logger.error(f"Failed to get stats: {e}")
            return {"backend": "qdrant", "error": str(e)}


class PineconeVectorStore(VectorStore):
    """Pinecone vector store adapter."""

    def __init__(self, config: VectorStoreConfig):
        super().__init__(config)
        self.index = None
        self.index_name = config.pinecone_index

        try:
            import pinecone

            pinecone.init(api_key=config.pinecone_api_key, environment=config.pinecone_environment)

            # Create index if it doesn't exist
            if self.index_name not in pinecone.list_indexes():
                metric_map = {"cosine": "cosine", "euclidean": "euclidean", "dot": "dotproduct"}

                pinecone.create_index(
                    name=self.index_name,
                    dimension=self.dimension,
                    metric=metric_map.get(config.similarity_metric, "cosine"),
                )

            self.index = pinecone.Index(self.index_name)
            logger.info(f"Connected to Pinecone index: {self.index_name}")

        except ImportError:
            logger.error("pinecone-client not installed. Run: pip install pinecone-client")
            raise
        except Exception as e:
            logger.error(f"Failed to connect to Pinecone: {e}")
            raise

    def upsert(self, id: str, embedding: List[float], metadata: Dict[str, Any] = None) -> bool:
        """Store or update a vector with metadata."""
        try:
            self.index.upsert(vectors=[(id, embedding, metadata or {})])
            return True
        except Exception as e:
            logger.error(f"Failed to upsert vector {id}: {e}")
            return False

    def search(
        self, query_embedding: List[float], top_k: int = 5, filter: Optional[Dict[str, Any]] = None
    ) -> List[VectorSearchResult]:
        """Search for similar vectors."""
        try:
            results = self.index.query(
                vector=query_embedding, top_k=top_k, filter=filter, include_metadata=True
            )

            return [
                VectorSearchResult(id=match.id, score=match.score, metadata=match.metadata or {})
                for match in results.matches
            ]

        except Exception as e:
            logger.error(f"Failed to search vectors: {e}")
            return []

    def delete(self, id: str) -> bool:
        """Delete a vector by ID."""
        try:
            self.index.delete(ids=[id])
            return True
        except Exception as e:
            logger.error(f"Failed to delete vector {id}: {e}")
            return False

    def get(self, id: str) -> Optional[VectorSearchResult]:
        """Get a specific vector by ID."""
        try:
            result = self.index.fetch(ids=[id])
            if id in result.vectors:
                vector_data = result.vectors[id]
                return VectorSearchResult(
                    id=id,
                    score=1.0,
                    metadata=vector_data.metadata or {},
                    embedding=vector_data.values,
                )
            return None
        except Exception as e:
            logger.error(f"Failed to get vector {id}: {e}")
            return None

    def list_ids(self, filter: Optional[Dict[str, Any]] = None) -> List[str]:
        """List all vector IDs, optionally filtered."""
        # Pinecone doesn't provide a direct list IDs method
        # This would require implementing pagination through queries
        logger.warning("list_ids not efficiently supported by Pinecone")
        return []

    def clear(self) -> bool:
        """Clear all vectors."""
        try:
            self.index.delete(delete_all=True)
            return True
        except Exception as e:
            logger.error(f"Failed to clear vectors: {e}")
            return False

    def stats(self) -> Dict[str, Any]:
        """Get storage statistics."""
        try:
            stats = self.index.describe_index_stats()
            return {
                "backend": "pinecone",
                "total_vectors": stats.total_vector_count,
                "dimension": self.dimension,
                "index_name": self.index_name,
                "environment": self.config.pinecone_environment,
                "similarity_metric": self.config.similarity_metric,
            }
        except Exception as e:
            logger.error(f"Failed to get stats: {e}")
            return {"backend": "pinecone", "error": str(e)}


class CloudflareVectorizeStore(VectorStore):
    """Cloudflare Vectorize adapter - perfect for edge deployment."""

    def __init__(self, config: VectorStoreConfig):
        super().__init__(config)
        self.api_token = config.vectorize_api_token or os.getenv("VECTORIZE_API_TOKEN")
        self.account_id = config.vectorize_account_id or os.getenv("VECTORIZE_ACCOUNT_ID")
        self.index_name = config.vectorize_index

        if not self.api_token or not self.account_id:
            raise ValueError("Vectorize API token and account ID required")

        self.base_url = f"https://api.cloudflare.com/client/v4/accounts/{self.account_id}/vectorize/indexes/{self.index_name}"
        self.headers = {
            "Authorization": f"Bearer {self.api_token}",
            "Content-Type": "application/json",
        }

        # Create index if needed
        self._ensure_index_exists()
        logger.info(f"Connected to Cloudflare Vectorize index: {self.index_name}")

    def _ensure_index_exists(self):
        """Create index if it doesn't exist."""
        try:
            import requests

            # Check if index exists
            response = requests.get(self.base_url, headers=self.headers)
            if response.status_code == 404:
                # Create index
                create_url = f"https://api.cloudflare.com/client/v4/accounts/{self.account_id}/vectorize/indexes"
                payload = {
                    "name": self.index_name,
                    "config": {
                        "dimensions": self.dimension,
                        "metric": "cosine",  # Vectorize uses cosine by default
                    },
                }
                response = requests.post(create_url, headers=self.headers, json=payload)
                response.raise_for_status()
                logger.info(f"Created Vectorize index: {self.index_name}")

        except Exception as e:
            logger.warning(f"Failed to ensure index exists: {e}")

    def upsert(self, id: str, embedding: List[float], metadata: Dict[str, Any] = None) -> bool:
        """Store or update a vector with metadata."""
        try:
            import requests

            payload = {"vectors": [{"id": id, "values": embedding, "metadata": metadata or {}}]}

            response = requests.post(f"{self.base_url}/upsert", headers=self.headers, json=payload)
            response.raise_for_status()
            return True

        except Exception as e:
            logger.error(f"Failed to upsert vector {id}: {e}")
            return False

    def search(
        self, query_embedding: List[float], top_k: int = 5, filter: Optional[Dict[str, Any]] = None
    ) -> List[VectorSearchResult]:
        """Search for similar vectors."""
        try:
            import requests

            payload = {
                "vector": query_embedding,
                "topK": top_k,
                "returnValues": True,
                "returnMetadata": True,
            }

            # Vectorize filter format (if supported)
            if filter:
                payload["filter"] = filter

            response = requests.post(f"{self.base_url}/query", headers=self.headers, json=payload)
            response.raise_for_status()

            data = response.json()
            if not data.get("success", False):
                logger.error(f"Vectorize query failed: {data}")
                return []

            results = []
            for match in data.get("result", {}).get("matches", []):
                results.append(
                    VectorSearchResult(
                        id=match["id"],
                        score=match["score"],
                        metadata=match.get("metadata", {}),
                        embedding=match.get("values"),
                    )
                )

            return results

        except Exception as e:
            logger.error(f"Failed to search vectors: {e}")
            return []

    def delete(self, id: str) -> bool:
        """Delete a vector by ID."""
        try:
            import requests

            payload = {"ids": [id]}
            response = requests.post(f"{self.base_url}/delete", headers=self.headers, json=payload)
            response.raise_for_status()
            return True

        except Exception as e:
            logger.error(f"Failed to delete vector {id}: {e}")
            return False

    def get(self, id: str) -> Optional[VectorSearchResult]:
        """Get a specific vector by ID."""
        try:
            import requests

            payload = {"ids": [id]}
            response = requests.post(
                f"{self.base_url}/getByIds", headers=self.headers, json=payload
            )
            response.raise_for_status()

            data = response.json()
            if data.get("success") and data.get("result"):
                vectors = data["result"]
                if vectors:
                    vector = vectors[0]
                    return VectorSearchResult(
                        id=vector["id"],
                        score=1.0,
                        metadata=vector.get("metadata", {}),
                        embedding=vector.get("values"),
                    )
            return None

        except Exception as e:
            logger.error(f"Failed to get vector {id}: {e}")
            return None

    def list_ids(self, filter: Optional[Dict[str, Any]] = None) -> List[str]:
        """List all vector IDs, optionally filtered."""
        # Vectorize doesn't provide a direct list method
        # Would need to implement via query with high top_k
        logger.warning("list_ids not efficiently supported by Vectorize")
        return []

    def clear(self) -> bool:
        """Clear all vectors."""
        try:
            import requests

            # Delete index and recreate
            response = requests.delete(self.base_url, headers=self.headers)
            if response.status_code in [200, 404]:  # OK or already deleted
                self._ensure_index_exists()
                return True
            return False

        except Exception as e:
            logger.error(f"Failed to clear vectors: {e}")
            return False

    def stats(self) -> Dict[str, Any]:
        """Get storage statistics."""
        try:
            import requests

            response = requests.get(self.base_url, headers=self.headers)
            response.raise_for_status()

            data = response.json()
            if data.get("success"):
                index_info = data["result"]
                return {
                    "backend": "vectorize",
                    "index_name": self.index_name,
                    "dimension": self.dimension,
                    "account_id": self.account_id,
                    "similarity_metric": "cosine",
                    "created_on": index_info.get("created_on"),
                    "modified_on": index_info.get("modified_on"),
                }

        except Exception as e:
            logger.error(f"Failed to get stats: {e}")

        return {"backend": "vectorize", "error": "Failed to get stats"}


def get_vector_store(config: Union[Dict[str, Any], VectorStoreConfig]) -> VectorStore:
    """Factory function to get appropriate vector store based on configuration."""

    if isinstance(config, dict):
        # Convert from vibelint config format
        embeddings_config = config.get("embeddings", {})
        vector_config = config.get("vector_store", {})

        store_config = VectorStoreConfig(
            backend=vector_config.get("backend", "qdrant"),  # Default to Qdrant
            dimension=embeddings_config.get("code_dimensions", 768),
            cache_size=vector_config.get("cache_size", 10000),
            qdrant_url=vector_config.get("qdrant_url", "http://localhost:6333"),
            qdrant_api_key=os.getenv("QDRANT_API_KEY"),
            qdrant_collection=vector_config.get("qdrant_collection", "vibelint_embeddings"),
            pinecone_api_key=os.getenv("PINECONE_API_KEY"),
            pinecone_environment=vector_config.get("pinecone_environment", "us-west1-gcp"),
            pinecone_index=vector_config.get("pinecone_index", "vibelint-embeddings"),
            similarity_metric=vector_config.get("similarity_metric", "cosine"),
        )
    else:
        store_config = config

    backend = store_config.backend.lower()

    if backend == "qdrant":
        return QdrantVectorStore(store_config)
    elif backend == "pinecone":
        return PineconeVectorStore(store_config)
    elif backend == "vectorize" or backend == "cloudflare":
        return CloudflareVectorizeStore(store_config)
    else:  # Default to memory
        return InMemoryVectorStore(store_config)
```

---
### File: src/vibelint/workflows/__init__.py

```python
"""
Workflow management subsystem for vibelint.

Modular workflow system with centralized registry and clear separation:
- core/: Base classes, registry, orchestration
- implementations/: Actual workflow implementations

Responsibility: Workflow module organization and re-exports only.
Individual workflow logic belongs in specific implementation modules.

vibelint/src/vibelint/workflow/__init__.py
"""

# Import implementation categories for direct access
from . import implementations
# Import core workflow system
from .core.base import (BaseWorkflow, WorkflowConfig, WorkflowMetrics,
                        WorkflowPriority, WorkflowResult, WorkflowStatus)
from .evaluation import WorkflowEvaluator
# Import orchestration
from .manager import WorkflowManager
from .orchestrator import AnalysisOrchestrator
# Import registry system
from .registry import WorkflowRegistry, register_workflow, workflow_registry

# Import specific implementations for backward compatibility
try:
    from .implementations.justification import FileJustificationWorkflow
    from .implementations.single_file_validation import \
        SingleFileValidationWorkflow
except ImportError:
    # These might not exist yet or have import issues
    pass

__all__ = [
    # Core workflow system
    "BaseWorkflow",
    "WorkflowResult",
    "WorkflowConfig",
    "WorkflowMetrics",
    "WorkflowStatus",
    "WorkflowPriority",
    # Registry system
    "WorkflowRegistry",
    "workflow_registry",
    "register_workflow",
    # Orchestration
    "WorkflowManager",
    "AnalysisOrchestrator",
    "WorkflowEvaluator",
    # Implementation modules
    "implementations",
]
```

---
### File: src/vibelint/workflows/cleanup.py

```python
"""
Vibelint Project Cleanup Workflow

Implements systematic project cleanup based on Workflow 7 principles.
Human-in-the-loop orchestration for cleaning up messy repositories.
"""

import json
import os
import shutil
import subprocess
import tempfile
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional


class ProjectCleanupWorkflow:
 """Systematic project cleanup with human decision points."""

 def __init__(self, project_root: Path):
 self.project_root = Path(project_root)
 self.cleanup_log = []
 self.temp_backup_dir = None

 def analyze_project_mess(self) -> Dict[str, Any]:
 """
 Analyze the project to identify cleanup opportunities.
 Human Decision Point: What types of mess to look for.
 """
 analysis = {
 "duplicate_files": self._find_duplicate_files(),
 "temp_files": self._find_temp_files(),
 "unused_files": self._find_unused_files(),
 "large_files": self._find_large_files(),
 "empty_directories": self._find_empty_directories(),
 "config_fragments": self._find_config_fragments(),
 "debug_scripts": self._find_debug_scripts(),
 "backup_files": self._find_backup_files(),
 "untracked_important": self._find_untracked_important_files(),
 }

 # Calculate mess score
 analysis["mess_score"] = self._calculate_mess_score(analysis)
 analysis["recommendations"] = self._generate_cleanup_recommendations(analysis)

 return analysis

 def _find_duplicate_files(self) -> List[Dict[str, Any]]:
 """Find duplicate files by content hash."""
 import hashlib

 file_hashes = {}
 duplicates = []

 for file_path in self.project_root.rglob("*"):
 if file_path.is_file() and not self._should_ignore_file(file_path):
 try:
 with open(file_path, "rb") as f:
 file_hash = hashlib.md5(f.read()).hexdigest()

 if file_hash in file_hashes:
 duplicates.append({
 "original": str(file_hashes[file_hash]),
 "duplicate": str(file_path),
 "size": file_path.stat().st_size,
 "hash": file_hash
 })
 else:
 file_hashes[file_hash] = file_path

 except (IOError, OSError):
 continue

 return duplicates

 def _find_temp_files(self) -> List[Dict[str, Any]]:
 """Find temporary and build artifacts."""
 temp_patterns = [
 "*.tmp", "*.temp", "*.bak", "*.swp", "*.swo",
 "*~", "#*#", ".#*", "*.orig", "*.rej",
 "__pycache__/", "*.pyc", "*.pyo", "*.pyd",
 ".pytest_cache/", ".coverage", "*.egg-info/",
 "node_modules/", ".npm/", "yarn-error.log",
 ".DS_Store", "Thumbs.db", "desktop.ini",
 ".vscode/", ".idea/", "*.log"
 ]

 temp_files = []
 for pattern in temp_patterns:
 for file_path in self.project_root.rglob(pattern):
 if file_path.exists():
 temp_files.append({
 "path": str(file_path),
 "type": "temp_file" if file_path.is_file() else "temp_directory",
 "size": self._get_size(file_path),
 "pattern": pattern
 })

 return temp_files

 def _find_unused_files(self) -> List[Dict[str, Any]]:
 """Find files that appear unused (not imported/referenced)."""
 # Simple heuristic - look for Python files that aren't imported
 python_files = list(self.project_root.rglob("*.py"))
 all_content = ""

 # Read all Python files to check for imports
 for py_file in python_files:
 try:
 with open(py_file, "r", encoding="utf-8") as f:
 all_content += f.read() + "\n"
 except (IOError, UnicodeDecodeError):
 continue

 unused_files = []
 for py_file in python_files:
 # Skip if it's a main script or test file
 if py_file.name in ["__main__.py", "__init__.py"] or "test" in py_file.name.lower():
 continue

 module_name = py_file.stem
 # Check if module is imported anywhere
 if f"import {module_name}" not in all_content and f"from {module_name}" not in all_content:
 unused_files.append({
 "path": str(py_file),
 "type": "potentially_unused_python_module",
 "size": py_file.stat().st_size
 })

 return unused_files

 def _find_large_files(self) -> List[Dict[str, Any]]:
 """Find unusually large files that might need attention."""
 large_files = []
 size_threshold = 1024 * 1024 # 1MB

 for file_path in self.project_root.rglob("*"):
 if file_path.is_file() and file_path.stat().st_size > size_threshold:
 large_files.append({
 "path": str(file_path),
 "size": file_path.stat().st_size,
 "size_mb": file_path.stat().st_size / (1024 * 1024)
 })

 return sorted(large_files, key=lambda x: x["size"], reverse=True)

 def _find_empty_directories(self) -> List[str]:
 """Find empty directories that can be removed."""
 empty_dirs = []

 for dir_path in self.project_root.rglob("*"):
 if dir_path.is_dir() and not any(dir_path.iterdir()):
 empty_dirs.append(str(dir_path))

 return empty_dirs

 def _find_config_fragments(self) -> List[Dict[str, Any]]:
 """Find scattered configuration files that might be consolidated."""
 config_patterns = [
 "*.toml", "*.yaml", "*.yml", "*.json", "*.ini", "*.cfg",
 ".env*", "Dockerfile*", "requirements*.txt", "setup.py",
 "pyproject.toml", "setup.cfg", "tox.ini", ".gitignore"
 ]

 config_files = []
 for pattern in config_patterns:
 for file_path in self.project_root.rglob(pattern):
 if file_path.is_file():
 config_files.append({
 "path": str(file_path),
 "type": "config_file",
 "pattern": pattern,
 "size": file_path.stat().st_size
 })

 return config_files

 def _find_debug_scripts(self) -> List[Dict[str, Any]]:
 """Find debug/test scripts that might be temporary."""
 debug_patterns = [
 "debug_*.py", "test_*.py", "*_debug.py", "*_test.py",
 "scratch*.py", "temp*.py", "fix_*.py", "quick_*.py"
 ]

 debug_files = []
 for pattern in debug_patterns:
 for file_path in self.project_root.rglob(pattern):
 if file_path.is_file():
 debug_files.append({
 "path": str(file_path),
 "type": "debug_script",
 "pattern": pattern,
 "size": file_path.stat().st_size
 })

 return debug_files

 def _find_backup_files(self) -> List[Dict[str, Any]]:
 """Find backup files that can be cleaned up."""
 backup_patterns = [
 "*.backup", "*.bkp", "*_backup.*", "*.old",
 "*_old.*", "*.save", "*_save.*", "*.copy"
 ]

 backup_files = []
 for pattern in backup_patterns:
 for file_path in self.project_root.rglob(pattern):
 if file_path.is_file():
 backup_files.append({
 "path": str(file_path),
 "type": "backup_file",
 "pattern": pattern,
 "size": file_path.stat().st_size
 })

 return backup_files

 def _find_untracked_important_files(self) -> List[Dict[str, Any]]:
 """Find untracked files that might be important."""
 try:
 result = subprocess.run(
 ["git", "ls-files", "--others", "--exclude-standard"],
 cwd=self.project_root,
 capture_output=True,
 text=True
 )

 untracked_files = []
 if result.returncode == 0:
 for line in result.stdout.strip().split("\n"):
 if line:
 file_path = self.project_root / line
 if file_path.is_file():
 untracked_files.append({
 "path": str(file_path),
 "type": "untracked_file",
 "size": file_path.stat().st_size if file_path.exists() else 0
 })

 return untracked_files

 except subprocess.SubprocessError:
 return []

 def _should_ignore_file(self, file_path: Path) -> bool:
 """Check if file should be ignored in analysis."""
 ignore_patterns = [
 ".git/", "__pycache__/", ".pytest_cache/", "node_modules/",
 ".venv/", "venv/", ".env/", "env/"
 ]

 path_str = str(file_path)
 return any(pattern in path_str for pattern in ignore_patterns)

 def _get_size(self, path: Path) -> int:
 """Get size of file or directory."""
 if path.is_file():
 return path.stat().st_size
 elif path.is_dir():
 return sum(f.stat().st_size for f in path.rglob("*") if f.is_file())
 return 0

 def _calculate_mess_score(self, analysis: Dict[str, Any]) -> float:
 """Calculate overall mess score (0-100)."""
 score = 0

 # Weight different types of mess
 score += len(analysis["duplicate_files"]) * 5
 score += len(analysis["temp_files"]) * 2
 score += len(analysis["unused_files"]) * 3
 score += len(analysis["empty_directories"]) * 1
 score += len(analysis["debug_scripts"]) * 2
 score += len(analysis["backup_files"]) * 3

 # Large files add to mess
 total_large_size = sum(f["size"] for f in analysis["large_files"])
 score += total_large_size / (1024 * 1024 * 10) # 10MB = 1 point

 return min(score, 100) # Cap at 100

 def _generate_cleanup_recommendations(self, analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
 """Generate prioritized cleanup recommendations."""
 recommendations = []

 if analysis["duplicate_files"]:
 recommendations.append({
 "type": "remove_duplicates",
 "priority": "high",
 "description": f"Remove {len(analysis['duplicate_files'])} duplicate files",
 "impact": "disk_space",
 "files": analysis["duplicate_files"]
 })

 if analysis["temp_files"]:
 recommendations.append({
 "type": "remove_temp_files",
 "priority": "high",
 "description": f"Remove {len(analysis['temp_files'])} temporary files",
 "impact": "cleanliness",
 "files": analysis["temp_files"]
 })

 if analysis["backup_files"]:
 recommendations.append({
 "type": "remove_backup_files",
 "priority": "medium",
 "description": f"Remove {len(analysis['backup_files'])} backup files",
 "impact": "cleanliness",
 "files": analysis["backup_files"]
 })

 if analysis["debug_scripts"]:
 recommendations.append({
 "type": "review_debug_scripts",
 "priority": "medium",
 "description": f"Review {len(analysis['debug_scripts'])} debug scripts",
 "impact": "organization",
 "files": analysis["debug_scripts"]
 })

 if analysis["empty_directories"]:
 recommendations.append({
 "type": "remove_empty_dirs",
 "priority": "low",
 "description": f"Remove {len(analysis['empty_directories'])} empty directories",
 "impact": "cleanliness",
 "files": analysis["empty_directories"]
 })

 return recommendations

 def commit_cleanup_results(self, results: Dict[str, Any], cleanup_name: str):
 """Commit cleanup results with detailed message."""
 if not results["executed"]:
 print("No cleanup actions were executed - nothing to commit")
 return

 # Stage all changes
 subprocess.run(["git", "add", "-A"], cwd=self.project_root, check=True)

 # Create comprehensive commit message
 commit_msg = f"Cleanup: {cleanup_name}\n\n"
 commit_msg += f"Space saved: {results['space_saved'] / (1024*1024):.1f} MB\n"
 commit_msg += f"Actions executed: {len(results['executed'])}\n"
 commit_msg += f"Actions skipped: {len(results['skipped'])}\n\n"

 if results["executed"]:
 commit_msg += "Executed cleanup actions:\n"
 for action in results["executed"]:
 commit_msg += f"- {action['description']}\n"

 if results["errors"]:
 commit_msg += f"\nErrors encountered: {len(results['errors'])}\n"
 for error in results["errors"]:
 commit_msg += f"- {error['error']}\n"

 commit_msg += f"\nGenerated with vibelint cleanup workflow"

 # Commit changes
 subprocess.run(
 ["git", "commit", "-m", commit_msg],
 cwd=self.project_root,
 check=True
 )

 print(f"Committed cleanup results: {cleanup_name}")


def run_cleanup_workflow(project_root: str, cleanup_name: str = "general") -> Dict[str, Any]:
 """
 Main entry point for cleanup workflow.
 Human Decision Points throughout the process.
 """
 workflow = ProjectCleanupWorkflow(Path(project_root))

 print(f"Starting cleanup analysis for: {project_root}")

 # Step 1: Analyze project mess
 print("Analyzing project structure...")
 analysis = workflow.analyze_project_mess()

 print(f"Mess score: {analysis['mess_score']:.1f}/100")
 print(f"Found {len(analysis['recommendations'])} cleanup recommendations")

 # Step 2: Present recommendations to human
 print("\nCleanup Recommendations:")
 for i, rec in enumerate(analysis["recommendations"], 1):
 print(f"{i}. [{rec['priority'].upper()}] {rec['description']}")
 print(f" Impact: {rec['impact']}")

 # HUMAN DECISION POINT: Which recommendations to execute
 print("\nHUMAN DECISION REQUIRED:")
 print("Which cleanup actions would you like to execute?")
 print("Available types:", [rec["type"] for rec in analysis["recommendations"]])

 # For now, return analysis for human review
 # In interactive mode, human would approve specific actions
 return {
 "analysis": analysis,
 "workflow": workflow,
 "next_step": "human_approval_required"
 }
```

---
### File: src/vibelint/workflows/core/__init__.py

```python
"""
Core workflow system package.

Contains base classes, registry, and orchestration infrastructure.

Responsibility: Core workflow infrastructure only.
Workflow implementations belong in the implementations/ package.

vibelint/src/vibelint/workflow/core/__init__.py
"""

from .base import (BaseWorkflow, WorkflowConfig, WorkflowMetrics,
                   WorkflowPriority, WorkflowResult, WorkflowStatus)

__all__ = [
    "BaseWorkflow",
    "WorkflowResult",
    "WorkflowConfig",
    "WorkflowMetrics",
    "WorkflowStatus",
    "WorkflowPriority",
]
```

---
### File: src/vibelint/workflows/core/base.py

```python
"""
Base workflow system for extensible analysis tasks.

Provides framework for creating modular, composable workflows with
built-in evaluation, metrics collection, and plugin integration.

vibelint/src/vibelint/workflows/base.py
"""

import logging
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Set

logger = logging.getLogger(__name__)

__all__ = [
    "WorkflowStatus",
    "WorkflowPriority",
    "WorkflowConfig",
    "WorkflowMetrics",
    "WorkflowResult",
    "BaseWorkflow",
]


class WorkflowStatus(Enum):
    """Workflow execution status."""

    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"


class WorkflowPriority(Enum):
    """Workflow execution priority."""

    LOW = 1
    MEDIUM = 2
    HIGH = 3
    CRITICAL = 4


@dataclass
class WorkflowConfig:
    """Configuration for workflow execution."""

    # Basic settings
    enabled: bool = True
    priority: WorkflowPriority = WorkflowPriority.MEDIUM
    timeout_seconds: Optional[int] = None

    # Dependencies and requirements
    required_tools: Set[str] = field(default_factory=set)
    required_data: Set[str] = field(default_factory=set)
    dependencies: List[str] = field(default_factory=list)  # Other workflow IDs

    # Execution settings
    parallel_execution: bool = False
    max_retries: int = 0
    cache_results: bool = True

    # Input/output settings
    input_filters: Dict[str, Any] = field(default_factory=dict)
    output_format: str = "json"

    # Custom parameters
    parameters: Dict[str, Any] = field(default_factory=dict)


@dataclass
class WorkflowMetrics:
    """Metrics collected during workflow execution."""

    # Timing metrics
    start_time: float
    end_time: Optional[float] = None
    execution_time_seconds: Optional[float] = None

    # Resource metrics
    memory_usage_mb: Optional[float] = None
    cpu_usage_percent: Optional[float] = None

    # Analysis metrics
    files_processed: int = 0
    findings_generated: int = 0
    errors_encountered: int = 0

    # Quality metrics
    confidence_score: float = 0.0
    accuracy_score: Optional[float] = None
    coverage_percentage: Optional[float] = None

    # Custom metrics
    custom_metrics: Dict[str, Any] = field(default_factory=dict)

    def finalize(self):
        """Finalize metrics calculation."""
        if self.end_time and self.start_time:
            self.execution_time_seconds = self.end_time - self.start_time


@dataclass
class WorkflowResult:
    """Result of workflow execution."""

    # Execution info
    workflow_id: str
    status: WorkflowStatus
    metrics: WorkflowMetrics

    # Results
    findings: List[Dict[str, Any]] = field(default_factory=list)
    artifacts: Dict[str, Any] = field(default_factory=dict)
    recommendations: List[str] = field(default_factory=list)

    # Error handling
    error_message: Optional[str] = None
    warnings: List[str] = field(default_factory=list)

    # Metadata
    timestamp: str = ""
    version: str = "1.0"

    def __post_init__(self):
        if not self.timestamp:
            self.timestamp = time.strftime("%Y-%m-%d %H:%M:%S")


class BaseWorkflow(ABC):
    """Abstract base class for vibelint workflows."""

    # Workflow identification
    workflow_id: str = ""
    name: str = ""
    description: str = ""
    version: str = "1.0"

    # Workflow categorization
    category: str = "analysis"  # analysis, validation, reporting, maintenance
    tags: Set[str] = set()

    def __init__(self, config: Optional[WorkflowConfig] = None):
        """Initialize workflow with configuration."""
        self.config = config or WorkflowConfig()
        self.metrics = WorkflowMetrics(start_time=time.time())
        self._status = WorkflowStatus.PENDING

        # Validate workflow setup
        self._validate_configuration()

    @abstractmethod
    async def execute(self, project_root: Path, context: Dict[str, Any]) -> WorkflowResult:
        """Execute the workflow with given context.

        Args:
            project_root: Root directory of the project
            context: Execution context with shared data

        Returns:
            WorkflowResult with findings and artifacts
        """
        pass

    @abstractmethod
    def get_required_inputs(self) -> Set[str]:
        """Get set of required input data keys."""
        pass

    @abstractmethod
    def get_produced_outputs(self) -> Set[str]:
        """Get set of output data keys this workflow produces."""
        pass

    def can_execute(self, context: Dict[str, Any]) -> bool:
        """Check if workflow can execute with given context."""
        # Check if enabled
        if not self.config.enabled:
            return False

        # Check required inputs
        required_inputs = self.get_required_inputs()
        available_inputs = set(context.keys())

        if not required_inputs.issubset(available_inputs):
            missing = required_inputs - available_inputs
            logger.debug(f"Workflow {self.workflow_id} missing inputs: {missing}")
            return False

        # Check required tools
        if self.config.required_tools:
            # This would check for tool availability
            pass

        return True

    def estimate_execution_time(self, context: Dict[str, Any]) -> float:
        """Estimate execution time in seconds based on context."""
        # Default implementation - workflows can override
        base_time = 10.0  # 10 seconds base

        # Scale by number of files if available
        if "file_count" in context:
            file_count = context["file_count"]
            base_time += file_count * 0.1  # 0.1 seconds per file

        return base_time

    def get_dependencies(self) -> List[str]:
        """Get list of workflow IDs this workflow depends on."""
        return self.config.dependencies

    def get_priority(self) -> WorkflowPriority:
        """Get workflow execution priority."""
        return self.config.priority

    def supports_parallel_execution(self) -> bool:
        """Check if workflow supports parallel execution."""
        return self.config.parallel_execution

    def _validate_configuration(self):
        """Validate workflow configuration."""
        if not self.workflow_id:
            raise ValueError(f"Workflow {self.__class__.__name__} must define workflow_id")

        if not self.name:
            raise ValueError(f"Workflow {self.workflow_id} must define name")

    def _update_status(self, status: WorkflowStatus):
        """Update workflow execution status."""
        self._status = status

        if status == WorkflowStatus.COMPLETED:
            self.metrics.end_time = time.time()
            self.metrics.finalize()

    def _create_result(
        self,
        status: WorkflowStatus,
        findings: Optional[List[Dict[str, Any]]] = None,
        artifacts: Optional[Dict[str, Any]] = None,
        error_message: Optional[str] = None,
    ) -> WorkflowResult:
        """Create workflow result."""
        self._update_status(status)

        return WorkflowResult(
            workflow_id=self.workflow_id,
            status=status,
            metrics=self.metrics,
            findings=findings or [],
            artifacts=artifacts or {},
            error_message=error_message,
        )

    async def _execute_with_error_handling(
        self, project_root: Path, context: Dict[str, Any]
    ) -> WorkflowResult:
        """Execute workflow with comprehensive error handling."""
        try:
            self._update_status(WorkflowStatus.RUNNING)

            # Check timeout
            if self.config.timeout_seconds:
                # Implementation would use asyncio.wait_for
                pass

            # Execute the workflow
            result = await self.execute(project_root, context)

            # Validate result
            if result.status == WorkflowStatus.PENDING:
                result.status = WorkflowStatus.COMPLETED

            return result

        except Exception as e:
            logger.error(f"Workflow {self.workflow_id} failed: {e}", exc_info=True)
            self.metrics.errors_encountered += 1

            return self._create_result(WorkflowStatus.FAILED, error_message=str(e))

    def get_evaluation_criteria(self) -> Dict[str, Any]:
        """Get criteria for evaluating workflow effectiveness."""
        return {
            "performance": {
                "max_execution_time": 60.0,  # seconds
                "max_memory_usage": 500.0,  # MB
            },
            "quality": {
                "min_confidence_score": 0.7,
                "min_coverage_percentage": 80.0,
            },
            "reliability": {
                "max_error_rate": 0.05,  # 5%
                "max_timeout_rate": 0.01,  # 1%
            },
        }

    def __repr__(self) -> str:
        return f"{self.__class__.__name__}(id={self.workflow_id}, status={self._status.value})"
```

---
### File: src/vibelint/workflows/evaluation.py

```python
"""
Workflow evaluation framework for measuring effectiveness and performance.

Provides metrics collection, benchmarking, and continuous improvement
capabilities for workflow analysis quality.

vibelint/src/vibelint/workflow_evaluation.py
"""

import logging
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional

from .core.base import BaseWorkflow, WorkflowResult, WorkflowStatus

logger = logging.getLogger(__name__)

__all__ = ["WorkflowEvaluator", "EvaluationResult", "PerformanceMetrics", "QualityMetrics"]


@dataclass
class PerformanceMetrics:
    """Performance evaluation metrics."""

    execution_time_seconds: float
    memory_usage_mb: Optional[float] = None
    cpu_usage_percent: Optional[float] = None
    throughput_files_per_second: Optional[float] = None

    def meets_performance_criteria(self, criteria: Dict[str, float]) -> bool:
        """Check if performance meets specified criteria."""
        if "max_execution_time" in criteria:
            if self.execution_time_seconds > criteria["max_execution_time"]:
                return False

        if "max_memory_usage" in criteria and self.memory_usage_mb:
            if self.memory_usage_mb > criteria["max_memory_usage"]:
                return False

        return True


@dataclass
class QualityMetrics:
    """Quality evaluation metrics."""

    confidence_score: float
    accuracy_score: Optional[float] = None
    precision: Optional[float] = None
    recall: Optional[float] = None
    coverage_percentage: Optional[float] = None
    false_positive_rate: Optional[float] = None

    def meets_quality_criteria(self, criteria: Dict[str, float]) -> bool:
        """Check if quality meets specified criteria."""
        if "min_confidence_score" in criteria:
            if self.confidence_score < criteria["min_confidence_score"]:
                return False

        if "min_coverage_percentage" in criteria and self.coverage_percentage:
            if self.coverage_percentage < criteria["min_coverage_percentage"]:
                return False

        return True


@dataclass
class EvaluationResult:
    """Result of workflow evaluation."""

    workflow_id: str
    timestamp: str
    overall_score: float  # 0.0 to 1.0

    # Detailed metrics
    performance: PerformanceMetrics
    quality: QualityMetrics

    # Compliance checks
    meets_criteria: bool
    criteria_violations: List[str] = field(default_factory=list)

    # Recommendations
    recommendations: List[str] = field(default_factory=list)
    improvement_opportunities: List[str] = field(default_factory=list)

    # Comparison data
    baseline_comparison: Optional[Dict[str, float]] = None
    trend_analysis: Optional[Dict[str, Any]] = None


class WorkflowEvaluator:
    """Evaluates workflow effectiveness and performance."""

    def __init__(self):
        self.evaluation_history: Dict[str, List[EvaluationResult]] = {}
        self.baselines: Dict[str, Dict[str, float]] = {}

    def evaluate_workflow_execution(
        self, workflow: BaseWorkflow, result: WorkflowResult
    ) -> EvaluationResult:
        """Evaluate a completed workflow execution."""

        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")

        # Extract performance metrics
        performance = PerformanceMetrics(
            execution_time_seconds=result.metrics.execution_time_seconds or 0.0,
            memory_usage_mb=result.metrics.memory_usage_mb,
            cpu_usage_percent=result.metrics.cpu_usage_percent,
            throughput_files_per_second=self._calculate_throughput(result.metrics),
        )

        # Extract quality metrics
        quality = QualityMetrics(
            confidence_score=result.metrics.confidence_score,
            accuracy_score=result.metrics.accuracy_score,
            coverage_percentage=result.metrics.coverage_percentage,
        )

        # Get evaluation criteria
        criteria = workflow.get_evaluation_criteria()

        # Check compliance
        meets_criteria = True
        violations = []

        if not performance.meets_performance_criteria(criteria.get("performance", {})):
            meets_criteria = False
            violations.append("Performance criteria not met")

        if not quality.meets_quality_criteria(criteria.get("quality", {})):
            meets_criteria = False
            violations.append("Quality criteria not met")

        # Calculate overall score
        overall_score = self._calculate_overall_score(performance, quality, result)

        # Generate recommendations
        recommendations = self._generate_recommendations(workflow, performance, quality, result)

        # Create evaluation result
        evaluation = EvaluationResult(
            workflow_id=workflow.workflow_id,
            timestamp=timestamp,
            overall_score=overall_score,
            performance=performance,
            quality=quality,
            meets_criteria=meets_criteria,
            criteria_violations=violations,
            recommendations=recommendations,
        )

        # Add baseline comparison if available
        if workflow.workflow_id in self.baselines:
            evaluation.baseline_comparison = self._compare_to_baseline(
                workflow.workflow_id, performance, quality
            )

        # Store evaluation
        if workflow.workflow_id not in self.evaluation_history:
            self.evaluation_history[workflow.workflow_id] = []
        self.evaluation_history[workflow.workflow_id].append(evaluation)

        # Update baseline if this is a good execution
        if overall_score > 0.8 and meets_criteria:
            self._update_baseline(workflow.workflow_id, performance, quality)

        return evaluation

    def _calculate_throughput(self, metrics) -> Optional[float]:
        """Calculate files processed per second."""
        if metrics.execution_time_seconds and metrics.files_processed:
            if metrics.execution_time_seconds > 0:
                return metrics.files_processed / metrics.execution_time_seconds
        return None

    def _calculate_overall_score(
        self, performance: PerformanceMetrics, quality: QualityMetrics, result: WorkflowResult
    ) -> float:
        """Calculate overall workflow score."""

        # Base score from execution status
        if result.status == WorkflowStatus.COMPLETED:
            base_score = 0.6
        elif result.status == WorkflowStatus.FAILED:
            return 0.0
        else:
            base_score = 0.3

        # Quality contribution (40% weight)
        quality_score = quality.confidence_score * 0.4

        # Performance contribution (20% weight)
        # Penalize slow execution
        performance_score = 0.2
        if performance.execution_time_seconds > 0:
            # Assume target is 30 seconds, linear penalty after that
            if performance.execution_time_seconds <= 30:
                performance_score = 0.2
            else:
                performance_score = max(0.0, 0.2 * (60 - performance.execution_time_seconds) / 30)

        # Findings contribution (20% weight)
        findings_score = 0.0
        if result.metrics.findings_generated > 0:
            # More findings generally better, up to a point
            findings_score = min(0.2, result.metrics.findings_generated * 0.02)

        return min(1.0, base_score + quality_score + performance_score + findings_score)

    def _generate_recommendations(
        self,
        workflow: BaseWorkflow,
        performance: PerformanceMetrics,
        quality: QualityMetrics,
        result: WorkflowResult,
    ) -> List[str]:
        """Generate improvement recommendations."""

        recommendations = []

        # Performance recommendations
        if performance.execution_time_seconds > 60:
            recommendations.append("Consider optimizing workflow execution time")

        if performance.memory_usage_mb and performance.memory_usage_mb > 1000:
            recommendations.append("Consider reducing memory usage")

        # Quality recommendations
        if quality.confidence_score < 0.7:
            recommendations.append(
                "Consider improving analysis confidence through better heuristics"
            )

        if result.metrics.errors_encountered > 0:
            recommendations.append("Address error handling to improve reliability")

        # Findings recommendations
        if result.metrics.findings_generated == 0:
            recommendations.append("Verify workflow is detecting issues appropriately")

        if quality.coverage_percentage and quality.coverage_percentage < 80:
            recommendations.append("Improve analysis coverage of target files")

        return recommendations

    def _compare_to_baseline(
        self, workflow_id: str, performance: PerformanceMetrics, quality: QualityMetrics
    ) -> Dict[str, float]:
        """Compare current metrics to baseline."""

        baseline = self.baselines[workflow_id]
        comparison = {}

        if "execution_time" in baseline:
            comparison["execution_time_ratio"] = (
                performance.execution_time_seconds / baseline["execution_time"]
            )

        if "confidence_score" in baseline:
            comparison["confidence_improvement"] = (
                quality.confidence_score - baseline["confidence_score"]
            )

        return comparison

    def _update_baseline(
        self, workflow_id: str, performance: PerformanceMetrics, quality: QualityMetrics
    ):
        """Update baseline metrics for workflow."""

        if workflow_id not in self.baselines:
            self.baselines[workflow_id] = {}

        baseline = self.baselines[workflow_id]

        # Update with exponential smoothing
        alpha = 0.3  # Smoothing factor

        if "execution_time" in baseline:
            baseline["execution_time"] = (
                alpha * performance.execution_time_seconds
                + (1 - alpha) * baseline["execution_time"]
            )
        else:
            baseline["execution_time"] = performance.execution_time_seconds

        if "confidence_score" in baseline:
            baseline["confidence_score"] = (
                alpha * quality.confidence_score + (1 - alpha) * baseline["confidence_score"]
            )
        else:
            baseline["confidence_score"] = quality.confidence_score

        logger.debug(f"Updated baseline for workflow {workflow_id}")

    def get_workflow_trends(self, workflow_id: str, days: int = 30) -> Optional[Dict[str, Any]]:
        """Get trend analysis for workflow over specified period."""

        if workflow_id not in self.evaluation_history:
            return None

        evaluations = self.evaluation_history[workflow_id]
        if len(evaluations) < 2:
            return None

        # Simple trend analysis
        recent_evaluations = evaluations[-min(days, len(evaluations)) :]

        execution_times = [e.performance.execution_time_seconds for e in recent_evaluations]
        confidence_scores = [e.quality.confidence_score for e in recent_evaluations]
        overall_scores = [e.overall_score for e in recent_evaluations]

        trends = {
            "evaluation_count": len(recent_evaluations),
            "avg_execution_time": sum(execution_times) / len(execution_times),
            "avg_confidence_score": sum(confidence_scores) / len(confidence_scores),
            "avg_overall_score": sum(overall_scores) / len(overall_scores),
            "performance_trend": self._calculate_trend(execution_times),
            "quality_trend": self._calculate_trend(confidence_scores),
            "overall_trend": self._calculate_trend(overall_scores),
        }

        return trends

    def _calculate_trend(self, values: List[float]) -> str:
        """Calculate simple trend direction."""
        if len(values) < 2:
            return "insufficient_data"

        first_half = values[: len(values) // 2]
        second_half = values[len(values) // 2 :]

        first_avg = sum(first_half) / len(first_half)
        second_avg = sum(second_half) / len(second_half)

        if second_avg > first_avg * 1.05:
            return "improving"
        elif second_avg < first_avg * 0.95:
            return "degrading"
        else:
            return "stable"

    def generate_evaluation_report(
        self, workflow_ids: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """Generate comprehensive evaluation report."""

        if workflow_ids is None:
            workflow_ids = list(self.evaluation_history.keys())

        report = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "workflows_evaluated": len(workflow_ids),
            "workflow_summaries": {},
        }

        for workflow_id in workflow_ids:
            if workflow_id in self.evaluation_history:
                evaluations = self.evaluation_history[workflow_id]
                latest = evaluations[-1] if evaluations else None

                if latest:
                    trends = self.get_workflow_trends(workflow_id)

                    report["workflow_summaries"][workflow_id] = {
                        "latest_score": latest.overall_score,
                        "meets_criteria": latest.meets_criteria,
                        "execution_count": len(evaluations),
                        "recommendations": latest.recommendations,
                        "trends": trends,
                    }

        return report

    def export_evaluation_data(self, output_path: Path):
        """Export evaluation data for analysis."""
        import json

        export_data = {
            "evaluation_history": {},
            "baselines": self.baselines,
            "export_timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        }

        # Convert evaluation history to serializable format
        for workflow_id, evaluations in self.evaluation_history.items():
            export_data["evaluation_history"][workflow_id] = []
            for eval_result in evaluations:
                eval_dict = {
                    "workflow_id": eval_result.workflow_id,
                    "timestamp": eval_result.timestamp,
                    "overall_score": eval_result.overall_score,
                    "meets_criteria": eval_result.meets_criteria,
                    "performance": {
                        "execution_time_seconds": eval_result.performance.execution_time_seconds,
                        "memory_usage_mb": eval_result.performance.memory_usage_mb,
                        "throughput_files_per_second": eval_result.performance.throughput_files_per_second,
                    },
                    "quality": {
                        "confidence_score": eval_result.quality.confidence_score,
                        "coverage_percentage": eval_result.quality.coverage_percentage,
                    },
                    "recommendations": eval_result.recommendations,
                }
                export_data["evaluation_history"][workflow_id].append(eval_dict)

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(export_data, f, indent=2)

        logger.info(f"Evaluation data exported to {output_path}")
```

---
### File: src/vibelint/workflows/implementations/__init__.py

```python
"""
Workflow implementations package.

Contains all concrete workflow implementations organized by functionality.

Responsibility: Workflow implementation organization only.
Individual workflow logic belongs in specific implementation modules.

vibelint/src/vibelint/workflow/implementations/__init__.py
"""

# Import available implementations
from . import (coverage_analysis, justification, justification_analysis,
               redundancy_detection, single_file_validation)

# Re-export specific workflows for convenience
try:
    from .justification import FileJustificationWorkflow
    from .single_file_validation import SingleFileValidationWorkflow
except ImportError:
    # These might have import issues
    pass

__all__ = [
    # Implementation modules
    "justification",
    "coverage_analysis",
    "single_file_validation",
    "redundancy_detection",
    "justification_analysis",
]
```

---
### File: src/vibelint/workflows/implementations/coverage_analysis.py

```python
"""
Coverage analysis workflow for comprehensive test coverage evaluation.

AI-powered coverage analysis, edge case detection, and automated
test generation using dual LLM architecture. Analyzes coverage gaps
and suggests targeted improvements.

vibelint/src/vibelint/workflows/coverage_analysis.py
"""

import ast
import logging
import subprocess
import xml.etree.ElementTree as ET
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Set

from vibelint.workflows.core.base import (BaseWorkflow, WorkflowConfig,
                                          WorkflowResult, WorkflowStatus)

logger = logging.getLogger(__name__)

__all__ = ["CoverageAnalysisWorkflow", "CoverageGap", "EdgeCase", "CodeSuggestion"]


@dataclass
class CoverageGap:
    """Represents a coverage gap in the codebase."""

    file_path: str
    line_number: int
    function_name: Optional[str]
    coverage_percentage: float
    gap_type: str  # 'uncovered_line', 'missing_branch', 'error_path'
    description: str


@dataclass
class EdgeCase:
    """Represents an identified edge case that should be tested."""

    file_path: str
    function_name: str
    line_number: int
    case_type: str  # 'boundary', 'error_condition', 'race_condition', 'null_input'
    description: str
    suggested_test_data: Optional[Dict[str, Any]] = None


@dataclass
class CodeSuggestion:
    """Represents a generated code suggestion for improving coverage."""

    suggestion_name: str
    file_path: str
    function_under_analysis: str
    suggested_code: str
    priority: int  # 1-5, 5 being highest priority
    edge_cases_covered: List[str]
    estimated_coverage_increase: float


class CoverageAnalysisWorkflow(BaseWorkflow):
    """Workflow for AI-powered test coverage analysis and improvement."""

    workflow_id = "coverage-analysis"
    name = "Coverage Analysis"
    description = "Analyzes test coverage gaps and suggests targeted improvements using AI"
    category = "testing"
    tags = {"coverage", "testing", "ai", "quality"}

    def __init__(self, config: Optional[WorkflowConfig] = None):
        super().__init__(config)
        self.coverage_gaps: List[CoverageGap] = []
        self.edge_cases: List[EdgeCase] = []
        self.code_suggestions: List[CodeSuggestion] = []

    def get_required_inputs(self) -> Set[str]:
        """Get set of required input data keys."""
        return {"project_root"}

    def get_produced_outputs(self) -> Set[str]:
        """Get set of output data keys this workflow produces."""
        return {
            "coverage_gaps",
            "edge_cases",
            "code_suggestions",
            "coverage_metrics",
            "improvement_recommendations",
        }

    async def execute(self, project_root: Path, context: Dict[str, Any]) -> WorkflowResult:
        """Execute coverage analysis workflow."""

        logger.info("Starting AI-powered coverage analysis...")

        try:
            # Step 1: Run coverage analysis
            coverage_data = await self._run_coverage_analysis(project_root)

            # Step 2: Analyze gaps with AI
            if coverage_data:
                await self._analyze_coverage_gaps(coverage_data, project_root)

            # Step 3: Identify edge cases with AI
            await self._identify_edge_cases(project_root)

            # Step 4: Generate improvement suggestions
            await self._generate_improvement_suggestions(project_root)

            # Create findings
            findings = []

            # Add coverage gap findings
            for gap in self.coverage_gaps:
                severity = (
                    "BLOCK"
                    if gap.coverage_percentage < 0.5
                    else "WARN" if gap.coverage_percentage < 0.8 else "INFO"
                )
                findings.append(
                    {
                        "rule_id": "COVERAGE-GAP",
                        "severity": severity,
                        "message": f"Coverage gap in {gap.function_name or 'unknown function'}: {gap.coverage_percentage:.1%} coverage",
                        "file_path": gap.file_path,
                        "line": gap.line_number,
                        "suggestion": gap.description,
                    }
                )

            # Add edge case findings
            for edge_case in self.edge_cases:
                findings.append(
                    {
                        "rule_id": "EDGE-CASE-MISSING",
                        "severity": "INFO",
                        "message": f"Missing edge case test for {edge_case.function_name}: {edge_case.case_type}",
                        "file_path": edge_case.file_path,
                        "line": edge_case.line_number,
                        "suggestion": edge_case.description,
                    }
                )

            # Create artifacts
            artifacts = {
                "coverage_gaps": [self._gap_to_dict(g) for g in self.coverage_gaps],
                "edge_cases": [self._edge_case_to_dict(e) for e in self.edge_cases],
                "code_suggestions": [self._suggestion_to_dict(s) for s in self.code_suggestions],
                "coverage_metrics": coverage_data or {},
                "improvement_recommendations": self._generate_recommendations(),
            }

            # Update metrics
            self.metrics.files_processed = len(self._get_python_files(project_root))
            self.metrics.findings_generated = len(findings)
            self.metrics.confidence_score = self._calculate_confidence()

            return self._create_result(
                WorkflowStatus.COMPLETED, findings=findings, artifacts=artifacts
            )

        except Exception as e:
            logger.error(f"Coverage analysis failed: {e}", exc_info=True)
            return self._create_result(WorkflowStatus.FAILED, error_message=str(e))

    async def _run_coverage_analysis(self, project_root: Path) -> Optional[Dict[str, Any]]:
        """Run coverage analysis using pytest-cov."""
        try:
            # Look for test directories
            test_dirs = []
            for candidate in ["tests", "test"]:
                test_path = project_root / candidate
                if test_path.exists():
                    test_dirs.append(str(test_path))

            if not test_dirs:
                logger.warning("No test directories found, skipping coverage analysis")
                return None

            # Run coverage
            cmd = [
                "python",
                "-m",
                "pytest",
                "--cov=src",
                "--cov-report=xml",
                "--cov-report=term-missing",
                *test_dirs,
            ]

            result = subprocess.run(
                cmd, cwd=project_root, capture_output=True, text=True, timeout=300
            )

            # Parse coverage XML if available
            coverage_xml = project_root / "coverage.xml"
            if coverage_xml.exists():
                return self._parse_coverage_xml(coverage_xml)

            return {"stdout": result.stdout, "stderr": result.stderr}

        except subprocess.TimeoutExpired:
            logger.warning("Coverage analysis timed out")
            return None
        except Exception as e:
            logger.warning(f"Coverage analysis failed: {e}")
            return None

    def _parse_coverage_xml(self, coverage_xml_path: Path) -> Dict[str, Any]:
        """Parse coverage.xml file."""
        try:
            tree = ET.parse(coverage_xml_path)
            root = tree.getroot()

            coverage_data = {"overall_coverage": 0.0, "file_coverage": {}, "line_coverage": {}}

            # Extract overall coverage
            if root.attrib.get("line-rate"):
                coverage_data["overall_coverage"] = float(root.attrib["line-rate"])

            # Extract per-file coverage
            for package in root.findall(".//package"):
                for class_elem in package.findall("classes/class"):
                    filename = class_elem.attrib.get("filename", "")
                    line_rate = float(class_elem.attrib.get("line-rate", 0))

                    coverage_data["file_coverage"][filename] = line_rate

                    # Extract line-by-line coverage
                    lines_data = {}
                    for line in class_elem.findall("lines/line"):
                        line_num = int(line.attrib.get("number", 0))
                        hits = int(line.attrib.get("hits", 0))
                        lines_data[line_num] = hits > 0

                    coverage_data["line_coverage"][filename] = lines_data

            return coverage_data

        except Exception as e:
            logger.warning(f"Failed to parse coverage XML: {e}")
            return {}

    async def _analyze_coverage_gaps(self, coverage_data: Dict[str, Any], project_root: Path):
        """Analyze coverage gaps using AI."""
        if not coverage_data.get("file_coverage"):
            return

        # Find files with low coverage
        for file_path, coverage_rate in coverage_data["file_coverage"].items():
            if coverage_rate < 0.8:  # Less than 80% coverage
                full_path = project_root / file_path
                if full_path.exists():
                    try:
                        # Analyze the file to understand what's missing
                        content = full_path.read_text(encoding="utf-8")
                        gap = await self._analyze_file_coverage_gap(
                            file_path, content, coverage_rate
                        )
                        if gap:
                            self.coverage_gaps.append(gap)
                    except Exception as e:
                        logger.debug(f"Failed to analyze coverage gap for {file_path}: {e}")

    async def _analyze_file_coverage_gap(
        self, file_path: str, content: str, coverage_rate: float
    ) -> Optional[CoverageGap]:
        """Analyze specific file coverage gap."""
        try:
            tree = ast.parse(content)

            # Find the largest function (likely the main issue)
            largest_function = None
            max_lines = 0

            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    lines = node.end_lineno - node.lineno if hasattr(node, "end_lineno") else 10
                    if lines > max_lines:
                        max_lines = lines
                        largest_function = node

            if largest_function:
                return CoverageGap(
                    file_path=file_path,
                    line_number=largest_function.lineno,
                    function_name=largest_function.name,
                    coverage_percentage=coverage_rate,
                    gap_type="uncovered_function",
                    description=f"Function '{largest_function.name}' likely has uncovered code paths",
                )

        except Exception as e:
            logger.debug(f"Failed to analyze file {file_path}: {e}")

        return None

    async def _identify_edge_cases(self, project_root: Path):
        """Identify missing edge cases using AI analysis."""
        python_files = self._get_python_files(project_root)

        for file_path in python_files[:5]:  # Limit for testing
            try:
                content = file_path.read_text(encoding="utf-8")
                edge_cases = await self._analyze_file_for_edge_cases(file_path, content)
                self.edge_cases.extend(edge_cases)
            except Exception as e:
                logger.debug(f"Failed to analyze edge cases for {file_path}: {e}")

    async def _analyze_file_for_edge_cases(self, file_path: Path, content: str) -> List[EdgeCase]:
        """Analyze file for potential edge cases."""
        edge_cases = []

        try:
            tree = ast.parse(content)

            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    # Look for common edge case patterns
                    if any(arg.arg in ["data", "input", "value"] for arg in node.args.args):
                        edge_cases.append(
                            EdgeCase(
                                file_path=str(file_path.relative_to(file_path.parents[2])),
                                function_name=node.name,
                                line_number=node.lineno,
                                case_type="null_input",
                                description=f"Consider testing {node.name} with null/empty inputs",
                            )
                        )

                    # Look for division or mathematical operations
                    for child in ast.walk(node):
                        if isinstance(child, ast.BinOp) and isinstance(child.op, ast.Div):
                            edge_cases.append(
                                EdgeCase(
                                    file_path=str(file_path.relative_to(file_path.parents[2])),
                                    function_name=node.name,
                                    line_number=getattr(child, "lineno", node.lineno),
                                    case_type="division_by_zero",
                                    description=f"Test division by zero in {node.name}",
                                )
                            )

        except Exception as e:
            logger.debug(f"AST analysis failed for {file_path}: {e}")

        return edge_cases

    async def _generate_improvement_suggestions(self, project_root: Path):
        """Generate AI-powered improvement suggestions."""
        # For now, generate simple suggestions based on gaps found
        for gap in self.coverage_gaps:
            suggestion = CodeSuggestion(
                suggestion_name=f"Test for {gap.function_name}",
                file_path=gap.file_path,
                function_under_analysis=gap.function_name or "unknown",
                suggested_code=f"def test_{gap.function_name or 'function'}():\n    # TODO: Add test for {gap.description}",
                priority=3,
                edge_cases_covered=[gap.gap_type],
                estimated_coverage_increase=0.1,
            )
            self.code_suggestions.append(suggestion)

    def _generate_recommendations(self) -> List[str]:
        """Generate high-level recommendations."""
        recommendations = []

        if self.coverage_gaps:
            recommendations.append(
                f"Address {len(self.coverage_gaps)} coverage gaps to improve test quality"
            )

        if self.edge_cases:
            recommendations.append(f"Add tests for {len(self.edge_cases)} identified edge cases")

        if self.code_suggestions:
            recommendations.append(
                f"Consider implementing {len(self.code_suggestions)} suggested test improvements"
            )

        if not recommendations:
            recommendations.append("Coverage analysis complete - no major issues found")

        return recommendations

    def _calculate_confidence(self) -> float:
        """Calculate confidence in analysis results."""
        # Base confidence on presence of coverage data and number of findings
        base_confidence = 0.7

        if self.coverage_gaps:
            base_confidence += 0.1
        if self.edge_cases:
            base_confidence += 0.1
        if self.code_suggestions:
            base_confidence += 0.1

        return min(base_confidence, 1.0)

    def _get_python_files(self, project_root: Path) -> List[Path]:
        """Get Python files for analysis."""
        source_candidates = [project_root / "src", project_root]

        python_files = []
        for source_root in source_candidates:
            if source_root.exists():
                files = list(source_root.rglob("*.py"))
                python_files.extend(files)

        # Filter out test files and common non-source files
        filtered_files = []
        for file_path in python_files:
            if not any(
                skip in str(file_path) for skip in ["__pycache__", ".pytest_cache", "test", "tests"]
            ):
                filtered_files.append(file_path)

        return filtered_files

    def _gap_to_dict(self, gap: CoverageGap) -> Dict[str, Any]:
        """Convert CoverageGap to dictionary."""
        return {
            "file_path": gap.file_path,
            "line_number": gap.line_number,
            "function_name": gap.function_name,
            "coverage_percentage": gap.coverage_percentage,
            "gap_type": gap.gap_type,
            "description": gap.description,
        }

    def _edge_case_to_dict(self, edge_case: EdgeCase) -> Dict[str, Any]:
        """Convert EdgeCase to dictionary."""
        return {
            "file_path": edge_case.file_path,
            "function_name": edge_case.function_name,
            "line_number": edge_case.line_number,
            "case_type": edge_case.case_type,
            "description": edge_case.description,
            "suggested_test_data": edge_case.suggested_test_data,
        }

    def _suggestion_to_dict(self, suggestion: CodeSuggestion) -> Dict[str, Any]:
        """Convert CodeSuggestion to dictionary."""
        return {
            "suggestion_name": suggestion.suggestion_name,
            "file_path": suggestion.file_path,
            "function_under_analysis": suggestion.function_under_analysis,
            "suggested_code": suggestion.suggested_code,
            "priority": suggestion.priority,
            "edge_cases_covered": suggestion.edge_cases_covered,
            "estimated_coverage_increase": suggestion.estimated_coverage_increase,
        }
```

---
### File: src/vibelint/workflows/implementations/justification_analysis.py

```python
#!/usr/bin/env python3
"""
File and Method Justification Analysis Workflow

A comprehensive workflow that analyzes every file and method to justify their existence,
then uses embeddings to find similar justifications and identify redundancies.

This is implemented as a workflow, not validators, because it requires:
1. Cross-file analysis and coordination
2. State collection across multiple files
3. Embedding generation and similarity analysis
4. Multi-phase processing (collection → analysis → reporting)
"""

import ast
import json
import logging
from collections import defaultdict
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Set

import numpy as np

logger = logging.getLogger(__name__)

# Try to import sentence transformers for embeddings
try:
    from sentence_transformers import SentenceTransformer

    EMBEDDINGS_AVAILABLE = True
except ImportError:
    EMBEDDINGS_AVAILABLE = False
    logger.warning(
        "sentence-transformers not available. Install with: pip install sentence-transformers"
    )


@dataclass
class FileJustification:
    """Represents a file's existence justification."""

    file_path: str
    primary_purpose: str
    secondary_purposes: List[str]
    method_count: int
    class_count: int
    import_dependencies: List[str]
    exported_symbols: List[str]
    module_docstring: Optional[str]
    complexity_score: int
    purpose_embedding: Optional[List[float]] = None


@dataclass
class MethodJustification:
    """Represents a method's existence justification."""

    file_path: str
    method_name: str
    line_number: int
    docstring: Optional[str]
    complexity_score: int
    unique_functionality: str
    dependencies: List[str]
    return_type_hint: Optional[str]
    is_private: bool
    purpose_embedding: Optional[List[float]] = None


@dataclass
class RedundancyCluster:
    """Represents a cluster of redundant files or methods."""

    cluster_type: str  # 'file' or 'method'
    similarity_score: float
    items: List[str]  # file paths or method signatures
    common_purpose: str
    recommendation: str


class JustificationAnalysisWorkflow:
    """Workflow for comprehensive justification analysis."""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.similarity_threshold = self.config.get("similarity_threshold", 0.85)
        self.min_complexity_for_analysis = self.config.get("min_complexity", 2)

        # Initialize embedding model if available
        self.model = None
        if EMBEDDINGS_AVAILABLE:
            try:
                model_name = self.config.get("model_name", "all-MiniLM-L6-v2")
                self.model = SentenceTransformer(model_name)
                logger.info(f"Loaded embedding model: {model_name}")
            except Exception as e:
                logger.warning(f"Failed to load embedding model: {e}")

        # Data collection
        self.file_justifications: Dict[str, FileJustification] = {}
        self.method_justifications: List[MethodJustification] = []
        self.processed_files: Set[str] = set()

    def analyze_file(self, file_path: Path, content: str = None) -> Dict[str, Any]:
        """Analyze a single file for justification. This is the deterministic part."""

        # Determine if this is a Python file or other type
        if file_path.suffix == ".py" and content is not None:
            return self._analyze_python_file(file_path, content)
        else:
            return self._analyze_non_python_file(file_path)

    def _analyze_python_file(self, file_path: Path, content: str) -> Dict[str, Any]:
        """Analyze a Python file for justification."""
        try:
            tree = ast.parse(content)
        except SyntaxError as e:
            logger.debug(f"Syntax error in {file_path}: {e}")
            return {"error": f"Syntax error: {e}"}

        # Analyze file-level justification
        file_justification = self._analyze_file_justification(file_path, tree, content)

        # Analyze method-level justifications
        method_justifications = self._analyze_methods_in_file(file_path, tree, content)

        # Store for cross-file analysis
        self.file_justifications[str(file_path)] = file_justification
        self.method_justifications.extend(method_justifications)
        self.processed_files.add(str(file_path))

        return {
            "file_justification": asdict(file_justification),
            "method_justifications": [asdict(m) for m in method_justifications],
            "analysis_quality": self._assess_analysis_quality(
                file_justification, method_justifications
            ),
        }

    def _analyze_non_python_file(self, file_path: Path) -> Dict[str, Any]:
        """Analyze a non-Python file for justification."""
        file_justification = self._analyze_generic_file_justification(file_path)

        # Store for cross-file analysis
        self.file_justifications[str(file_path)] = file_justification
        self.processed_files.add(str(file_path))

        return {
            "file_justification": asdict(file_justification),
            "method_justifications": [],
            "analysis_quality": self._assess_generic_file_quality(file_justification),
        }

    def _analyze_file_justification(
        self, file_path: Path, tree: ast.AST, content: str
    ) -> FileJustification:
        """Analyze what justifies this file's existence."""

        # Extract module docstring
        module_docstring = ast.get_docstring(tree)

        # Count structural elements
        classes = [node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
        functions = [node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
        imports = [
            node for node in ast.walk(tree) if isinstance(node, (ast.Import, ast.ImportFrom))
        ]

        # Infer primary purpose
        primary_purpose = self._infer_primary_purpose(
            file_path, classes, functions, module_docstring
        )

        # Identify secondary purposes
        secondary_purposes = self._identify_secondary_purposes(classes, functions, content)

        # Extract dependencies and exports
        import_dependencies = self._extract_import_dependencies(imports)
        exported_symbols = self._extract_exported_symbols(tree)

        # Calculate complexity
        complexity_score = len(functions) + len(classes) * 2 + len(imports)

        # Generate embedding for purpose if model available
        purpose_embedding = None
        if self.model:
            purpose_text = self._create_purpose_text(
                file_path, primary_purpose, secondary_purposes, module_docstring
            )
            if purpose_text:
                try:
                    embedding = self.model.encode([purpose_text])[0]
                    purpose_embedding = embedding.tolist()
                except Exception as e:
                    logger.warning(f"Failed to generate embedding for {file_path}: {e}")

        return FileJustification(
            file_path=str(file_path),
            primary_purpose=primary_purpose,
            secondary_purposes=secondary_purposes,
            method_count=len(functions),
            class_count=len(classes),
            import_dependencies=import_dependencies,
            exported_symbols=exported_symbols,
            module_docstring=module_docstring,
            complexity_score=complexity_score,
            purpose_embedding=purpose_embedding,
        )

    def _analyze_methods_in_file(
        self, file_path: Path, tree: ast.AST, content: str
    ) -> List[MethodJustification]:
        """Analyze all methods in a file for their justification."""
        methods = []

        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                method_justification = self._analyze_single_method(file_path, node, content)
                if method_justification:
                    methods.append(method_justification)

        return methods

    def _analyze_single_method(
        self, file_path: Path, node: ast.FunctionDef, content: str
    ) -> Optional[MethodJustification]:
        """Analyze a single method's justification for existence."""

        # Calculate complexity
        complexity = self._calculate_method_complexity(node)

        # Skip trivial methods unless configured otherwise
        if complexity < self.min_complexity_for_analysis and not node.name.startswith("__"):
            return None

        # Extract method information
        docstring = ast.get_docstring(node)
        dependencies = self._extract_method_dependencies(node)
        unique_functionality = self._describe_unique_functionality(node, docstring)
        return_type_hint = self._extract_return_type_hint(node)
        is_private = node.name.startswith("_")

        # Generate embedding for method purpose
        purpose_embedding = None
        if self.model:
            purpose_text = self._create_method_purpose_text(
                node.name, docstring, unique_functionality
            )
            if purpose_text:
                try:
                    embedding = self.model.encode([purpose_text])[0]
                    purpose_embedding = embedding.tolist()
                except Exception as e:
                    logger.warning(f"Failed to generate embedding for {file_path}:{node.name}: {e}")

        return MethodJustification(
            file_path=str(file_path),
            method_name=node.name,
            line_number=node.lineno,
            docstring=docstring,
            complexity_score=complexity,
            unique_functionality=unique_functionality,
            dependencies=dependencies,
            return_type_hint=return_type_hint,
            is_private=is_private,
            purpose_embedding=purpose_embedding,
        )

    def _infer_primary_purpose(
        self,
        file_path: Path,
        classes: List[ast.ClassDef],
        functions: List[ast.FunctionDef],
        docstring: Optional[str],
    ) -> str:
        """Infer the primary purpose of a file."""

        filename = file_path.stem

        # Check for common patterns
        if filename.endswith("_test") or filename.startswith("test_"):
            return "testing"
        elif filename in ["__init__", "main", "__main__"]:
            return "module_initialization" if filename == "__init__" else "application_entry_point"
        elif filename.endswith("_config") or "config" in filename:
            return "configuration"
        elif filename.endswith("_utils") or filename == "utils":
            return "utilities"
        elif filename.endswith("_types") or filename == "types":
            return "type_definitions"
        elif len(classes) > len(functions) and classes:
            return "class_definitions"
        elif len(functions) > len(classes) and functions:
            return "functional_operations"
        elif docstring:
            return self._extract_purpose_from_docstring(docstring)
        else:
            return "unclear_purpose"

    def _identify_secondary_purposes(
        self, classes: List[ast.ClassDef], functions: List[ast.FunctionDef], content: str
    ) -> List[str]:
        """Identify secondary purposes beyond the primary file purpose."""
        secondary = []

        # Check for helper functions
        helper_funcs = [
            f for f in functions if f.name.startswith("_") and not f.name.startswith("__")
        ]
        if helper_funcs:
            secondary.append("helper_functions")

        # Check for error handling
        if "except" in content or "raise" in content:
            secondary.append("error_handling")

        # Check for constants
        if content.count("=") > len(functions) + len(classes):
            secondary.append("constant_definitions")

        # Check for decorators
        if "@" in content:
            secondary.append("decorators")

        return secondary

    def _calculate_method_complexity(self, node: ast.FunctionDef) -> int:
        """Calculate cyclomatic complexity of a method."""
        complexity = 1  # Base complexity

        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1
            elif isinstance(child, ast.ExceptHandler):
                complexity += 1
            elif isinstance(child, (ast.With, ast.AsyncWith)):
                complexity += 1

        return complexity

    def _describe_unique_functionality(
        self, node: ast.FunctionDef, docstring: Optional[str]
    ) -> str:
        """Describe what makes this method unique."""

        if docstring:
            # Extract first sentence of docstring
            first_sentence = docstring.split(".")[0].strip()
            if first_sentence and len(first_sentence) > 10:
                return first_sentence

        # Analyze method body for patterns
        patterns = []

        for child in ast.walk(node):
            if isinstance(child, ast.Return):
                patterns.append("returns_value")
            elif isinstance(child, ast.Yield):
                patterns.append("generator")
            elif isinstance(child, ast.Await):
                patterns.append("async_operation")
            elif isinstance(child, ast.Call) and isinstance(child.func, ast.Name):
                if child.func.id in ["print", "log"]:
                    patterns.append("logging")
                elif child.func.id in ["open", "read", "write"]:
                    patterns.append("file_io")

        base_desc = f"method_{node.name}"
        if patterns:
            base_desc += "_" + "_".join(patterns[:3])  # Limit to 3 patterns

        return base_desc

    def _analyze_generic_file_justification(self, file_path: Path) -> FileJustification:
        """Analyze what justifies a non-Python file's existence."""

        # Get file stats
        try:
            file_stats = file_path.stat()
            file_size = file_stats.st_size
        except (OSError, IOError):
            file_size = 0

        # Infer purpose from file type and location
        primary_purpose = self._infer_generic_file_purpose(file_path)
        secondary_purposes = self._identify_generic_secondary_purposes(file_path)

        # Try to read content for documentation/config files
        content_sample = self._get_content_sample(file_path)

        # Generate embedding if model available
        purpose_embedding = None
        if self.model and content_sample:
            purpose_text = self._create_generic_purpose_text(
                file_path, primary_purpose, content_sample
            )
            if purpose_text:
                try:
                    embedding = self.model.encode([purpose_text])[0]
                    purpose_embedding = embedding.tolist()
                except Exception as e:
                    logger.warning(f"Failed to generate embedding for {file_path}: {e}")

        return FileJustification(
            file_path=str(file_path),
            primary_purpose=primary_purpose,
            secondary_purposes=secondary_purposes,
            method_count=0,
            class_count=0,
            import_dependencies=[],
            exported_symbols=[],
            module_docstring=content_sample[:200] if content_sample else None,
            complexity_score=self._calculate_file_complexity(file_path, file_size),
            purpose_embedding=purpose_embedding,
        )

    def _infer_generic_file_purpose(self, file_path: Path) -> str:
        """Infer the purpose of a non-Python file."""

        filename = file_path.name.lower()
        suffix = file_path.suffix.lower()
        parent_dir = file_path.parent.name.lower()

        # Configuration files
        if filename in [
            "pyproject.toml",
            "setup.py",
            "setup.cfg",
            "requirements.txt",
            "pipfile",
            "poetry.lock",
            "package.json",
            "yarn.lock",
            "composer.json",
        ]:
            return "project_configuration"

        if filename in [
            ".gitignore",
            ".gitattributes",
            ".gitmodules",
            ".pre-commit-config.yaml",
            ".github",
            ".gitlab-ci.yml",
            "tox.ini",
            ".flake8",
            ".pylintrc",
        ]:
            return "development_tooling"

        if filename in ["dockerfile", "docker-compose.yml", "docker-compose.yaml", ".dockerignore"]:
            return "containerization"

        # Documentation
        if suffix in [".md", ".rst", ".txt"] and any(
            doc_word in filename
            for doc_word in [
                "readme",
                "changelog",
                "history",
                "license",
                "contributing",
                "install",
                "usage",
            ]
        ):
            return "documentation"

        if filename in ["license", "copying", "authors", "contributors", "notice"]:
            return "legal_compliance"

        # Data files
        if suffix in [".json", ".yaml", ".yml", ".toml", ".ini", ".cfg", ".conf"]:
            if parent_dir in ["config", "configs", "settings"]:
                return "configuration_data"
            elif parent_dir in ["data", "datasets", "fixtures"]:
                return "test_data"
            else:
                return "structured_data"

        if suffix in [".csv", ".tsv", ".xlsx", ".parquet", ".h5", ".hdf5"]:
            return "dataset"

        if suffix in [".sql", ".db", ".sqlite", ".sqlite3"]:
            return "database_schema"

        # Templates and static files
        if suffix in [".html", ".htm", ".css", ".js", ".jsx", ".ts", ".tsx", ".vue"]:
            return "web_frontend"

        if suffix in [".j2", ".jinja", ".jinja2", ".template", ".tmpl"]:
            return "template"

        # CI/CD and deployment
        if (
            parent_dir in [".github", ".gitlab", "ci", "deploy", "deployment"]
            or "workflow" in filename
            or "pipeline" in filename
        ):
            return "cicd_automation"

        # Logs
        if suffix in [".log", ".logs"] or parent_dir in ["logs", "log"]:
            return "logs"

        # Tests
        if parent_dir in ["test", "tests", "testing"] or "test" in filename:
            return "test_fixtures"

        # Scripts
        if suffix in [".sh", ".bash", ".zsh", ".fish", ".ps1", ".bat", ".cmd"]:
            return "automation_script"

        # Media/assets
        if suffix in [".png", ".jpg", ".jpeg", ".gif", ".svg", ".ico", ".webp"]:
            return "image_asset"

        if suffix in [".pdf", ".doc", ".docx", ".odt"]:
            return "document"

        # Fallback based on extension
        if suffix:
            return f"unknown_file_type_{suffix[1:]}"

        return "unknown_purpose"

    def _identify_generic_secondary_purposes(self, file_path: Path) -> List[str]:
        """Identify secondary purposes for generic files."""
        secondary = []

        # Check if it's in a hidden directory (development tooling)
        if any(part.startswith(".") for part in file_path.parts):
            secondary.append("hidden_development_file")

        # Check if it's a lock file (dependency management)
        if "lock" in file_path.name.lower():
            secondary.append("dependency_lock")

        # Check if it's executable
        try:
            if file_path.stat().st_mode & 0o111:  # Has execute permission
                secondary.append("executable")
        except (OSError, IOError):
            pass

        # Check if it's large (might be generated)
        try:
            if file_path.stat().st_size > 1024 * 1024:  # > 1MB
                secondary.append("large_file")
        except (OSError, IOError):
            pass

        return secondary

    def _get_content_sample(self, file_path: Path) -> Optional[str]:
        """Get a sample of file content for analysis."""
        try:
            # Only read text files, and only first 1KB
            if file_path.suffix.lower() in [
                ".md",
                ".txt",
                ".rst",
                ".yaml",
                ".yml",
                ".json",
                ".toml",
                ".ini",
                ".cfg",
                ".conf",
                ".xml",
                ".html",
            ]:
                with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                    return f.read(1024)  # First 1KB
        except (OSError, IOError, UnicodeDecodeError):
            pass
        return None

    def _create_generic_purpose_text(
        self, file_path: Path, primary_purpose: str, content_sample: str
    ) -> str:
        """Create text for embedding generation for generic files."""
        parts = [
            f"File: {file_path.name}",
            f"Type: {file_path.suffix}",
            f"Purpose: {primary_purpose}",
            f"Location: {file_path.parent.name}",
        ]

        if content_sample:
            # Extract meaningful content (first few lines)
            lines = content_sample.split("\n")[:5]
            content_preview = " ".join(line.strip() for line in lines if line.strip())
            if content_preview:
                parts.append(f"Content: {content_preview}")

        return " ".join(parts)

    def _calculate_file_complexity(self, file_path: Path, file_size: int) -> int:
        """Calculate complexity score for a generic file."""
        complexity = 0

        # Size-based complexity
        if file_size > 100 * 1024:  # > 100KB
            complexity += 3
        elif file_size > 10 * 1024:  # > 10KB
            complexity += 2
        elif file_size > 1024:  # > 1KB
            complexity += 1

        # Location-based complexity (deeper = more complex)
        depth = len(file_path.parts) - 1
        complexity += min(depth, 5)

        # Extension-based complexity
        if file_path.suffix.lower() in [".json", ".yaml", ".toml", ".xml"]:
            complexity += 2  # Structured data is more complex

        return complexity

    def _assess_generic_file_quality(self, file_justification: FileJustification) -> Dict[str, Any]:
        """Assess the quality of justification analysis for a generic file."""

        quality_score = 50  # Base score for non-Python files
        issues = []

        # Check for unclear purpose
        if file_justification.primary_purpose.startswith("unknown"):
            issues.append("unclear_file_purpose")
            quality_score -= 20

        # Check for appropriate location
        filename = Path(file_justification.file_path).name.lower()
        if (
            filename in ["readme.md", "license", "changelog.md"]
            and "root" not in str(file_justification.file_path).lower()
        ):
            issues.append("misplaced_project_file")
            quality_score -= 10

        # Check for configuration files without clear purpose
        if (
            "config" in file_justification.primary_purpose
            and not file_justification.module_docstring
        ):
            issues.append("undocumented_configuration")
            quality_score -= 15

        return {
            "quality_score": max(quality_score, 0),
            "issues": issues,
            "documentation_ratio": 1.0 if file_justification.module_docstring else 0.0,
            "complexity_assessment": (
                "high"
                if file_justification.complexity_score > 8
                else "medium" if file_justification.complexity_score > 4 else "low"
            ),
        }

    def _extract_method_dependencies(self, node: ast.FunctionDef) -> List[str]:
        """Extract what this method depends on."""
        dependencies = []

        for child in ast.walk(node):
            if isinstance(child, ast.Call) and isinstance(child.func, ast.Name):
                dependencies.append(child.func.id)
            elif isinstance(child, ast.Attribute):
                dependencies.append(child.attr)

        return list(set(dependencies))

    def _extract_return_type_hint(self, node: ast.FunctionDef) -> Optional[str]:
        """Extract return type hint if present."""
        if node.returns:
            return ast.unparse(node.returns) if hasattr(ast, "unparse") else str(node.returns)
        return None

    def _extract_import_dependencies(self, imports: List[ast.stmt]) -> List[str]:
        """Extract import dependencies."""
        dependencies = []

        for imp in imports:
            if isinstance(imp, ast.Import):
                dependencies.extend([alias.name for alias in imp.names])
            elif isinstance(imp, ast.ImportFrom) and imp.module:
                dependencies.append(imp.module)

        return dependencies

    def _extract_exported_symbols(self, tree: ast.AST) -> List[str]:
        """Extract symbols that this file exports."""
        exports = []

        # Look for __all__ definition
        for node in ast.walk(tree):
            if isinstance(node, ast.Assign):
                for target in node.targets:
                    if isinstance(target, ast.Name) and target.id == "__all__":
                        if isinstance(node.value, ast.List):
                            exports.extend(
                                [elt.s for elt in node.value.elts if isinstance(elt, ast.Str)]
                            )

        # If no __all__, extract public functions and classes
        if not exports:
            for node in ast.walk(tree):
                if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
                    if not node.name.startswith("_"):
                        exports.append(node.name)

        return exports

    def _extract_purpose_from_docstring(self, docstring: str) -> str:
        """Extract purpose from module docstring."""
        sentences = docstring.strip().split(".")
        for sentence in sentences:
            clean_sentence = sentence.strip()
            if len(clean_sentence) > 10:
                return clean_sentence.lower().replace(" ", "_")
        return "documented_module"

    def _create_purpose_text(
        self,
        file_path: Path,
        primary_purpose: str,
        secondary_purposes: List[str],
        docstring: Optional[str],
    ) -> str:
        """Create text for embedding generation."""
        parts = [f"File: {file_path.name}", f"Purpose: {primary_purpose}"]

        if secondary_purposes:
            parts.append(f"Secondary: {', '.join(secondary_purposes)}")

        if docstring:
            parts.append(docstring[:200])  # Limit docstring length

        return " ".join(parts)

    def _create_method_purpose_text(
        self, method_name: str, docstring: Optional[str], unique_functionality: str
    ) -> str:
        """Create text for method embedding generation."""
        parts = [f"Method: {method_name}"]

        if docstring:
            parts.append(docstring[:100])  # Limit docstring length

        parts.append(f"Functionality: {unique_functionality}")

        return " ".join(parts)

    def _assess_analysis_quality(
        self,
        file_justification: FileJustification,
        method_justifications: List[MethodJustification],
    ) -> Dict[str, Any]:
        """Assess the quality of justification analysis for this file."""

        quality_score = 0
        issues = []

        # File-level assessment
        if file_justification.module_docstring:
            quality_score += 20
        else:
            issues.append("missing_module_docstring")

        if not file_justification.primary_purpose.startswith("unclear"):
            quality_score += 20
        else:
            issues.append("unclear_primary_purpose")

        if file_justification.exported_symbols:
            quality_score += 10
        else:
            issues.append("no_clear_exports")

        # Method-level assessment
        documented_methods = sum(1 for m in method_justifications if m.docstring)
        if method_justifications:
            doc_ratio = documented_methods / len(method_justifications)
            quality_score += int(doc_ratio * 30)

            if doc_ratio < 0.5:
                issues.append("insufficient_method_documentation")

        # Complexity assessment
        if file_justification.complexity_score > 50:
            issues.append("high_complexity_file")

        complex_methods = sum(1 for m in method_justifications if m.complexity_score > 10)
        if complex_methods > len(method_justifications) * 0.3:
            issues.append("many_complex_methods")

        return {
            "quality_score": min(quality_score, 100),
            "issues": issues,
            "documentation_ratio": (
                documented_methods / len(method_justifications) if method_justifications else 0
            ),
            "complexity_assessment": (
                "high"
                if file_justification.complexity_score > 50
                else "medium" if file_justification.complexity_score > 20 else "low"
            ),
        }

    def find_redundancies(self) -> List[RedundancyCluster]:
        """Find redundancies across all analyzed files and methods."""

        clusters = []

        # Find file redundancies
        if self.model:
            clusters.extend(self._find_embedding_redundancies())

        clusters.extend(self._find_lexical_redundancies())

        return clusters

    def _find_embedding_redundancies(self) -> List[RedundancyCluster]:
        """Find redundancies using semantic embeddings."""

        clusters = []

        # File-level redundancies
        file_embeddings = []
        file_paths = []

        for file_path, justification in self.file_justifications.items():
            if justification.purpose_embedding:
                file_embeddings.append(justification.purpose_embedding)
                file_paths.append(file_path)

        if len(file_embeddings) > 1:
            file_clusters = self._cluster_by_similarity(file_embeddings, file_paths, "file")
            clusters.extend(file_clusters)

        # Method-level redundancies
        method_embeddings = []
        method_signatures = []

        for method in self.method_justifications:
            if method.purpose_embedding:
                method_embeddings.append(method.purpose_embedding)
                method_signatures.append(
                    f"{method.file_path}:{method.line_number}:{method.method_name}"
                )

        if len(method_embeddings) > 1:
            method_clusters = self._cluster_by_similarity(
                method_embeddings, method_signatures, "method"
            )
            clusters.extend(method_clusters)

        return clusters

    def _cluster_by_similarity(
        self, embeddings: List[List[float]], identifiers: List[str], cluster_type: str
    ) -> List[RedundancyCluster]:
        """Cluster items by embedding similarity."""

        clusters = []
        embeddings_array = np.array(embeddings)

        # Calculate cosine similarity matrix
        norms = np.linalg.norm(embeddings_array, axis=1)
        similarity_matrix = np.dot(embeddings_array, embeddings_array.T) / np.outer(norms, norms)

        used_indices = set()

        for i in range(len(embeddings)):
            if i in used_indices:
                continue

            # Find similar items
            similar_indices = []
            for j in range(i + 1, len(embeddings)):
                if similarity_matrix[i, j] >= self.similarity_threshold:
                    similar_indices.append(j)

            if similar_indices:
                cluster_items = [identifiers[i]] + [identifiers[j] for j in similar_indices]
                avg_similarity = np.mean([similarity_matrix[i, j] for j in similar_indices])

                # Extract common purpose
                common_purpose = self._extract_common_purpose_from_cluster(
                    cluster_items, cluster_type
                )

                # Generate recommendation
                recommendation = self._generate_redundancy_recommendation(
                    cluster_items, cluster_type, avg_similarity
                )

                cluster = RedundancyCluster(
                    cluster_type=cluster_type,
                    similarity_score=avg_similarity,
                    items=cluster_items,
                    common_purpose=common_purpose,
                    recommendation=recommendation,
                )

                clusters.append(cluster)

                used_indices.add(i)
                used_indices.update(similar_indices)

        return clusters

    def _find_lexical_redundancies(self) -> List[RedundancyCluster]:
        """Find redundancies using lexical analysis (fallback when no embeddings)."""

        clusters = []

        # Group files by primary purpose
        purpose_groups = defaultdict(list)
        for file_path, justification in self.file_justifications.items():
            if not justification.primary_purpose.startswith("unclear"):
                purpose_groups[justification.primary_purpose].append(file_path)

        # Report groups with multiple files
        for purpose, file_paths in purpose_groups.items():
            if len(file_paths) > 1:
                cluster = RedundancyCluster(
                    cluster_type="file",
                    similarity_score=1.0,  # Exact lexical match
                    items=file_paths,
                    common_purpose=purpose,
                    recommendation=f"Consider consolidating files with purpose '{purpose}'",
                )
                clusters.append(cluster)

        # Group methods by functionality description
        functionality_groups = defaultdict(list)
        for method in self.method_justifications:
            functionality_groups[method.unique_functionality].append(
                f"{method.file_path}:{method.line_number}:{method.method_name}"
            )

        # Report groups with multiple methods across different files
        for functionality, method_sigs in functionality_groups.items():
            if len(method_sigs) > 1:
                # Check if methods are in different files
                files = set(sig.split(":")[0] for sig in method_sigs)
                if len(files) > 1:
                    cluster = RedundancyCluster(
                        cluster_type="method",
                        similarity_score=1.0,  # Exact lexical match
                        items=method_sigs,
                        common_purpose=functionality,
                        recommendation=f"Consider extracting common functionality: {functionality}",
                    )
                    clusters.append(cluster)

        return clusters

    def _extract_common_purpose_from_cluster(self, items: List[str], cluster_type: str) -> str:
        """Extract common purpose from a cluster of similar items."""

        if cluster_type == "file":
            # For files, use the primary purpose of the first file
            first_file = items[0]
            if first_file in self.file_justifications:
                return self.file_justifications[first_file].primary_purpose

        elif cluster_type == "method":
            # For methods, find common functionality
            method_functionalities = []
            for item in items:
                file_path, line_num, method_name = item.split(":", 2)
                for method in self.method_justifications:
                    if method.file_path == file_path and method.method_name == method_name:
                        method_functionalities.append(method.unique_functionality)
                        break

            if method_functionalities:
                # Find common words
                all_words = []
                for func in method_functionalities:
                    all_words.extend(func.split("_"))

                word_counts = defaultdict(int)
                for word in all_words:
                    word_counts[word] += 1

                common_words = [
                    word
                    for word, count in word_counts.items()
                    if count >= len(method_functionalities) // 2
                ]

                return "_".join(common_words[:3]) if common_words else "similar_functionality"

        return "unknown_common_purpose"

    def _generate_redundancy_recommendation(
        self, items: List[str], cluster_type: str, similarity_score: float
    ) -> str:
        """Generate recommendation for addressing redundancy."""

        if similarity_score > 0.95:
            action = "merge or eliminate duplicate"
        elif similarity_score > 0.85:
            action = "consolidate similar"
        else:
            action = "review related"

        if cluster_type == "file":
            return f"{action.capitalize()} files: consider merging into single file"
        else:
            return f"{action.capitalize()} methods: consider extracting common functionality"

    def generate_report(self) -> Dict[str, Any]:
        """Generate comprehensive justification analysis report."""

        redundancies = self.find_redundancies()

        # Calculate statistics
        total_files = len(self.file_justifications)
        total_methods = len(self.method_justifications)

        files_with_unclear_purpose = sum(
            1 for f in self.file_justifications.values() if f.primary_purpose.startswith("unclear")
        )

        methods_without_docs = sum(
            1 for m in self.method_justifications if not m.docstring and m.complexity_score > 3
        )

        # Redundancy statistics
        file_redundancies = [r for r in redundancies if r.cluster_type == "file"]
        method_redundancies = [r for r in redundancies if r.cluster_type == "method"]

        return {
            "summary": {
                "total_files_analyzed": total_files,
                "total_methods_analyzed": total_methods,
                "files_with_unclear_purpose": files_with_unclear_purpose,
                "methods_without_documentation": methods_without_docs,
                "embedding_analysis_enabled": self.model is not None,
            },
            "justification_quality": {
                "files_needing_attention": files_with_unclear_purpose,
                "methods_needing_documentation": methods_without_docs,
                "documentation_coverage": (
                    (total_methods - methods_without_docs) / total_methods
                    if total_methods > 0
                    else 0
                ),
            },
            "redundancy_analysis": {
                "file_redundancy_clusters": len(file_redundancies),
                "method_redundancy_clusters": len(method_redundancies),
                "total_redundant_files": sum(len(r.items) for r in file_redundancies),
                "total_redundant_methods": sum(len(r.items) for r in method_redundancies),
            },
            "recommendations": self._generate_recommendations(redundancies),
            "detailed_redundancies": [asdict(r) for r in redundancies],
        }

    def _generate_recommendations(
        self, redundancies: List[RedundancyCluster]
    ) -> List[Dict[str, Any]]:
        """Generate actionable recommendations."""

        recommendations = []

        # High-priority redundancies
        high_priority = [r for r in redundancies if r.similarity_score > 0.9]
        if high_priority:
            recommendations.append(
                {
                    "priority": "high",
                    "action": "immediate_consolidation",
                    "description": f"Found {len(high_priority)} clusters with >90% similarity requiring immediate attention",
                    "affected_items": sum(len(r.items) for r in high_priority),
                }
            )

        # Files with unclear purpose
        unclear_files = [
            f for f in self.file_justifications.values() if f.primary_purpose.startswith("unclear")
        ]
        if unclear_files:
            recommendations.append(
                {
                    "priority": "medium",
                    "action": "clarify_purpose",
                    "description": f"Add clear documentation to {len(unclear_files)} files with unclear purpose",
                    "affected_items": len(unclear_files),
                }
            )

        # Complex methods without documentation
        complex_undocumented = [
            m for m in self.method_justifications if not m.docstring and m.complexity_score > 5
        ]
        if complex_undocumented:
            recommendations.append(
                {
                    "priority": "medium",
                    "action": "document_complex_methods",
                    "description": f"Add documentation to {len(complex_undocumented)} complex methods",
                    "affected_items": len(complex_undocumented),
                }
            )

        return recommendations

    def export_data(self, output_dir: Path) -> None:
        """Export analysis data for external processing."""

        output_dir.mkdir(parents=True, exist_ok=True)

        # Export file justifications
        with open(output_dir / "file_justifications.json", "w") as f:
            json.dump([asdict(fj) for fj in self.file_justifications.values()], f, indent=2)

        # Export method justifications
        with open(output_dir / "method_justifications.json", "w") as f:
            json.dump([asdict(mj) for mj in self.method_justifications], f, indent=2)

        # Export redundancy analysis
        redundancies = self.find_redundancies()
        with open(output_dir / "redundancy_clusters.json", "w") as f:
            json.dump([asdict(r) for r in redundancies], f, indent=2)

        # Export comprehensive report
        report = self.generate_report()
        with open(output_dir / "justification_report.json", "w") as f:
            json.dump(report, f, indent=2)

        logger.info(f"Exported justification analysis data to {output_dir}")
```

---
### File: src/vibelint/workflows/implementations/redundancy_detection.py

```python
"""
Redundancy detection workflow for vibelint codebase analysis.

Analyzes code redundancies and dead code patterns starting from CLI entry points
using dynamic analysis and static code inspection techniques.

vibelint/src/vibelint/workflows/redundancy_detection.py
"""

import ast
import logging
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

from .base import BaseWorkflow, WorkflowConfig, WorkflowResult, WorkflowStatus

logger = logging.getLogger(__name__)

__all__ = ["RedundancyDetectionWorkflow", "RedundancyPattern", "DeadCodeCandidate"]


@dataclass
class RedundancyPattern:
    """Represents a potentially redundant code pattern."""

    pattern_type: str  # "function", "class", "import", "logic_block"
    signature: str  # Normalized signature or pattern
    locations: List[Tuple[str, int]]  # (file_path, line_number)
    similarity_score: float  # 0.0 to 1.0
    estimated_redundancy: str  # "duplicate", "similar", "refactorable"


@dataclass
class DeadCodeCandidate:
    """Represents potentially dead code."""

    code_type: str  # "function", "class", "import"
    name: str
    file_path: str
    line_number: int
    reason: str  # Why it's considered dead
    confidence: float  # 0.0 to 1.0


class RedundancyDetectionWorkflow(BaseWorkflow):
    """Workflow for detecting code redundancies and dead code from CLI entry points."""

    workflow_id = "redundancy-detection"
    name = "Redundancy Detection"
    description = "Analyzes code redundancies and dead code patterns from CLI entry points"
    category = "maintenance"
    tags = {"redundancy", "dead_code", "cleanup", "maintenance"}

    def __init__(self, config: Optional[WorkflowConfig] = None):
        super().__init__(config)

        # Analysis state
        self.all_functions = {}  # signature -> locations
        self.all_classes = {}  # signature -> locations
        self.import_usage = defaultdict(set)  # module -> using_files
        self.function_calls = defaultdict(set)  # function -> calling_functions
        self.entry_points = []
        self.cli_commands = []

    def get_required_inputs(self) -> Set[str]:
        """Get set of required input data keys."""
        return {"project_root"}

    def get_produced_outputs(self) -> Set[str]:
        """Get set of output data keys this workflow produces."""
        return {
            "redundancy_patterns",
            "dead_code_candidates",
            "import_redundancies",
            "consolidation_opportunities",
            "removal_benefit_estimate",
        }

    async def execute(self, project_root: Path, context: Dict[str, Any]) -> WorkflowResult:
        """Execute redundancy detection workflow."""

        logger.info("Starting redundancy detection from CLI entry points...")

        try:
            # Step 1: Map CLI entry points
            self._discover_cli_entry_points(project_root)

            # Step 2: Trace code paths from entry points
            reachable_code = self._trace_reachable_code(project_root)

            # Step 3: Find dead code candidates
            dead_code = self._find_dead_code_candidates(project_root, reachable_code)

            # Step 4: Detect redundant patterns
            redundant_patterns = self._detect_redundant_patterns(project_root)

            # Step 5: Analyze import redundancies
            import_redundancies = self._analyze_import_redundancies(project_root)

            # Step 6: Find consolidation opportunities
            consolidation_opportunities = self._find_consolidation_opportunities(project_root)

            # Step 7: Estimate removal benefits
            removal_benefits = self._estimate_removal_benefit(dead_code, redundant_patterns)

            # Create findings
            findings = []

            # Add dead code findings
            for candidate in dead_code:
                findings.append(
                    {
                        "rule_id": "DEAD-CODE-CANDIDATE",
                        "severity": "INFO",
                        "message": f"Potentially dead {candidate.code_type}: {candidate.name}",
                        "file_path": candidate.file_path,
                        "line": candidate.line_number,
                        "suggestion": f"Consider removing if truly unused. Reason: {candidate.reason}",
                        "confidence": candidate.confidence,
                    }
                )

            # Add redundancy findings
            for pattern in redundant_patterns:
                findings.append(
                    {
                        "rule_id": "REDUNDANT-PATTERN",
                        "severity": "WARN" if pattern.similarity_score > 0.8 else "INFO",
                        "message": f"Redundant {pattern.pattern_type} pattern found in {len(pattern.locations)} locations",
                        "locations": pattern.locations,
                        "suggestion": f"Consider consolidating similar {pattern.pattern_type}s",
                        "similarity_score": pattern.similarity_score,
                    }
                )

            # Create artifacts
            artifacts = {
                "redundancy_patterns": [self._pattern_to_dict(p) for p in redundant_patterns],
                "dead_code_candidates": [self._candidate_to_dict(c) for c in dead_code],
                "import_redundancies": import_redundancies,
                "consolidation_opportunities": consolidation_opportunities,
                "removal_benefit_estimate": removal_benefits,
                "entry_points_traced": self.entry_points,
                "cli_commands_found": self.cli_commands,
            }

            # Update metrics
            self.metrics.files_processed = len(self._get_all_python_files(project_root))
            self.metrics.findings_generated = len(findings)
            self.metrics.confidence_score = self._calculate_overall_confidence(
                dead_code, redundant_patterns
            )

            return self._create_result(
                WorkflowStatus.COMPLETED, findings=findings, artifacts=artifacts
            )

        except Exception as e:
            logger.error(f"Redundancy detection failed: {e}", exc_info=True)
            return self._create_result(WorkflowStatus.FAILED, error_message=str(e))

    def _discover_cli_entry_points(self, project_root: Path):
        """Discover CLI entry points from setup and CLI modules."""
        logger.debug("Discovering CLI entry points...")

        # Check pyproject.toml for script entries
        pyproject_path = project_root / "pyproject.toml"
        if pyproject_path.exists():
            self._parse_pyproject_entry_points(pyproject_path)

        # Analyze cli.py for command handlers
        source_root = project_root / "src"
        cli_candidates = [
            source_root / "vibelint" / "cli.py",
            project_root / "cli.py",
            project_root / "main.py",
        ]

        for cli_path in cli_candidates:
            if cli_path.exists():
                self._analyze_cli_module(cli_path)

    def _parse_pyproject_entry_points(self, pyproject_path: Path):
        """Parse entry points from pyproject.toml."""
        try:
            import sys

            if sys.version_info >= (3, 11):
                import tomllib

                with open(pyproject_path, "rb") as f:
                    config = tomllib.load(f)
            else:
                import tomli

                content = pyproject_path.read_text(encoding="utf-8")
                config = tomli.loads(content)

            # Script entry points
            scripts = config.get("project", {}).get("scripts", {})
            for script_name, entry_point in scripts.items():
                self.entry_points.append(f"script:{script_name}={entry_point}")

            # Plugin entry points
            entry_points = config.get("project", {}).get("entry-points", {})
            for group, entries in entry_points.items():
                for name, entry_point in entries.items():
                    self.entry_points.append(f"plugin:{group}:{name}={entry_point}")

        except Exception as e:
            logger.warning(f"Failed to parse pyproject.toml: {e}")

    def _analyze_cli_module(self, cli_path: Path):
        """Analyze CLI module for command handlers."""
        try:
            content = cli_path.read_text(encoding="utf-8")
            tree = ast.parse(content)

            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    # Look for click command decorators
                    for decorator in node.decorator_list:
                        if self._is_click_command(decorator):
                            command_name = node.name
                            self.cli_commands.append(command_name)

        except Exception as e:
            logger.warning(f"Failed to analyze CLI module: {e}")

    def _is_click_command(self, decorator) -> bool:
        """Check if decorator is a click command."""
        if isinstance(decorator, ast.Call):
            if isinstance(decorator.func, ast.Attribute):
                return decorator.func.attr in ["command", "group"]
            elif isinstance(decorator.func, ast.Name):
                return decorator.func.id in ["command", "group"]
        return False

    def _trace_reachable_code(self, project_root: Path) -> Set[str]:
        """Trace all code reachable from CLI entry points."""
        logger.debug("Tracing reachable code from entry points...")

        reachable = set()

        # Start from CLI command handlers
        for command_name in self.cli_commands:
            reachable.add(f"function:{command_name}")

        # Add entry point functions
        for entry_point in self.entry_points:
            if "=" in entry_point:
                module_func = entry_point.split("=")[1]
                reachable.add(f"entry:{module_func}")

        # Simple heuristic: if we found CLI commands, assume moderate reachability
        # Full implementation would require building complete call graph
        return reachable

    def _find_dead_code_candidates(
        self, project_root: Path, reachable_code: Set[str]
    ) -> List[DeadCodeCandidate]:
        """Find code that appears to be unreachable from entry points."""
        logger.debug("Finding dead code candidates...")

        dead_code = []
        all_files = self._get_all_python_files(project_root)

        for file_path in all_files:
            try:
                content = file_path.read_text(encoding="utf-8")
                tree = ast.parse(content)
                relative_path = str(file_path.relative_to(project_root))

                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef):
                        func_id = f"function:{node.name}"
                        if func_id not in reachable_code:
                            if not self._is_exempt_function(node, file_path):
                                dead_code.append(
                                    DeadCodeCandidate(
                                        code_type="function",
                                        name=node.name,
                                        file_path=relative_path,
                                        line_number=node.lineno,
                                        reason="unreachable_from_entry_points",
                                        confidence=0.6,  # Moderate confidence
                                    )
                                )

                    elif isinstance(node, ast.ClassDef):
                        class_id = f"class:{node.name}"
                        if class_id not in reachable_code:
                            if not self._is_exempt_class(node, file_path):
                                dead_code.append(
                                    DeadCodeCandidate(
                                        code_type="class",
                                        name=node.name,
                                        file_path=relative_path,
                                        line_number=node.lineno,
                                        reason="unreachable_from_entry_points",
                                        confidence=0.5,  # Lower confidence for classes
                                    )
                                )

            except Exception as e:
                logger.debug(f"Failed to analyze {file_path}: {e}")

        return dead_code

    def _is_exempt_function(self, node: ast.FunctionDef, file_path: Path) -> bool:
        """Check if function should be exempt from dead code detection."""
        # Test functions
        if "test" in file_path.name or node.name.startswith("test_"):
            return True

        # Private functions (might be used dynamically)
        if node.name.startswith("_"):
            return True

        # Special methods
        if node.name.startswith("__") and node.name.endswith("__"):
            return True

        # Entry point functions
        if node.name in ["main", "cli"]:
            return True

        return False

    def _is_exempt_class(self, node: ast.ClassDef, file_path: Path) -> bool:
        """Check if class should be exempt from dead code detection."""
        # Test classes
        if "test" in file_path.name or node.name.startswith("Test"):
            return True

        # Exception classes
        if node.name.endswith("Error") or node.name.endswith("Exception"):
            return True

        return False

    def _detect_redundant_patterns(self, project_root: Path) -> List[RedundancyPattern]:
        """Detect redundant code patterns across the codebase."""
        logger.debug("Detecting redundant patterns...")

        patterns = []
        function_signatures = defaultdict(list)

        all_files = self._get_all_python_files(project_root)

        # Collect function signatures
        for file_path in all_files:
            try:
                content = file_path.read_text(encoding="utf-8")
                tree = ast.parse(content)
                relative_path = str(file_path.relative_to(project_root))

                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef):
                        # Create normalized signature
                        signature = self._normalize_function_signature(node)
                        function_signatures[signature].append(
                            (relative_path, node.lineno, node.name)
                        )

            except Exception as e:
                logger.debug(f"Failed to analyze {file_path}: {e}")

        # Find patterns with multiple occurrences
        for signature, locations in function_signatures.items():
            if len(locations) > 1:
                # Calculate similarity score based on signature complexity
                similarity_score = self._calculate_similarity_score(signature, locations)

                if similarity_score > 0.6:  # Reasonable similarity threshold
                    patterns.append(
                        RedundancyPattern(
                            pattern_type="function",
                            signature=signature,
                            locations=[(loc[0], loc[1]) for loc in locations],
                            similarity_score=similarity_score,
                            estimated_redundancy=(
                                "duplicate" if similarity_score > 0.9 else "similar"
                            ),
                        )
                    )

        return patterns

    def _normalize_function_signature(self, node: ast.FunctionDef) -> str:
        """Create normalized signature for function comparison."""
        args = []
        for arg in node.args.args:
            args.append(arg.arg)

        body_structure = []
        for stmt in node.body[:3]:  # Only look at first few statements
            body_structure.append(type(stmt).__name__)

        return f"{len(args)}args:{':'.join(body_structure)}"

    def _calculate_similarity_score(self, signature: str, locations: List[Tuple]) -> float:
        """Calculate similarity score for function pattern."""
        base_score = 0.4

        # More locations = higher redundancy potential
        location_score = min(len(locations) * 0.15, 0.3)

        # Complex signatures more likely to be actual redundancy
        complexity_score = min(len(signature) * 0.01, 0.2)

        return min(base_score + location_score + complexity_score, 1.0)

    def _analyze_import_redundancies(self, project_root: Path) -> List[Dict[str, Any]]:
        """Find redundant or unused imports."""
        logger.debug("Analyzing import redundancies...")

        import_redundancies = []
        import_usage = defaultdict(set)
        all_imports = defaultdict(list)

        all_files = self._get_all_python_files(project_root)

        # Collect all imports and usage
        for file_path in all_files:
            try:
                content = file_path.read_text(encoding="utf-8")
                tree = ast.parse(content)
                relative_path = str(file_path.relative_to(project_root))

                # Track imports
                for node in ast.walk(tree):
                    if isinstance(node, ast.Import):
                        for alias in node.names:
                            module_name = alias.name
                            all_imports[module_name].append((relative_path, node.lineno))

                    elif isinstance(node, ast.ImportFrom):
                        if node.module:
                            for alias in node.names:
                                import_name = f"{node.module}.{alias.name}"
                                all_imports[import_name].append((relative_path, node.lineno))

                # Track usage (simplified analysis)
                for line in content.split("\n"):
                    for module_name in all_imports.keys():
                        if module_name.split(".")[-1] in line:
                            import_usage[module_name].add(relative_path)

            except Exception as e:
                logger.debug(f"Failed to analyze imports in {file_path}: {e}")

        # Find redundancies
        for module_name, locations in all_imports.items():
            if len(locations) > 1:
                unused_locations = []
                for file_path, line_no in locations:
                    if file_path not in import_usage[module_name]:
                        unused_locations.append((file_path, line_no))

                if unused_locations:
                    import_redundancies.append(
                        {
                            "module": module_name,
                            "unused_locations": unused_locations,
                            "total_imports": len(locations),
                            "estimated_savings": f"{len(unused_locations)} unused imports",
                        }
                    )

        return import_redundancies

    def _find_consolidation_opportunities(self, project_root: Path) -> List[Dict[str, Any]]:
        """Find opportunities to consolidate similar code."""
        opportunities = []

        # Look for similar file structures
        file_purposes = defaultdict(list)
        all_files = self._get_all_python_files(project_root)

        for file_path in all_files:
            purpose = self._infer_file_purpose(file_path)
            relative_path = str(file_path.relative_to(project_root))
            file_purposes[purpose].append(relative_path)

        # Find consolidation opportunities
        for purpose, files in file_purposes.items():
            if len(files) > 3 and purpose not in ["testing", "unknown"]:
                opportunities.append(
                    {
                        "type": "module_consolidation",
                        "purpose": purpose,
                        "files": files,
                        "suggestion": f"Consider consolidating {purpose} files into subpackage",
                        "estimated_benefit": f"Reduce {len(files)} files to organized submodule",
                    }
                )

        return opportunities

    def _estimate_removal_benefit(
        self, dead_code: List[DeadCodeCandidate], redundant_patterns: List[RedundancyPattern]
    ) -> Dict[str, Any]:
        """Estimate benefits of removing dead/redundant code."""
        total_dead_functions = len([d for d in dead_code if d.code_type == "function"])
        total_dead_classes = len([d for d in dead_code if d.code_type == "class"])
        total_redundant_patterns = len(redundant_patterns)

        # Rough estimation
        estimated_lines_saved = (total_dead_functions * 10) + (total_dead_classes * 20)
        for pattern in redundant_patterns:
            estimated_lines_saved += len(pattern.locations) * 5

        return {
            "dead_functions": total_dead_functions,
            "dead_classes": total_dead_classes,
            "redundant_patterns": total_redundant_patterns,
            "estimated_lines_saved": estimated_lines_saved,
            "estimated_files_reducible": len(set(d.file_path for d in dead_code)),
            "maintainability_improvement": "Medium" if estimated_lines_saved > 100 else "Low",
        }

    def _get_all_python_files(self, project_root: Path) -> List[Path]:
        """Get all Python files in the project."""
        source_candidates = [project_root / "src", project_root]

        python_files = []
        for source_root in source_candidates:
            if source_root.exists():
                files = list(source_root.rglob("*.py"))
                python_files.extend(files)

        # Filter out common non-code files
        filtered_files = []
        for file_path in python_files:
            if not any(
                skip in str(file_path) for skip in ["__pycache__", ".pytest_cache", "build", "dist"]
            ):
                filtered_files.append(file_path)

        return filtered_files

    def _infer_file_purpose(self, file_path: Path) -> str:
        """Infer the purpose of a file from its name and location."""
        name = file_path.name.lower()
        path_parts = file_path.parts

        if "test" in name or "tests" in path_parts:
            return "testing"
        elif name == "__init__.py":
            return "package_init"
        elif name in ["cli.py", "main.py", "__main__.py"]:
            return "entry_point"
        elif any(keyword in name for keyword in ["util", "helper", "tool"]):
            return "utility"
        elif any(keyword in name for keyword in ["config", "settings"]):
            return "configuration"
        elif any(keyword in name for keyword in ["validator", "check", "lint"]):
            return "validation"
        elif any(keyword in name for keyword in ["report", "format"]):
            return "reporting"
        elif any(keyword in name for keyword in ["llm", "ai", "model"]):
            return "ai_integration"
        else:
            return "unknown"

    def _calculate_overall_confidence(
        self, dead_code: List[DeadCodeCandidate], redundant_patterns: List[RedundancyPattern]
    ) -> float:
        """Calculate overall confidence score for the analysis."""
        if not dead_code and not redundant_patterns:
            return 1.0

        total_confidence = 0.0
        total_items = 0

        for candidate in dead_code:
            total_confidence += candidate.confidence
            total_items += 1

        for pattern in redundant_patterns:
            total_confidence += pattern.similarity_score
            total_items += 1

        return total_confidence / total_items if total_items > 0 else 0.0

    def _pattern_to_dict(self, pattern: RedundancyPattern) -> Dict[str, Any]:
        """Convert RedundancyPattern to dictionary."""
        return {
            "pattern_type": pattern.pattern_type,
            "signature": pattern.signature,
            "locations": pattern.locations,
            "similarity_score": pattern.similarity_score,
            "estimated_redundancy": pattern.estimated_redundancy,
        }

    def _candidate_to_dict(self, candidate: DeadCodeCandidate) -> Dict[str, Any]:
        """Convert DeadCodeCandidate to dictionary."""
        return {
            "code_type": candidate.code_type,
            "name": candidate.name,
            "file_path": candidate.file_path,
            "line_number": candidate.line_number,
            "reason": candidate.reason,
            "confidence": candidate.confidence,
        }
```

---
### File: src/vibelint/workflows/implementations/single_file_validation.py

```python
"""
Workflow 1: Single File Validation

Implements the single file validation workflow that takes a Python file and runs
validation checks against it, returning structured violation reports with actionable feedback.

This follows the requirements defined in WORKFLOW_1_REQUIREMENTS.md and the workflow
specification in VIBELINT_WORKFLOWS.md.
"""

import ast
import logging
from pathlib import Path
from typing import Dict, List, Optional

from ..config import Config
from ..plugin_system import Finding, Severity, plugin_manager
from ..rules import RuleEngine

logger = logging.getLogger(__name__)


class SingleFileValidationResult:
    """Results from single file validation workflow."""

    def __init__(self):
        self.file_path: Optional[Path] = None
        self.success: bool = False
        self.violations: List[Finding] = []
        self.health_score: float = 0.0
        self.error_message: Optional[str] = None
        self.ast_tree: Optional[ast.AST] = None
        self.execution_time_ms: float = 0.0


class SingleFileValidator:
    """Core single file validation functionality."""

    def __init__(self, config: Config):
        self.config = config
        self.rule_engine = RuleEngine(config.settings)

        # Load single file validators
        plugin_manager.load_plugins()
        self.validators = self._load_single_file_validators()

    def _load_single_file_validators(self) -> Dict[str, type]:
        """Load validators from src/vibelint/validators/single_file/."""
        validators = {}
        all_validators = plugin_manager.get_all_validators()

        for rule_id, validator_class in all_validators.items():
            module_name = validator_class.__module__
            # Filter for single file validators
            if "validators.single_file" in module_name:
                validators[rule_id] = validator_class
                logger.debug(f"Loaded single file validator: {rule_id}")

        return validators

    def validate_file(self, file_path: Path) -> SingleFileValidationResult:
        """
        Validate a single Python file.

        Implements the workflow activities from VIBELINT_WORKFLOWS.md:
        1. File Loading - Read and validate file exists
        2. Single File Validator Execution - Run validators
        3. Violation Aggregation - Collect and sort violations
        4. Report Generation - Generate structured output
        """
        import time

        start_time = time.time()

        result = SingleFileValidationResult()
        result.file_path = file_path

        try:
            # Activity 1: File Loading
            if not self._validate_file_exists(file_path):
                result.error_message = f"File not found or not readable: {file_path}"
                return result

            # Parse Python AST
            ast_tree = self._parse_python_file(file_path)
            if ast_tree is None:
                result.error_message = f"Failed to parse Python AST for: {file_path}"
                return result

            result.ast_tree = ast_tree

            # Activity 2: Single File Validator Execution
            violations = self._run_validators(file_path, ast_tree)

            # Activity 3: Violation Aggregation
            violations = self._aggregate_violations(violations)

            # Activity 4: Report Generation
            result.violations = violations
            result.health_score = self._calculate_health_score(violations)
            result.success = True

        except Exception as e:
            logger.error(f"Error validating file {file_path}: {e}", exc_info=True)
            result.error_message = str(e)
            result.success = False

        finally:
            result.execution_time_ms = (time.time() - start_time) * 1000

        return result

    def _validate_file_exists(self, file_path: Path) -> bool:
        """Validate file exists and is readable."""
        try:
            return file_path.exists() and file_path.is_file() and file_path.suffix == ".py"
        except (OSError, PermissionError):
            return False

    def _parse_python_file(self, file_path: Path) -> Optional[ast.AST]:
        """Parse Python file into AST representation."""
        try:
            content = file_path.read_text(encoding="utf-8")
            return ast.parse(content, filename=str(file_path))
        except (OSError, UnicodeDecodeError, SyntaxError) as e:
            logger.warning(f"Failed to parse {file_path}: {e}")
            return None

    def _run_validators(self, file_path: Path, ast_tree: ast.AST) -> List[Finding]:
        """Run each validator against the file."""
        all_violations = []
        enabled_validators = self.rule_engine.get_enabled_validators()
        enabled_rule_ids = {v.rule_id for v in enabled_validators}

        for rule_id, validator_class in self.validators.items():
            if rule_id not in enabled_rule_ids:
                logger.debug(f"Skipping disabled validator: {rule_id}")
                continue

            try:
                # Instantiate validator using rule engine to ensure proper configuration
                validator = self.rule_engine.create_validator_instance(validator_class)
                if not validator:
                    logger.warning(f"Failed to create validator instance for {rule_id}")
                    continue

                # Run validator with proper interface
                if hasattr(validator, "validate_file"):
                    violations = validator.validate_file(file_path, ast_tree)
                elif hasattr(validator, "validate"):
                    # Read file content for validators that need it
                    try:
                        content = file_path.read_text(encoding="utf-8")
                        violations = list(validator.validate(file_path, content))
                    except (OSError, UnicodeDecodeError) as e:
                        logger.warning(
                            f"Could not read file {file_path} for validator {rule_id}: {e}"
                        )
                        continue
                else:
                    logger.warning(f"Validator {rule_id} has no validate method")
                    continue

                if violations:
                    all_violations.extend(violations)
                    logger.debug(f"Validator {rule_id} found {len(violations)} violations")

            except Exception as e:
                logger.error(f"Error running validator {rule_id}: {e}", exc_info=True)
                # Create error finding using the correct interface
                error_finding = Finding(
                    rule_id=rule_id,
                    message=f"Validator error: {e}",
                    file_path=file_path,
                    line=1,
                    column=1,
                    severity=Severity.BLOCK,
                    suggestion=None,
                )
                all_violations.append(error_finding)

        return all_violations

    def _aggregate_violations(self, violations: List[Finding]) -> List[Finding]:
        """
        Aggregate violations from all validators.

        - Sort by line number and severity
        - Apply ignore rules and exceptions
        """
        # Apply ignore rules first
        filtered_violations = self._apply_ignore_rules(violations)

        # Sort by line number, then by severity (BLOCK first, then WARN)
        sorted_violations = sorted(
            filtered_violations, key=lambda v: (v.line or 0, v.severity.value, v.rule_id)
        )

        return sorted_violations

    def _apply_ignore_rules(self, violations: List[Finding]) -> List[Finding]:
        """Apply ignore rules and exceptions to violations."""
        # For now, return all violations
        # TODO: Implement ignore patterns from config
        return violations

    def _calculate_health_score(self, violations: List[Finding]) -> float:
        """
        Calculate file-level health score between 0-100.

        Score calculation:
        - Start with 100
        - Subtract 10 for each ERROR
        - Subtract 5 for each WARN
        - Minimum score is 0
        """
        score = 100.0

        for violation in violations:
            if violation.severity == Severity.BLOCK:
                score -= 10.0
            elif violation.severity == Severity.WARN:
                score -= 5.0

        return max(0.0, score)


class SingleFileValidationWorkflow:
    """
    Workflow 1: Single File Validation

    User runs `vibelint validate file.py`
    """

    def __init__(self, config: Config):
        self.config = config
        self.validator = SingleFileValidator(config)

    def execute(
        self, file_path: Path, output_format: str = "natural"
    ) -> SingleFileValidationResult:
        """
        Execute single file validation workflow.

        Args:
            file_path: Path to Python file to validate
            output_format: Output format (natural, json, etc.)

        Returns:
            SingleFileValidationResult with violations and health score
        """
        logger.info(f"Starting single file validation for: {file_path}")

        result = self.validator.validate_file(file_path)

        if result.success:
            logger.info(f"Validation completed. Health score: {result.health_score:.1f}")
        else:
            logger.error(f"Validation failed: {result.error_message}")

        return result

    def format_output(
        self, result: SingleFileValidationResult, output_format: str = "natural"
    ) -> str:
        """Format validation result for output."""
        if output_format == "json":
            return self._format_json(result)
        elif output_format == "natural":
            return self._format_natural(result)
        else:
            return self._format_natural(result)

    def _format_json(self, result: SingleFileValidationResult) -> str:
        """Format result as JSON."""
        import json

        output = {
            "file_path": str(result.file_path) if result.file_path else None,
            "success": result.success,
            "health_score": result.health_score,
            "execution_time_ms": result.execution_time_ms,
            "violations": [],
        }

        if result.error_message:
            output["error"] = result.error_message

        for violation in result.violations:
            output["violations"].append(
                {
                    "rule_id": violation.rule_id,
                    "severity": violation.severity.name,
                    "message": violation.message,
                    "line_number": violation.line,
                    "column_number": violation.column,
                    "fix_suggestion": violation.suggestion,
                }
            )

        return json.dumps(output, indent=2)

    def _format_natural(self, result: SingleFileValidationResult) -> str:
        """Format result in natural language."""
        if not result.success:
            return f"❌ Validation failed: {result.error_message}"

        output_lines = []

        # Header
        file_name = result.file_path.name if result.file_path else "Unknown"
        output_lines.append(f"📄 File: {file_name}")
        output_lines.append(f"💯 Health Score: {result.health_score:.1f}/100")
        output_lines.append(f"⏱️  Execution Time: {result.execution_time_ms:.1f}ms")

        if not result.violations:
            output_lines.append("\n✅ No violations found!")
            return "\n".join(output_lines)

        # Group violations by severity
        errors = [v for v in result.violations if v.severity == Severity.BLOCK]
        warnings = [v for v in result.violations if v.severity == Severity.WARN]

        if errors:
            output_lines.append(f"\n🚨 Errors ({len(errors)}):")
            for error in errors:
                line_info = f":{error.line}" if error.line else ""
                output_lines.append(f"  • [{error.rule_id}] {error.message} {line_info}")
                if error.suggestion:
                    output_lines.append(f"    💡 Fix: {error.suggestion}")

        if warnings:
            output_lines.append(f"\n⚠️  Warnings ({len(warnings)}):")
            for warning in warnings:
                line_info = f":{warning.line}" if warning.line else ""
                output_lines.append(f"  • [{warning.rule_id}] {warning.message} {line_info}")
                if warning.suggestion:
                    output_lines.append(f"    💡 Fix: {warning.suggestion}")

        return "\n".join(output_lines)


def run_single_file_validation(
    file_path: Path, config: Config, output_format: str = "natural"
) -> SingleFileValidationResult:
    """
    Convenience function to run single file validation.

    This is the main entry point for Workflow 1.
    """
    workflow = SingleFileValidationWorkflow(config)
    return workflow.execute(file_path, output_format)
```

---
### File: src/vibelint/workflows/manager.py

```python
"""
Workflow manager for orchestrating and executing analysis workflows.

Handles workflow scheduling, dependency resolution, parallel execution,
and results aggregation with comprehensive error handling.

vibelint/src/vibelint/workflow_manager.py
"""

import asyncio
import logging
import time
from collections import defaultdict, deque
from pathlib import Path
from typing import Any, Dict, List, Optional, Set

from .core.base import BaseWorkflow, WorkflowResult, WorkflowStatus
from .evaluation import WorkflowEvaluator
from .registry import workflow_registry

logger = logging.getLogger(__name__)

__all__ = ["WorkflowManager", "WorkflowExecutionPlan", "WorkflowSession"]


class WorkflowExecutionPlan:
    """Execution plan for workflow orchestration."""

    def __init__(self):
        self.workflows: List[BaseWorkflow] = []
        self.dependency_graph: Dict[str, Set[str]] = defaultdict(set)
        self.execution_order: List[List[str]] = []  # Batches of workflows
        self.estimated_duration: float = 0.0

    def add_workflow(self, workflow: BaseWorkflow):
        """Add workflow to execution plan."""
        self.workflows.append(workflow)

        # Build dependency graph
        workflow_id = workflow.workflow_id
        dependencies = workflow.get_dependencies()

        for dep_id in dependencies:
            self.dependency_graph[workflow_id].add(dep_id)

    def resolve_execution_order(self):
        """Resolve workflow execution order using topological sort."""
        # Topological sort with batching for parallel execution
        in_degree = defaultdict(int)
        workflow_map = {w.workflow_id: w for w in self.workflows}

        # Calculate in-degrees
        for workflow_id in workflow_map:
            for dep_id in self.dependency_graph[workflow_id]:
                in_degree[workflow_id] += 1

        # Initialize queue with workflows that have no dependencies
        queue = deque()
        for workflow in self.workflows:
            if in_degree[workflow.workflow_id] == 0:
                queue.append(workflow.workflow_id)

        execution_batches = []

        while queue:
            # Process all workflows in current batch (can run in parallel)
            current_batch = []
            batch_size = len(queue)

            for _ in range(batch_size):
                workflow_id = queue.popleft()
                current_batch.append(workflow_id)

                # Update in-degrees for dependent workflows
                for dependent_id, deps in self.dependency_graph.items():
                    if workflow_id in deps:
                        in_degree[dependent_id] -= 1
                        if in_degree[dependent_id] == 0:
                            queue.append(dependent_id)

            if current_batch:
                execution_batches.append(current_batch)

        self.execution_order = execution_batches

        # Estimate total duration
        self.estimated_duration = sum(
            max(workflow_map[wid].estimate_execution_time({}) for wid in batch)
            for batch in execution_batches
        )

    def get_workflow_by_id(self, workflow_id: str) -> Optional[BaseWorkflow]:
        """Get workflow by ID."""
        for workflow in self.workflows:
            if workflow.workflow_id == workflow_id:
                return workflow
        return None


class WorkflowSession:
    """Session for tracking workflow execution state."""

    def __init__(self, session_id: str):
        self.session_id = session_id
        self.start_time = time.time()
        self.end_time: Optional[float] = None

        # Execution state
        self.context: Dict[str, Any] = {}
        self.results: Dict[str, WorkflowResult] = {}
        self.errors: List[str] = []

        # Metrics
        self.total_workflows = 0
        self.completed_workflows = 0
        self.failed_workflows = 0

    def add_result(self, result: WorkflowResult):
        """Add workflow result to session."""
        self.results[result.workflow_id] = result

        if result.status == WorkflowStatus.COMPLETED:
            self.completed_workflows += 1
        elif result.status == WorkflowStatus.FAILED:
            self.failed_workflows += 1
            if result.error_message:
                self.errors.append(f"{result.workflow_id}: {result.error_message}")

    def update_context(self, key: str, value: Any):
        """Update shared context data."""
        self.context[key] = value

    def get_success_rate(self) -> float:
        """Get workflow success rate."""
        if self.total_workflows == 0:
            return 0.0
        return self.completed_workflows / self.total_workflows

    def finalize(self):
        """Finalize session."""
        self.end_time = time.time()

    def get_duration(self) -> float:
        """Get session duration in seconds."""
        end = self.end_time or time.time()
        return end - self.start_time


class WorkflowManager:
    """Manages workflow execution and orchestration."""

    def __init__(self, evaluator: Optional[WorkflowEvaluator] = None):
        self.evaluator = evaluator or WorkflowEvaluator()
        self.active_sessions: Dict[str, WorkflowSession] = {}

    async def execute_workflows(
        self,
        workflow_ids: List[str],
        project_root: Path,
        initial_context: Optional[Dict[str, Any]] = None,
        session_id: Optional[str] = None,
    ) -> WorkflowSession:
        """Execute specified workflows with dependency resolution."""

        if session_id is None:
            session_id = f"session_{int(time.time())}"

        # Create session
        session = WorkflowSession(session_id)
        session.context.update(initial_context or {})
        self.active_sessions[session_id] = session

        try:
            # Load workflows
            workflows = self._load_workflows(workflow_ids)
            session.total_workflows = len(workflows)

            # Create execution plan
            plan = self._create_execution_plan(workflows, session.context)

            # Execute workflows
            await self._execute_plan(plan, project_root, session)

        except Exception as e:
            logger.error(f"Workflow execution failed: {e}", exc_info=True)
            session.errors.append(f"Execution failed: {e}")

        finally:
            session.finalize()

        return session

    def _load_workflows(self, workflow_ids: List[str]) -> List[BaseWorkflow]:
        """Load workflow instances from registry."""
        workflows = []

        for workflow_id in workflow_ids:
            workflow_class = workflow_registry.get_workflow(workflow_id)
            if workflow_class:
                workflow = workflow_class()
                workflows.append(workflow)
            else:
                logger.warning(f"Workflow not found: {workflow_id}")

        return workflows

    def _create_execution_plan(
        self, workflows: List[BaseWorkflow], context: Dict[str, Any]
    ) -> WorkflowExecutionPlan:
        """Create optimized execution plan."""

        plan = WorkflowExecutionPlan()

        # Filter workflows that can execute
        executable_workflows = []
        for workflow in workflows:
            if workflow.can_execute(context):
                executable_workflows.append(workflow)
            else:
                logger.info(f"Skipping workflow {workflow.workflow_id}: requirements not met")

        # Add workflows to plan
        for workflow in executable_workflows:
            plan.add_workflow(workflow)

        # Resolve execution order
        plan.resolve_execution_order()

        logger.info(
            f"Created execution plan: {len(executable_workflows)} workflows in "
            f"{len(plan.execution_order)} batches, estimated duration: {plan.estimated_duration:.1f}s"
        )

        return plan

    async def _execute_plan(
        self, plan: WorkflowExecutionPlan, project_root: Path, session: WorkflowSession
    ):
        """Execute workflow plan with parallel batching."""

        for batch_idx, workflow_ids in enumerate(plan.execution_order):
            logger.info(
                f"Executing batch {batch_idx + 1}/{len(plan.execution_order)}: {workflow_ids}"
            )

            # Group workflows by parallel execution capability
            parallel_workflows = []
            sequential_workflows = []

            for workflow_id in workflow_ids:
                workflow = plan.get_workflow_by_id(workflow_id)
                if workflow and workflow.supports_parallel_execution():
                    parallel_workflows.append(workflow)
                else:
                    sequential_workflows.append(workflow)

            # Execute parallel workflows concurrently
            if parallel_workflows:
                tasks = []
                for workflow in parallel_workflows:
                    task = asyncio.create_task(
                        self._execute_single_workflow(workflow, project_root, session)
                    )
                    tasks.append(task)

                parallel_results = await asyncio.gather(*tasks, return_exceptions=True)

                for result in parallel_results:
                    if isinstance(result, Exception):
                        logger.error(f"Parallel workflow failed: {result}")
                    elif isinstance(result, WorkflowResult):
                        session.add_result(result)
                        self._update_context_from_result(session, result)

            # Execute sequential workflows
            for workflow in sequential_workflows:
                result = await self._execute_single_workflow(workflow, project_root, session)
                session.add_result(result)
                self._update_context_from_result(session, result)

    async def _execute_single_workflow(
        self, workflow: BaseWorkflow, project_root: Path, session: WorkflowSession
    ) -> WorkflowResult:
        """Execute single workflow with monitoring."""

        logger.debug(f"Starting workflow: {workflow.workflow_id}")

        try:
            # Execute workflow
            result = await workflow._execute_with_error_handling(project_root, session.context)

            # Evaluate workflow performance
            if self.evaluator:
                evaluation = self.evaluator.evaluate_workflow_execution(workflow, result)
                result.artifacts["evaluation"] = evaluation

            logger.info(
                f"Workflow {workflow.workflow_id} completed: "
                f"status={result.status.value}, "
                f"findings={len(result.findings)}, "
                f"duration={result.metrics.execution_time_seconds:.2f}s"
            )

            return result

        except Exception as e:
            logger.error(f"Workflow {workflow.workflow_id} execution failed: {e}", exc_info=True)

            # Create failed result
            return WorkflowResult(
                workflow_id=workflow.workflow_id,
                status=WorkflowStatus.FAILED,
                metrics=workflow.metrics,
                error_message=str(e),
            )

    def _update_context_from_result(self, session: WorkflowSession, result: WorkflowResult):
        """Update session context with workflow results."""

        # Add result artifacts to context
        for key, value in result.artifacts.items():
            context_key = f"{result.workflow_id}_{key}"
            session.update_context(context_key, value)

        # Add findings summary
        if result.findings:
            findings_key = f"{result.workflow_id}_findings"
            session.update_context(findings_key, result.findings)

        # Add metrics
        metrics_key = f"{result.workflow_id}_metrics"
        session.update_context(metrics_key, result.metrics)

    def get_available_workflows(self) -> Dict[str, Dict[str, Any]]:
        """Get information about available workflows."""
        available = {}

        for workflow_id, workflow_class in workflow_registry.get_all_workflows().items():
            # Create temporary instance to get metadata
            try:
                temp_workflow = workflow_class()
                available[workflow_id] = {
                    "name": temp_workflow.name,
                    "description": temp_workflow.description,
                    "category": temp_workflow.category,
                    "version": temp_workflow.version,
                    "tags": list(temp_workflow.tags),
                    "required_inputs": list(temp_workflow.get_required_inputs()),
                    "produced_outputs": list(temp_workflow.get_produced_outputs()),
                    "dependencies": temp_workflow.get_dependencies(),
                    "supports_parallel": temp_workflow.supports_parallel_execution(),
                }
            except Exception as e:
                logger.warning(f"Failed to get metadata for workflow {workflow_id}: {e}")

        return available

    def validate_workflow_plan(self, workflow_ids: List[str]) -> Dict[str, Any]:
        """Validate workflow execution plan."""
        validation_result = {
            "valid": True,
            "errors": [],
            "warnings": [],
            "execution_order": [],
            "estimated_duration": 0.0,
        }

        try:
            # Load workflows
            workflows = self._load_workflows(workflow_ids)

            if len(workflows) != len(workflow_ids):
                missing = set(workflow_ids) - {w.workflow_id for w in workflows}
                validation_result["errors"].append(f"Missing workflows: {missing}")
                validation_result["valid"] = False

            # Create execution plan
            plan = self._create_execution_plan(workflows, {})
            validation_result["execution_order"] = plan.execution_order
            validation_result["estimated_duration"] = plan.estimated_duration

            # Check for circular dependencies
            if self._has_circular_dependencies(plan):
                validation_result["errors"].append("Circular dependencies detected")
                validation_result["valid"] = False

        except Exception as e:
            validation_result["errors"].append(f"Validation failed: {e}")
            validation_result["valid"] = False

        return validation_result

    def _has_circular_dependencies(self, plan: WorkflowExecutionPlan) -> bool:
        """Check for circular dependencies in workflow plan."""
        # Simple cycle detection using DFS
        visited = set()
        rec_stack = set()

        def has_cycle(workflow_id: str) -> bool:
            visited.add(workflow_id)
            rec_stack.add(workflow_id)

            for dep_id in plan.dependency_graph[workflow_id]:
                if dep_id not in visited:
                    if has_cycle(dep_id):
                        return True
                elif dep_id in rec_stack:
                    return True

            rec_stack.remove(workflow_id)
            return False

        for workflow in plan.workflows:
            if workflow.workflow_id not in visited:
                if has_cycle(workflow.workflow_id):
                    return True

        return False

    def get_session_status(self, session_id: str) -> Optional[Dict[str, Any]]:
        """Get status of workflow session."""
        session = self.active_sessions.get(session_id)
        if not session:
            return None

        return {
            "session_id": session_id,
            "duration": session.get_duration(),
            "total_workflows": session.total_workflows,
            "completed_workflows": session.completed_workflows,
            "failed_workflows": session.failed_workflows,
            "success_rate": session.get_success_rate(),
            "errors": session.errors,
            "context_keys": list(session.context.keys()),
        }
```

---
### File: src/vibelint/workflows/orchestrator.py

```python
"""
Multi-level workflow orchestrator for comprehensive code quality assessment.

Coordinates tree-level, content-level, and deep analysis using specialized
LLM agents to catch organizational violations at different granularities.

vibelint/src/vibelint/workflow_orchestrator.py
"""

import json
import logging
import time
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional

from ..context.analyzer import ContextAnalyzer
from ..context.prompts import AgentPrompts, AnalysisLevel
from ..llm import LLMManager, LLMRequest, LLMRole
from ..project_map import ProjectMapper

logger = logging.getLogger(__name__)

__all__ = ["AnalysisOrchestrator", "OrchestrationResult", "AnalysisReport"]


@dataclass
class OrchestrationResult:
    """Result of multi-level analysis orchestration."""

    success: bool
    analysis_duration: float
    reports_generated: List[str]
    total_findings: int
    critical_issues: int
    quick_wins: List[str]
    strategic_recommendations: List[str]
    error_message: Optional[str] = None


@dataclass
class AnalysisReport:
    """Comprehensive analysis report for development feedback."""

    timestamp: str
    project_root: str
    analysis_levels_completed: List[str]

    # Level-specific results
    tree_violations: List[Dict[str, Any]]
    content_findings: List[Dict[str, Any]]
    architectural_findings: List[Dict[str, Any]]

    # Synthesized recommendations
    executive_summary: Dict[str, Any]
    priority_actions: List[Dict[str, Any]]
    quick_wins: List[str]
    strategic_initiatives: List[str]

    # Metrics and tracking
    current_health_scores: Dict[str, float]
    improvement_targets: Dict[str, float]
    next_review_triggers: List[str]


class AnalysisOrchestrator:
    """Orchestrates multi-level code quality analysis using specialized agents."""

    def __init__(self, llm_manager: LLMManager, project_root: Path):
        self.llm = llm_manager
        self.project_root = project_root
        self.context_analyzer = ContextAnalyzer(project_root)
        self.project_mapper = ProjectMapper(project_root)

    async def run_comprehensive_analysis(
        self, target_files: Optional[List[Path]] = None, analysis_levels: Optional[List[str]] = None
    ) -> OrchestrationResult:
        """Run comprehensive multi-level analysis."""
        start_time = time.time()

        if analysis_levels is None:
            analysis_levels = [AnalysisLevel.TREE, AnalysisLevel.CONTENT, AnalysisLevel.DEEP]

        logger.info(f"Starting comprehensive analysis with levels: {analysis_levels}")

        try:
            # Step 1: Tree-level analysis (organizational structure)
            tree_results = {}
            if AnalysisLevel.TREE in analysis_levels:
                tree_results = await self._run_tree_analysis()

            # Step 2: Content-level analysis (file structure)
            content_results = {}
            if AnalysisLevel.CONTENT in analysis_levels:
                content_results = await self._run_content_analysis(target_files)

            # Step 3: Deep analysis (architectural assessment)
            deep_results = {}
            if AnalysisLevel.DEEP in analysis_levels:
                deep_results = await self._run_deep_analysis(
                    target_files, tree_results, content_results
                )

            # Step 4: Synthesis and orchestration
            synthesis_result = await self._synthesize_results(
                tree_results, content_results, deep_results, analysis_levels
            )

            # Step 5: Generate reports
            report_paths = await self._generate_reports(synthesis_result)

            duration = time.time() - start_time

            return OrchestrationResult(
                success=True,
                analysis_duration=duration,
                reports_generated=report_paths,
                total_findings=synthesis_result.get("total_findings", 0),
                critical_issues=synthesis_result.get("critical_issues", 0),
                quick_wins=synthesis_result.get("quick_wins", []),
                strategic_recommendations=synthesis_result.get("strategic_initiatives", []),
            )

        except Exception as e:
            logger.error(f"Analysis orchestration failed: {e}", exc_info=True)
            duration = time.time() - start_time

            return OrchestrationResult(
                success=False,
                analysis_duration=duration,
                reports_generated=[],
                total_findings=0,
                critical_issues=0,
                quick_wins=[],
                strategic_recommendations=[],
                error_message=str(e),
            )

    async def _run_tree_analysis(self) -> Dict[str, Any]:
        """Run tree-level organizational analysis."""
        logger.info("Running tree-level analysis...")

        # Generate project map
        project_map = self.project_mapper.generate_project_map()

        # Use context analyzer for quick organizational checks
        quick_violations = self.context_analyzer.analyze_tree_level()

        # Prepare context for LLM analysis
        context_data = {
            "project_map": json.dumps(project_map, indent=2, default=str),
            "quick_violations": [asdict(v) for v in quick_violations],
        }

        prompt = AgentPrompts.get_prompt_for_analysis_level(AnalysisLevel.TREE)
        context = AgentPrompts.get_context_for_analysis(AnalysisLevel.TREE, context_data)

        # Use fast LLM for tree analysis
        llm_request = LLMRequest(
            content=f"{prompt}\n\n{context}",
            task_type="tree_analysis",
            max_tokens=2048,
            temperature=0.1,
        )

        try:
            response = await self.llm.process_request(llm_request)
            llm_analysis = self._parse_llm_response(response["content"])

            return {
                "project_map": project_map,
                "quick_violations": [asdict(v) for v in quick_violations],
                "llm_analysis": llm_analysis,
                "organization_score": project_map.get("organization_metrics", {}).get(
                    "organization_score", 0.5
                ),
            }

        except Exception as e:
            logger.warning(f"LLM tree analysis failed, using quick analysis only: {e}")
            return {
                "project_map": project_map,
                "quick_violations": [asdict(v) for v in quick_violations],
                "llm_analysis": {"violations": [], "organization_score": 0.5},
                "organization_score": project_map.get("organization_metrics", {}).get(
                    "organization_score", 0.5
                ),
            }

    async def _run_content_analysis(self, target_files: Optional[List[Path]]) -> Dict[str, Any]:
        """Run content-level structural analysis."""
        logger.info("Running content-level analysis...")

        if target_files is None:
            # Discover Python files to analyze
            target_files = list(self.project_root.rglob("*.py"))
            target_files = [f for f in target_files if not self._should_skip_file(f)]

        file_analyses = []

        # Analyze up to 10 most important files to avoid overwhelming the LLM
        important_files = self._select_important_files(target_files)

        for file_path in important_files[:10]:
            try:
                file_analysis = await self._analyze_single_file(file_path)
                file_analyses.append(file_analysis)
            except Exception as e:
                logger.warning(f"Failed to analyze {file_path}: {e}")

        return {
            "files_analyzed": len(file_analyses),
            "total_files": len(target_files),
            "file_analyses": file_analyses,
            "structural_health": self._calculate_structural_health(file_analyses),
        }

    async def _analyze_single_file(self, file_path: Path) -> Dict[str, Any]:
        """Analyze a single file for structural issues."""
        try:
            content = file_path.read_text(encoding="utf-8")
            relative_path = file_path.relative_to(self.project_root)

            context_data = {
                "file_path": str(relative_path),
                "file_content": content,
                "file_size": len(content),
                "file_purpose": self._infer_file_purpose(file_path),
                "dependencies": self._extract_imports(content),
                "exports": self._extract_exports(content),
            }

            prompt = AgentPrompts.get_prompt_for_analysis_level(AnalysisLevel.CONTENT)
            context = AgentPrompts.get_context_for_analysis(AnalysisLevel.CONTENT, context_data)

            llm_request = LLMRequest(
                content=f"{prompt}\n\n{context}",
                task_type="content_analysis",
                max_tokens=1500,
                temperature=0.1,
            )

            response = await self.llm.process_request(llm_request)
            return {
                "file_path": str(relative_path),
                "analysis": self._parse_llm_response(response["content"]),
                "metadata": {
                    "size": len(content),
                    "purpose": context_data["file_purpose"],
                    "lines": content.count("\n") + 1,
                },
            }

        except Exception as e:
            logger.warning(f"Single file analysis failed for {file_path}: {e}")
            return {
                "file_path": str(file_path.relative_to(self.project_root)),
                "analysis": {"findings": [], "file_health": {}},
                "error": str(e),
            }

    async def _run_deep_analysis(
        self,
        target_files: Optional[List[Path]],
        tree_results: Dict[str, Any],
        content_results: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Run deep architectural analysis."""
        logger.info("Running deep architectural analysis...")

        # Check if orchestrator LLM is available
        if not self.llm.is_llm_available(LLMRole.ORCHESTRATOR):
            logger.warning("Orchestrator LLM not available, skipping deep analysis")
            return {"skipped": True, "reason": "Orchestrator LLM not configured"}

        # Select key files for deep analysis
        if target_files is None:
            target_files = list(self.project_root.rglob("*.py"))
            target_files = [f for f in target_files if not self._should_skip_file(f)]

        key_files = self._select_key_files_for_deep_analysis(target_files)

        # Prepare comprehensive context
        files_content = {}
        for file_path in key_files[:5]:  # Limit to 5 files for context size
            try:
                content = file_path.read_text(encoding="utf-8")
                relative_path = file_path.relative_to(self.project_root)
                files_content[str(relative_path)] = content
            except Exception as e:
                logger.warning(f"Failed to read {file_path}: {e}")

        context_data = {
            "project_context": f"Project: {self.project_root.name}",
            "files_content": self._format_files_for_analysis(files_content),
            "tree_results": json.dumps(tree_results, indent=2, default=str),
            "content_results": json.dumps(content_results, indent=2, default=str),
        }

        prompt = AgentPrompts.get_prompt_for_analysis_level(AnalysisLevel.DEEP)
        context = AgentPrompts.get_context_for_analysis(AnalysisLevel.DEEP, context_data)

        # Use orchestrator LLM for deep analysis
        llm_request = LLMRequest(
            content=f"{prompt}\n\n{context}",
            task_type="architectural_analysis",
            max_tokens=4096,
            temperature=0.2,
        )

        try:
            response = await self.llm.process_request(llm_request)
            deep_analysis = self._parse_llm_response(response["content"])

            return {
                "files_analyzed": list(files_content.keys()),
                "architectural_analysis": deep_analysis,
                "analysis_scope": "comprehensive",
            }

        except Exception as e:
            logger.error(f"Deep analysis failed: {e}")
            return {
                "files_analyzed": [],
                "architectural_analysis": {"architectural_findings": [], "code_smells": []},
                "error": str(e),
            }

    async def _synthesize_results(
        self,
        tree_results: Dict[str, Any],
        content_results: Dict[str, Any],
        deep_results: Dict[str, Any],
        analysis_levels: List[str],
    ) -> Dict[str, Any]:
        """Synthesize multi-level results into actionable recommendations."""
        logger.info("Synthesizing analysis results...")

        # Count total findings
        total_findings = 0
        critical_issues = 0

        if tree_results.get("quick_violations"):
            total_findings += len(tree_results["quick_violations"])
        if tree_results.get("llm_analysis", {}).get("violations"):
            total_findings += len(tree_results["llm_analysis"]["violations"])

        for file_analysis in content_results.get("file_analyses", []):
            findings = file_analysis.get("analysis", {}).get("findings", [])
            total_findings += len(findings)
            critical_issues += len([f for f in findings if f.get("severity") == "BLOCK"])

        if deep_results.get("architectural_analysis"):
            arch_findings = deep_results["architectural_analysis"].get("architectural_findings", [])
            code_smells = deep_results["architectural_analysis"].get("code_smells", [])
            total_findings += len(arch_findings) + len(code_smells)
            critical_issues += len([f for f in arch_findings if f.get("severity") == "BLOCK"])

        # Prepare synthesis context
        synthesis_data = {
            "tree_analysis": tree_results,
            "content_analysis": content_results,
            "deep_analysis": deep_results,
            "total_findings": total_findings,
            "critical_issues": critical_issues,
        }

        context = f"""ANALYSIS SYNTHESIS REQUEST

Analysis Results Summary:
- Tree Analysis: {len(tree_results.get('quick_violations', []))} organizational violations
- Content Analysis: {content_results.get('files_analyzed', 0)} files analyzed
- Deep Analysis: {len(deep_results.get('files_analyzed', []))} files examined

Detailed Results:
{json.dumps(synthesis_data, indent=2, default=str)}

Synthesize these results into actionable development feedback with prioritized recommendations."""

        prompt = AgentPrompts.get_orchestrator_prompt()

        llm_request = LLMRequest(
            content=f"{prompt}\n\n{context}",
            task_type="synthesis",
            max_tokens=3072,
            temperature=0.1,
        )

        try:
            response = await self.llm.process_request(llm_request)
            synthesis = self._parse_llm_response(response["content"])

            # Add computed metrics
            synthesis["total_findings"] = total_findings
            synthesis["critical_issues"] = critical_issues
            synthesis["analysis_levels_completed"] = analysis_levels

            return synthesis

        except Exception as e:
            logger.warning(f"Synthesis failed, generating basic summary: {e}")

            # Fallback synthesis
            return {
                "executive_summary": {
                    "overall_health": 0.7,
                    "critical_issues": critical_issues,
                    "improvement_opportunities": total_findings,
                    "estimated_effort": "Unknown",
                },
                "priority_actions": [],
                "quick_wins": ["Review organizational violations", "Address structural issues"],
                "strategic_initiatives": ["Consider architectural improvements"],
                "total_findings": total_findings,
                "critical_issues": critical_issues,
                "analysis_levels_completed": analysis_levels,
            }

    async def _generate_reports(self, synthesis_result: Dict[str, Any]) -> List[str]:
        """Generate analysis reports for development feedback."""
        reports_dir = self.project_root / ".vibelint-reports"
        reports_dir.mkdir(exist_ok=True)

        timestamp = time.strftime("%Y%m%d_%H%M%S")
        report_paths = []

        # Generate comprehensive JSON report
        json_report_path = reports_dir / f"comprehensive_analysis_{timestamp}.json"
        try:
            with open(json_report_path, "w", encoding="utf-8") as f:
                json.dump(synthesis_result, f, indent=2, default=str)
            report_paths.append(str(json_report_path))
            logger.info(f"Generated JSON report: {json_report_path}")
        except Exception as e:
            logger.error(f"Failed to generate JSON report: {e}")

        # Generate human-readable summary
        summary_path = reports_dir / f"analysis_summary_{timestamp}.md"
        try:
            summary_content = self._generate_markdown_summary(synthesis_result)
            summary_path.write_text(summary_content, encoding="utf-8")
            report_paths.append(str(summary_path))
            logger.info(f"Generated summary report: {summary_path}")
        except Exception as e:
            logger.error(f"Failed to generate summary report: {e}")

        return report_paths

    def _generate_markdown_summary(self, synthesis: Dict[str, Any]) -> str:
        """Generate human-readable markdown summary."""
        executive = synthesis.get("executive_summary", {})
        priority_actions = synthesis.get("priority_actions", [])
        quick_wins = synthesis.get("quick_wins", [])
        strategic = synthesis.get("strategic_initiatives", [])

        content = f"""# Vibelint Analysis Report

Generated: {time.strftime("%Y-%m-%d %H:%M:%S")}
Project: {self.project_root.name}

## Executive Summary

- **Overall Health**: {executive.get('overall_health', 'Unknown')}
- **Critical Issues**: {executive.get('critical_issues', 0)}
- **Improvement Opportunities**: {executive.get('improvement_opportunities', 0)}
- **Estimated Effort**: {executive.get('estimated_effort', 'Unknown')}

## Priority Actions

"""

        for i, action in enumerate(priority_actions[:5], 1):
            content += f"""### {i}. {action.get('title', 'Unknown Action')} ({action.get('priority', 'P?')})

{action.get('description', 'No description available')}

**Effort**: {action.get('effort_hours', '?')} hours
**Risk if ignored**: {action.get('risk_if_ignored', 'Unknown')}

"""

        if quick_wins:
            content += "\n## Quick Wins\n\n"
            for win in quick_wins:
                content += f"- {win}\n"

        if strategic:
            content += "\n## Strategic Initiatives\n\n"
            for initiative in strategic:
                content += f"- {initiative}\n"

        return content

    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM JSON response with fallback."""
        try:
            # Extract JSON from response
            start = response.find("{")
            end = response.rfind("}") + 1

            if start >= 0 and end > start:
                json_str = response[start:end]
                return json.loads(json_str)
            else:
                logger.warning("No JSON found in LLM response")
                return {"error": "No JSON in response", "raw_response": response}

        except json.JSONDecodeError as e:
            logger.warning(f"Failed to parse LLM JSON response: {e}")
            return {"error": "Invalid JSON", "raw_response": response}

    def _should_skip_file(self, file_path: Path) -> bool:
        """Check if file should be skipped in analysis."""
        skip_patterns = [
            "__pycache__",
            ".git",
            ".pytest_cache",
            "build",
            "dist",
            ".venv",
            "venv",
            ".mypy_cache",
            ".tox",
        ]
        return any(pattern in str(file_path) for pattern in skip_patterns)

    def _select_important_files(self, files: List[Path]) -> List[Path]:
        """Select most important files for analysis."""
        # Prioritize entry points, main modules, and larger files
        scored_files = []

        for file_path in files:
            score = 0
            name = file_path.name.lower()

            # Entry points and main modules
            if name in ["__init__.py", "main.py", "__main__.py", "cli.py"]:
                score += 10
            elif "main" in name or "cli" in name:
                score += 5

            # Size-based scoring
            try:
                size = file_path.stat().st_size
                if size > 5000:  # Larger files likely more important
                    score += 3
                elif size > 1000:
                    score += 1
            except OSError:
                pass

            scored_files.append((score, file_path))

        # Sort by score (descending)
        scored_files.sort(key=lambda x: x[0], reverse=True)
        return [f[1] for f in scored_files]

    def _select_key_files_for_deep_analysis(self, files: List[Path]) -> List[Path]:
        """Select key files for architectural analysis."""
        # Similar to important files but focus on architectural significance
        return self._select_important_files(files)

    def _infer_file_purpose(self, file_path: Path) -> str:
        """Infer the purpose of a file."""
        name = file_path.name.lower()

        if name == "__init__.py":
            return "package_init"
        elif name in ["main.py", "__main__.py", "cli.py"]:
            return "entry_point"
        elif "test" in name:
            return "testing"
        elif "config" in name:
            return "configuration"
        elif any(keyword in name for keyword in ["util", "helper", "tool"]):
            return "utility"
        else:
            return "module"

    def _extract_imports(self, content: str) -> List[str]:
        """Extract import statements from Python code."""
        imports = []
        for line in content.split("\n"):
            line = line.strip()
            if line.startswith("import ") or line.startswith("from "):
                imports.append(line)
        return imports

    def _extract_exports(self, content: str) -> List[str]:
        """Extract __all__ exports from Python code."""
        exports = []
        in_all = False
        all_content = ""

        for line in content.split("\n"):
            if "__all__" in line:
                in_all = True
                all_content = line
            elif in_all:
                all_content += line
                if "]" in line:
                    break

        if all_content:
            try:
                # Simple extraction - could be improved with AST
                import re

                matches = re.findall(r'"([^"]+)"', all_content)
                matches.extend(re.findall(r"'([^']+)'", all_content))
                exports = matches
            except Exception:
                pass

        return exports

    def _calculate_structural_health(self, file_analyses: List[Dict[str, Any]]) -> Dict[str, float]:
        """Calculate overall structural health scores."""
        if not file_analyses:
            return {"overall": 0.5}

        total_files = len(file_analyses)
        healthy_files = sum(
            1
            for analysis in file_analyses
            if analysis.get("analysis", {}).get("file_health", {}).get("overall", False)
        )

        return {
            "overall": healthy_files / total_files if total_files > 0 else 0.5,
            "files_analyzed": total_files,
            "healthy_files": healthy_files,
        }

    def _format_files_for_analysis(self, files_content: Dict[str, str]) -> str:
        """Format multiple files for LLM analysis."""
        formatted = ""
        for file_path, content in files_content.items():
            formatted += f"\n\n=== {file_path} ===\n```python\n{content}\n```"
        return formatted
```

---
### File: src/vibelint/workflows/registry.py

```python
"""
Workflow registry for managing available workflows.

Provides centralized registration and discovery of workflows with
metadata and dependency information.

Responsibility: Workflow discovery and registration only.
Workflow logic belongs in individual workflow implementation modules.

vibelint/src/vibelint/workflow/registry.py
"""

import logging
from typing import Dict, List, Optional, Type

from .core.base import BaseWorkflow

logger = logging.getLogger(__name__)

__all__ = ["WorkflowRegistry", "workflow_registry", "register_workflow"]


class WorkflowRegistry:
    """Registry for managing available workflows."""

    def __init__(self):
        self._workflows: Dict[str, Type[BaseWorkflow]] = {}
        self._metadata: Dict[str, Dict] = {}
        self._loaded = False

    def register(self, workflow_class: Type[BaseWorkflow]) -> None:
        """Register a workflow class."""
        # Create temporary instance to get metadata
        temp_instance = workflow_class()
        workflow_id = temp_instance.workflow_id

        if not workflow_id:
            raise ValueError(f"Workflow {workflow_class.__name__} must define workflow_id")

        if workflow_id in self._workflows:
            logger.warning(f"Overwriting existing workflow: {workflow_id}")

        self._workflows[workflow_id] = workflow_class

        # Store metadata
        self._metadata[workflow_id] = {
            "name": temp_instance.name,
            "description": temp_instance.description,
            "category": temp_instance.category,
            "version": temp_instance.version,
            "tags": list(temp_instance.tags),
            "required_inputs": list(temp_instance.get_required_inputs()),
            "produced_outputs": list(temp_instance.get_produced_outputs()),
            "dependencies": temp_instance.get_dependencies(),
            "supports_parallel": temp_instance.supports_parallel_execution(),
            "class_name": workflow_class.__name__,
        }

        logger.debug(f"Registered workflow: {workflow_id}")

    def get_workflow(self, workflow_id: str) -> Optional[Type[BaseWorkflow]]:
        """Get workflow class by ID."""
        return self._workflows.get(workflow_id)

    def get_all_workflows(self) -> Dict[str, Type[BaseWorkflow]]:
        """Get all registered workflows."""
        return self._workflows.copy()

    def get_workflows_by_category(self, category: str) -> Dict[str, Type[BaseWorkflow]]:
        """Get workflows by category."""
        filtered = {}
        for workflow_id, metadata in self._metadata.items():
            if metadata["category"] == category:
                filtered[workflow_id] = self._workflows[workflow_id]
        return filtered

    def get_workflows_by_tag(self, tag: str) -> Dict[str, Type[BaseWorkflow]]:
        """Get workflows by tag."""
        filtered = {}
        for workflow_id, metadata in self._metadata.items():
            if tag in metadata["tags"]:
                filtered[workflow_id] = self._workflows[workflow_id]
        return filtered

    def get_workflow_metadata(self, workflow_id: str) -> Optional[Dict]:
        """Get workflow metadata."""
        return self._metadata.get(workflow_id)

    def list_workflow_ids(self) -> List[str]:
        """List all workflow IDs."""
        return list(self._workflows.keys())

    def validate_dependencies(self, workflow_ids: List[str]) -> Dict[str, List[str]]:
        """Validate workflow dependencies."""
        missing_deps = {}

        for workflow_id in workflow_ids:
            if workflow_id not in self._workflows:
                missing_deps[workflow_id] = [f"Workflow '{workflow_id}' not found"]
                continue

            metadata = self._metadata[workflow_id]
            for dep_id in metadata["dependencies"]:
                if dep_id not in self._workflows:
                    if workflow_id not in missing_deps:
                        missing_deps[workflow_id] = []
                    missing_deps[workflow_id].append(f"Missing dependency: '{dep_id}'")

        return missing_deps

    def unregister(self, workflow_id: str) -> bool:
        """Unregister a workflow."""
        if workflow_id in self._workflows:
            del self._workflows[workflow_id]
            del self._metadata[workflow_id]
            logger.debug(f"Unregistered workflow: {workflow_id}")
            return True
        return False

    def clear(self) -> None:
        """Clear all registered workflows."""
        self._workflows.clear()
        self._metadata.clear()
        logger.debug("Cleared all workflows from registry")


# Global registry instance
workflow_registry = WorkflowRegistry()


def register_workflow(workflow_class: Type[BaseWorkflow]) -> Type[BaseWorkflow]:
    """Decorator for registering workflows."""
    workflow_registry.register(workflow_class)
    return workflow_class


# Auto-discover and register workflows
def _discover_and_register_workflows():
    """Auto-discover and register all workflow classes."""
    import importlib
    from pathlib import Path

    # Discover built-in workflows in this package
    workflows_dir = Path(__file__).parent

    for file_path in workflows_dir.glob("*.py"):
        if file_path.name.startswith("__") or file_path.name in [
            "base.py",
            "registry.py",
            "manager.py",
            "evaluation.py",
        ]:
            continue

        module_name = file_path.stem
        try:
            module = importlib.import_module(f".{module_name}", package="vibelint.workflow")

            # Find workflow classes in the module
            for attr_name in dir(module):
                attr = getattr(module, attr_name)
                if (
                    isinstance(attr, type)
                    and issubclass(attr, BaseWorkflow)
                    and attr != BaseWorkflow
                    and hasattr(attr, "workflow_id")
                    and attr.workflow_id
                ):

                    logger.debug(
                        f"Auto-registering workflow: {attr.workflow_id} from {module_name}"
                    )
                    workflow_registry.register(attr)

        except Exception as e:
            logger.warning(f"Failed to load workflow module {module_name}: {e}")

    # Also check for user workflows in project entry points
    _register_entry_point_workflows()


def _register_entry_point_workflows():
    """Register workflows from project entry points."""
    try:
        import pkg_resources

        for entry_point in pkg_resources.iter_entry_points("vibelint.workflows"):
            try:
                workflow_class = entry_point.load()
                if issubclass(workflow_class, BaseWorkflow):
                    logger.info(f"Registering user workflow: {workflow_class.workflow_id}")
                    workflow_registry.register(workflow_class)
            except Exception as e:
                logger.warning(f"Failed to load workflow from entry point {entry_point.name}: {e}")

    except ImportError:
        # pkg_resources not available, skip entry point discovery
        pass


# Auto-register workflows on module import
_discover_and_register_workflows()
```

---
### File: src/vibelint/workflows/single_file_validation.py

```python

```

---
### File: tests/fixtures/check_success/myproject/src/mypkg/__init__.py

```python
"""
Package init.

mypkg/__init__.py
"""

__all__ = []
```

---
### File: tests/fixtures/check_success/myproject/src/mypkg/module.py

```python
"""
A sample module.

mypkg/module.py
"""

__all__ = ["hello"]


def hello():
    """
    Returns hello.

    mypkg/module.py
    """
    return "Hello, world!"
```

---
### File: tests/fixtures/fix_missing_all/fixproj/another.py

```python
"""Another file. MISSING PATH"""

__all__ = []


def something():
    pass
```

---
### File: tests/fixtures/fix_missing_all/fixproj/needs_fix.py

```python
"""Module needing a fix. MISSING PATH"""


def func_one():
    pass


def _internal_func():
    """Internal func doc. MISSING PATH"""
    pass


def func_two():
    pass
```

---
### File: tests/test_cli.py

```python
# tests/test_cli.py
"""
Baseline tests for the vibelint CLI interface.

tests/test_cli.py
"""
import os
import re
import shutil
import sys
from collections.abc import Iterator
from pathlib import Path
from typing import Any

import pytest
from click.testing import CliRunner, Result

# Conditional TOML library import
if sys.version_info >= (3, 11):
    import tomllib
else:
    try:
        import tomli as tomllib
    except ImportError:
        tomllib = None

try:
    import tomli_w
except ImportError:
    tomli_w = None

from vibelint import __version__
from vibelint.cli import cli  # Import the main Click group

FIXTURES_DIR = Path(__file__).parent / "fixtures"

# --- Helper Functions ---


def clean_output(output: str) -> str:
    """Removes ANSI escape codes and strips leading/trailing whitespace from each line."""
    cleaned = re.sub(r"\x1b\[.*?m", "", output)  # Remove ANSI codes
    cleaned = re.sub(r"\r\n?", "\n", cleaned)  # Normalize line endings
    # Process line by line to strip, then rejoin. Filter removes empty lines.
    lines = [line.strip() for line in cleaned.splitlines() if line.strip()]  # Filter empty lines
    return "\n".join(lines)


def assert_output_contains(result: Result, substring: str, msg: str = ""):
    """Asserts substring is in cleaned output."""
    cleaned = clean_output(result.output)
    assert (
        substring in cleaned
    ), f"{msg}\nSubstring '{substring}' not found in cleaned output:\n---\n{cleaned}\n---\nOriginal output:\n{result.output}"


def assert_output_does_not_contain(result: Result, substring: str, msg: str = ""):
    """Asserts substring is NOT in cleaned output."""
    cleaned = clean_output(result.output)
    assert (
        substring not in cleaned
    ), f"{msg}\nSubstring '{substring}' unexpectedly found in cleaned output:\n---\n{cleaned}\n---\nOriginal output:\n{result.output}"


def assert_output_matches(result: Result, pattern: str, msg: str = ""):
    """Asserts regex pattern matches cleaned output (multiline, dotall)."""
    cleaned = clean_output(result.output)
    # Using DOTALL means '.' matches newline characters as well
    # Using MULTILINE ensures ^/$ match line beginnings/ends if needed
    assert re.search(
        pattern, cleaned, re.MULTILINE | re.DOTALL
    ), f"{msg}\nPattern '{pattern}' not found in cleaned output:\n---\n{cleaned}\n---\nOriginal output:\n{result.output}"


def assert_output_does_not_match(result: Result, pattern: str, msg: str = ""):
    """Asserts regex pattern does NOT match cleaned output (multiline, dotall)."""
    cleaned = clean_output(result.output)
    assert not re.search(
        pattern, cleaned, re.MULTILINE | re.DOTALL
    ), f"{msg}\nPattern '{pattern}' unexpectedly found in cleaned output:\n---\n{cleaned}\n---\nOriginal output:\n{result.output}"


# --- Fixtures ---


@pytest.fixture
def runner() -> CliRunner:
    """Provides a Click CliRunner instance."""
    return CliRunner()


@pytest.fixture
def setup_test_project(tmp_path: Path, request: pytest.FixtureRequest) -> Iterator[Path]:
    """
    Copies a fixture project structure (e.g., fixtures/check_success/myproject/*)
    into a temporary directory, changes the CWD to the identified project root
    within that temp structure, ensures pyproject.toml exists there, and yields the path.
    """
    fixture_name = request.param
    source_fixture_path = FIXTURES_DIR / fixture_name
    if not source_fixture_path.is_dir():
        pytest.fail(f"Fixture directory not found: {source_fixture_path}")

    # Target directory in tmp_path named after the fixture
    target_base_dir = tmp_path / fixture_name
    target_base_dir.mkdir(parents=True, exist_ok=True)

    # Copy the *contents* of the source fixture directory
    for item in source_fixture_path.iterdir():
        source_item = source_fixture_path / item.name
        target_item = target_base_dir / item.name
        if source_item.is_dir():
            # Ensure target directory exists before copying into it
            target_item.parent.mkdir(parents=True, exist_ok=True)
            shutil.copytree(source_item, target_item, dirs_exist_ok=True)
        else:
            # Ensure target directory exists before copying file
            target_item.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(source_item, target_item)  # copy2 preserves metadata
    print(f"DEBUG: Copied contents of {source_fixture_path} to {target_base_dir}")

    # --- Determine the actual project root within the copied structure ---
    target_project_root = None
    if (target_base_dir / "pyproject.toml").is_file() or (target_base_dir / ".git").is_dir():
        target_project_root = target_base_dir
        print(f"DEBUG: Project marker found in target base directory: {target_project_root}")
    else:
        potential_project_dirs = [
            d for d in target_base_dir.iterdir() if d.is_dir() and not d.name.startswith(".")
        ]
        found_marker_in_subdir = False
        if potential_project_dirs:
            for potential_dir in potential_project_dirs:
                if (potential_dir / "pyproject.toml").is_file() or (
                    potential_dir / ".git"
                ).is_dir():
                    target_project_root = potential_dir
                    print(f"DEBUG: Project marker found in subdirectory: {target_project_root}")
                    found_marker_in_subdir = True
                    break
            if not found_marker_in_subdir and len(potential_project_dirs) == 1:
                target_project_root = potential_project_dirs[0]
                print(
                    f"DEBUG: Assuming single subdirectory is project root (no marker found): {target_project_root}"
                )
        if target_project_root is None:
            target_project_root = target_base_dir
            print(
                f"DEBUG: WARNING - No definitive project root identified via markers or single subdir heuristic. Defaulting to base: {target_project_root}"
            )

    # --- Ensure pyproject.toml exists in the determined root ---
    pyproject_path_in_root = target_project_root / "pyproject.toml"
    if not pyproject_path_in_root.is_file():
        print(f"DEBUG: Creating dummy pyproject.toml in identified root {target_project_root}")
        pyproject_path_in_root.parent.mkdir(parents=True, exist_ok=True)
        # Ensure the dummy toml is valid
        pyproject_path_in_root.write_text(
            "[tool.vibelint]\ninclude_globs = [] # Dummy\n", encoding="utf-8"
        )
    else:
        print(f"DEBUG: Found existing pyproject.toml in identified root: {pyproject_path_in_root}")
        # Ensure the existing file is valid TOML if modifying later
        try:
            with open(pyproject_path_in_root, "rb") as f:
                content = f.read()
                if content.strip():
                    tomllib.loads(content.decode("utf-8", errors="replace"))
        except Exception as e:
            print(
                f"DEBUG: Warning - Existing pyproject.toml at {pyproject_path_in_root} may be invalid: {e}"
            )
            # Overwrite with a basic valid one if parsing fails? Risky. Let modify handle it.
            pass

    # --- Change CWD to the determined project root and Yield ---
    original_cwd = Path.cwd()
    resolved_root = target_project_root.resolve()
    print(f"DEBUG: Original CWD: {original_cwd}")
    print(f"DEBUG: Changing CWD to identified project root: {resolved_root}")
    os.chdir(resolved_root)
    try:
        yield resolved_root
    finally:
        print(f"DEBUG: Restoring CWD to: {original_cwd}")
        os.chdir(original_cwd)


def modify_pyproject(project_path: Path, updates: dict[str, Any]):
    """Modifies the [tool.vibelint] section of pyproject.toml."""
    if tomllib is None or tomli_w is None:
        pytest.skip("Skipping test: 'tomli'/'tomli-w' not available for modifying pyproject.toml.")

    pyproject_file = project_path / "pyproject.toml"
    if not pyproject_file.is_file():
        pytest.fail(f"pyproject.toml unexpectedly missing in {project_path} despite fixture logic.")

    try:
        with open(pyproject_file, "rb") as f:
            content = f.read()
            if not content.strip():
                data = {}
                print(f"DEBUG: Reading empty or whitespace-only {pyproject_file}")
            else:
                try:
                    data = tomllib.loads(content.decode("utf-8", errors="replace"))
                    print(f"DEBUG: Successfully loaded TOML from {pyproject_file}")
                except tomllib.TOMLDecodeError as e:
                    pytest.fail(
                        f"Failed to parse non-empty {pyproject_file}: {e}\nContent:\n>>>\n{content.decode('utf-8', errors='replace')}\n<<<"
                    )
    except Exception as e:
        pytest.fail(f"Failed to read {pyproject_file}: {e}")

    data.setdefault("tool", {}).setdefault("vibelint", {}).update(updates)

    try:
        with open(pyproject_file, "wb") as f:
            tomli_w.dump(data, f)
        print(f"DEBUG: Successfully modified {pyproject_file} with updates: {updates}")
    except Exception as e:
        pytest.fail(f"Failed to write modified {pyproject_file}: {e}")


# --- Test Cases ---


def test_cli_version(runner: CliRunner):
    """Test the --version flag."""
    result = runner.invoke(cli, ["--version"], prog_name="vibelint", catch_exceptions=False)
    assert result.exit_code == 0
    assert f"vibelint, version {__version__}" in result.output


def test_cli_help(runner: CliRunner):
    """Test the --help flag."""
    result = runner.invoke(cli, ["--help"], prog_name="vibelint", catch_exceptions=False)
    assert result.exit_code == 0
    assert "Usage: vibelint [OPTIONS] COMMAND [ARGS]..." in result.output
    assert "check" in result.output
    assert "namespace" in result.output
    assert "snapshot" in result.output
    assert "Vibe Check" in result.output
    assert "Visualize" in result.output
    assert "snapshot" in result.output


@pytest.mark.parametrize("setup_test_project", ["check_success"], indirect=True)
def test_cli_no_command_shows_art_and_help(runner: CliRunner, setup_test_project: Path):
    """Test running vibelint with no command shows ASCII art and help hint."""
    vibe_file = setup_test_project / "VIBECHECKER.txt"
    if not vibe_file.is_file():
        vibe_file.write_text(":-)", encoding="utf-8")

    result = runner.invoke(cli, [], catch_exceptions=False)
    assert result.exit_code == 0
    assert_output_matches(result, r"[^\s]")
    assert_output_contains(result, "Run vibelint --help for available commands.")
    assert "Usage: vibelint [OPTIONS] COMMAND [ARGS]..." not in result.output


def test_cli_no_project_root(runner: CliRunner, tmp_path: Path):
    """Test CLI behavior when no project root (pyproject.toml/.git) is found."""
    original_cwd = Path.cwd()
    empty_dir = tmp_path / "empty_test_dir_no_root_v5"  # Use different name
    empty_dir.mkdir()
    os.chdir(empty_dir)
    try:
        result = runner.invoke(cli, ["--debug", "check"], catch_exceptions=False)
        print(f"Output (no project root):\n{result.output}")
        assert (
            result.exit_code == 1
        ), f"Expected exit code 1 when no root found, got {result.exit_code}"
        assert_output_contains(result, "Error: Could not find project root.")
        assert_output_contains(result, "pyproject.toml")
        assert_output_contains(result, ".git")
    finally:
        os.chdir(original_cwd)


# --- 'check' Command Tests ---


@pytest.mark.parametrize("setup_test_project", ["check_success"], indirect=True)
def test_check_success(runner: CliRunner, setup_test_project: Path):
    """Test `vibelint check` on a project fixture expected to pass."""
    # Add default includes if missing from fixture's pyproject.toml
    modify_pyproject(setup_test_project, {"include_globs": ["src/**/*.py", "pyproject.toml"]})

    result = runner.invoke(cli, ["--debug", "check"], catch_exceptions=False)
    print(f"Output:\n{result.output}")
    print(f"DEBUG: CWD during test_check_success: {Path.cwd()}")
    print(f"DEBUG: setup_test_project path (yielded root): {setup_test_project}")
    assert (
        result.exit_code == 0
    ), f"Expected exit code 0, got {result.exit_code}. Output:\n{result.output}"
    assert_output_matches(result, r"Initiating Vibe Check")
    assert_output_matches(
        result, r"Summary: 0 errors, 1 warnings, 3 info", msg="Expected the fixture's known issues"
    )
    assert_output_does_not_match(result, r"\[VBL\d{3}\]", msg="Unexpected VBL codes found")
    assert_output_does_not_contain(result, "Collision Summary", msg="Unexpected collision summary")


@pytest.mark.parametrize("setup_test_project", ["fix_missing_all"], indirect=True)
def test_check_failure(runner: CliRunner, setup_test_project: Path):
    """Test `vibelint check` on a project fixture expected to fail."""
    # Add default includes if missing from fixture's pyproject.toml
    modify_pyproject(setup_test_project, {"include_globs": ["*.py"]})

    result = runner.invoke(cli, ["--debug", "check"], catch_exceptions=False)
    print(f"Output:\n{result.output}")
    print(f"DEBUG: CWD during test_check_failure: {Path.cwd()}")
    print(f"DEBUG: setup_test_project path (yielded root): {setup_test_project}")
    # INFO-level findings don't cause exit code 1 in modern vibelint
    assert (
        result.exit_code == 0
    ), f"Expected exit code 0, got {result.exit_code}. Output:\n{result.output}"
    assert_output_matches(result, r"Initiating Vibe Check")
    # Check for semantic rule IDs instead of VBL codes
    assert_output_matches(result, r"DOCSTRING-MISSING", msg="Missing docstring detection")
    assert_output_matches(result, r"EXPORTS-MISSING-ALL", msg="Missing __all__ detection")
    # Check for specific validation findings
    assert_output_contains(result, "Function 'func_one' is missing docstring")
    # Check for modern docstring path reference findings
    assert_output_matches(
        result,
        r"DOCSTRING-PATH-REFERENCE.*another\.py",
        msg="Missing path reference detection for another.py",
    )
    assert_output_matches(
        result,
        r"DOCSTRING-PATH-REFERENCE.*needs_fix\.py",
        msg="Missing path reference detection for needs_fix.py",
    )
    # Check for missing __all__ detection
    assert_output_matches(
        result,
        r"EXPORTS-MISSING-ALL.*needs_fix\.py",
        msg="Missing __all__ detection for needs_fix.py",
    )
    assert_output_does_not_contain(result, "Collision Summary", msg="Unexpected collision summary")


@pytest.mark.parametrize("setup_test_project", ["check_success"], indirect=True)
def test_check_output_report(runner: CliRunner, setup_test_project: Path):
    """Test `vibelint check -o report.md` generates a report."""
    # Add default includes if missing from fixture's pyproject.toml
    modify_pyproject(setup_test_project, {"include_globs": ["src/**/*.py", "pyproject.toml"]})

    report_file = setup_test_project / "vibelint_report.md"
    assert not report_file.exists()

    result = runner.invoke(
        cli, ["--debug", "check", "-o", str(report_file)], catch_exceptions=False
    )
    print(f"Output:\n{result.output}")

    assert (
        result.exit_code == 0
    ), f"Expected exit code 0 for check, got {result.exit_code}. Output:\n{result.output}"
    assert report_file.is_file(), f"Report file was not created at {report_file}"

    # Check for the message parts separately due to potential wrapping
    cleaned_output = clean_output(result.output)
    assert (
        "SUCCESS: Detailed Vibe Report generated at" in cleaned_output
    ), f"Report text missing. Output:\n{cleaned_output}"
    # Check if the filename exists somewhere in the cleaned output
    assert (
        report_file.name in cleaned_output
    ), f"Report filename missing. Filename: {report_file.name}\nOutput:\n{cleaned_output}"

    report_content = report_file.read_text()
    assert "# vibelint Report" in report_content
    assert "## Linting Results" in report_content
    # The success fixture has INFO-level findings, not no issues
    assert "| Findings with errors | 0 |" in report_content
    assert "| Findings with warnings | 0 |" in report_content
    assert "## Namespace Structure" in report_content
    assert setup_test_project.name in report_content
    assert "hello (member)" in report_content
    assert "## Namespace Collisions" in report_content
    assert "*No hard collisions detected.*" in report_content
    assert "## File Contents" in report_content
    assert "src/mypkg/module.py" in report_content.replace("\\", "/")


# --- 'namespace' Command Tests ---


@pytest.mark.parametrize("setup_test_project", ["check_success"], indirect=True)
def test_namespace_basic(runner: CliRunner, setup_test_project: Path):
    """Test basic `vibelint namespace` output."""
    # Add default includes if missing from fixture's pyproject.toml
    modify_pyproject(setup_test_project, {"include_globs": ["src/**/*.py", "pyproject.toml"]})

    result = runner.invoke(cli, ["--debug", "namespace"], catch_exceptions=False)
    print(f"Output:\n{result.output}")
    assert (
        result.exit_code == 0
    ), f"Expected exit code 0, got {result.exit_code}. Output:\n{result.output}"
    assert_output_matches(result, r"Namespace Structure Visualization")
    assert_output_contains(result, setup_test_project.name)
    assert_output_contains(result, "src")
    assert_output_contains(result, "mypkg")
    assert_output_contains(result, "module")
    assert_output_contains(result, "hello (member)")


@pytest.mark.parametrize("setup_test_project", ["check_success"], indirect=True)
def test_namespace_output_file(runner: CliRunner, setup_test_project: Path):
    """Test `vibelint namespace -o tree.txt` saves the tree."""
    # Add default includes if missing from fixture's pyproject.toml
    modify_pyproject(setup_test_project, {"include_globs": ["src/**/*.py", "pyproject.toml"]})

    tree_file = setup_test_project / "namespace_tree.txt"
    assert not tree_file.exists()

    result = runner.invoke(
        cli, ["--debug", "namespace", "-o", str(tree_file)], catch_exceptions=False
    )
    print(f"Output:\n{result.output}")

    assert (
        result.exit_code == 0
    ), f"Expected exit code 0, got {result.exit_code}. Output:\n{result.output}"
    assert tree_file.is_file(), f"Namespace tree file not created at {tree_file}"

    # Check for the message parts separately due to potential wrapping
    cleaned_output = clean_output(result.output)
    assert (
        "Namespace tree saved to" in cleaned_output
    ), f"Namespace save text missing. Output:\n{cleaned_output}"
    assert (
        tree_file.name in cleaned_output
    ), f"Namespace save filename missing. Filename: {tree_file.name}\nOutput:\n{cleaned_output}"

    tree_content = tree_file.read_text()
    assert setup_test_project.name in tree_content
    assert "hello (member)" in tree_content


# --- 'snapshot' Command Tests ---


@pytest.mark.parametrize("setup_test_project", ["check_success"], indirect=True)
def test_snapshot_basic(runner: CliRunner, setup_test_project: Path):
    """Test basic `vibelint snapshot` default output."""
    modify_pyproject(setup_test_project, {"include_globs": ["src/**/*.py", "pyproject.toml"]})

    snapshot_file = setup_test_project / "codebase_snapshot.md"
    assert not snapshot_file.exists()

    result = runner.invoke(cli, ["--debug", "snapshot"], catch_exceptions=False)
    print(f"Output:\n{result.output}")
    assert (
        result.exit_code == 0
    ), f"Expected exit code 0, got {result.exit_code}. Output:\n{result.output}"
    assert snapshot_file.is_file(), f"Snapshot file not created at {snapshot_file}"

    # Check for the message parts separately due to potential wrapping
    cleaned_output = clean_output(result.output)
    assert (
        "Codebase snapshot created at" in cleaned_output
    ), f"Snapshot creation text missing. Output:\n{cleaned_output}"
    assert (
        snapshot_file.name in cleaned_output
    ), f"Snapshot creation filename missing. Filename: {snapshot_file.name}\nOutput:\n{cleaned_output}"

    snapshot_content = snapshot_file.read_text()
    normalized_content = snapshot_content.replace("\\", "/")  # Normalize paths in content
    assert "# Snapshot" in normalized_content
    assert "## Filesystem Tree" in normalized_content
    # Check for the project root name (should be the directory name vibelint ran in)
    assert setup_test_project.name + "/" in normalized_content
    assert "pyproject.toml" in normalized_content
    assert "src/" in normalized_content
    assert "__init__.py" in normalized_content
    assert "module.py" in normalized_content
    assert "## File Contents" in normalized_content
    assert "### File: pyproject.toml" in normalized_content
    assert "[tool.vibelint]" in normalized_content
    assert "### File: src/mypkg/__init__.py" in normalized_content
    assert "### File: src/mypkg/module.py" in normalized_content


@pytest.mark.parametrize("setup_test_project", ["check_success"], indirect=True)
def test_snapshot_exclude(runner: CliRunner, setup_test_project: Path):
    """Test snapshot respects exclude_globs from config."""
    modify_pyproject(
        setup_test_project,
        {
            "include_globs": ["src/**/*.py", "pyproject.toml"],
            "exclude_globs": ["src/mypkg/module.py"],
        },
    )

    snapshot_file = setup_test_project / "codebase_snapshot.md"
    result = runner.invoke(cli, ["--debug", "snapshot"], catch_exceptions=False)
    print(f"Output:\n{result.output}")
    assert (
        result.exit_code == 0
    ), f"Expected exit code 0, got {result.exit_code}. Output:\n{result.output}"
    assert snapshot_file.is_file()

    snapshot_content = snapshot_file.read_text()
    tree_match = re.search(r"## Filesystem Tree\s*```\s*(.*?)\s*```", snapshot_content, re.DOTALL)
    assert tree_match, "Filesystem Tree section not found"
    tree_content = tree_match.group(1)
    print(f"DEBUG Tree Content:\n{tree_content}")

    assert "module.py" not in tree_content
    # Simpler string check for presence in tree
    assert "__init__.py" in tree_content, "__init__.py missing from tree"
    assert "pyproject.toml" in tree_content, "pyproject.toml missing from tree"

    normalized_content = snapshot_content.replace("\\", "/")
    assert "## File Contents" in normalized_content
    assert "### File: src/mypkg/module.py" not in normalized_content
    assert "### File: src/mypkg/__init__.py" in normalized_content
    assert "### File: pyproject.toml" in normalized_content


@pytest.mark.parametrize("setup_test_project", ["check_success"], indirect=True)
def test_snapshot_exclude_output_file(runner: CliRunner, setup_test_project: Path):
    """Test snapshot doesn't include its own output file."""
    modify_pyproject(
        setup_test_project, {"include_globs": ["src/**/*.py", "pyproject.toml", "*.md"]}
    )

    snapshot_file = setup_test_project / "my_own_snapshot.md"

    # First run creates the file
    result1 = runner.invoke(
        cli, ["--debug", "snapshot", "-o", str(snapshot_file)], catch_exceptions=False
    )
    assert result1.exit_code == 0
    assert snapshot_file.is_file()
    print(f"Snapshot 1 Output:\n{result1.output}")
    snapshot_content_1 = snapshot_file.read_text()
    assert "my_own_snapshot.md" not in snapshot_content_1

    # Second run should explicitly exclude the existing snapshot file
    result2 = runner.invoke(
        cli, ["--debug", "snapshot", "-o", str(snapshot_file)], catch_exceptions=False
    )
    assert result2.exit_code == 0
    print(f"Snapshot 2 Output:\n{result2.output}")

    snapshot_content_2 = snapshot_file.read_text()
    cleaned_content_tree = ""
    tree_match = re.search(r"## Filesystem Tree\s*```\s*(.*?)\s*```", snapshot_content_2, re.DOTALL)
    if tree_match:
        cleaned_content_tree = "\n".join(
            [line.strip() for line in tree_match.group(1).splitlines() if line.strip()]
        )
    print(f"DEBUG Tree Content (Run 2):\n{cleaned_content_tree}")

    cleaned_content_files = ""
    files_match = re.search(r"## File Contents\s*(.*)", snapshot_content_2, re.DOTALL)
    if files_match:
        cleaned_content_files = "\n".join(
            line.strip() for line in files_match.group(1).splitlines() if line.strip()
        )
    print(f"DEBUG File Content (Run 2):\n{cleaned_content_files}")

    # Check exclusion from tree
    assert (
        "my_own_snapshot.md" not in cleaned_content_tree
    ), "Snapshot file included in tree section on second run"
    # Check exclusion from content
    assert (
        "### File: my_own_snapshot.md" not in cleaned_content_files
    ), "Snapshot file content included on second run"

    # Verify other expected files are still present using simpler string checks
    assert "pyproject.toml" in cleaned_content_tree, "pyproject.toml missing from tree (run 2)"
    assert "### File: pyproject.toml" in cleaned_content_files
    assert "__init__.py" in cleaned_content_tree, "__init__.py missing from tree (run 2)"
    assert "### File: src/mypkg/__init__.py" in cleaned_content_files.replace("\\", "/")
    assert "module.py" in cleaned_content_tree, "module.py missing from tree (run 2)"
    assert "### File: src/mypkg/module.py" in cleaned_content_files.replace("\\", "/")


# --- Output Format Tests ---


@pytest.mark.parametrize("setup_test_project", ["check_success"], indirect=True)
def test_check_json_output_format(runner: CliRunner, setup_test_project: Path):
    """Test `vibelint check --format json` produces clean JSON output to stdout."""
    modify_pyproject(setup_test_project, {"include_globs": ["src/**/*.py", "pyproject.toml"]})

    result = runner.invoke(cli, ["check", "--format", "json"], catch_exceptions=False)
    print(f"JSON Output:\n{result.output}")

    # Should exit cleanly
    assert result.exit_code in [0, 1], f"Expected exit code 0 or 1, got {result.exit_code}"

    # Output should be valid JSON
    import json

    try:
        data = json.loads(result.output)
        assert "summary" in data, "JSON output missing 'summary' key"
        assert "findings" in data, "JSON output missing 'findings' key"
        assert isinstance(data["summary"], dict), "summary should be a dictionary"
        assert isinstance(data["findings"], list), "findings should be a list"
    except json.JSONDecodeError as e:
        pytest.fail(f"Invalid JSON output: {e}\nOutput:\n{result.output}")

    # Output should not contain ANSI codes or log messages
    assert "\x1b[" not in result.output, "JSON output contains ANSI escape codes"
    assert "Loaded" not in result.output, "JSON output contaminated with log messages"
    assert "settings" not in result.output, "JSON output contaminated with log messages"


@pytest.mark.parametrize("setup_test_project", ["check_success"], indirect=True)
def test_check_sarif_output_format(runner: CliRunner, setup_test_project: Path):
    """Test `vibelint check --format sarif` produces clean SARIF output to stdout."""
    modify_pyproject(setup_test_project, {"include_globs": ["src/**/*.py", "pyproject.toml"]})

    result = runner.invoke(cli, ["check", "--format", "sarif"], catch_exceptions=False)
    print(f"SARIF Output:\n{result.output}")

    # Should exit cleanly
    assert result.exit_code in [0, 1], f"Expected exit code 0 or 1, got {result.exit_code}"

    # Output should be valid JSON with SARIF structure
    import json

    try:
        data = json.loads(result.output)
        assert "version" in data, "SARIF output missing 'version' key"
        assert "$schema" in data, "SARIF output missing '$schema' key"
        assert "runs" in data, "SARIF output missing 'runs' key"
        assert isinstance(data["runs"], list), "runs should be a list"
        assert (
            data["version"] == "2.1.0"
        ), f"Expected SARIF version 2.1.0, got {data.get('version')}"
    except json.JSONDecodeError as e:
        pytest.fail(f"Invalid JSON output: {e}\nOutput:\n{result.output}")

    # Output should not contain ANSI codes or log messages
    assert "\x1b[" not in result.output, "SARIF output contains ANSI escape codes"
    assert "Loaded" not in result.output, "SARIF output contaminated with log messages"
    assert "settings" not in result.output, "SARIF output contaminated with log messages"


@pytest.mark.parametrize("setup_test_project", ["check_success"], indirect=True)
def test_check_human_output_format_default(runner: CliRunner, setup_test_project: Path):
    """Test `vibelint check` (default human format) includes UI messages."""
    modify_pyproject(setup_test_project, {"include_globs": ["src/**/*.py", "pyproject.toml"]})

    result = runner.invoke(cli, ["check"], catch_exceptions=False)
    print(f"Human Output:\n{result.output}")

    # Should exit cleanly
    assert result.exit_code in [0, 1], f"Expected exit code 0 or 1, got {result.exit_code}"

    # Human format should contain UI messages
    cleaned_output = clean_output(result.output)
    assert "Initiating Vibe Check" in cleaned_output, "Human output missing UI messages"

    # Should contain vibe check results
    assert any(
        word in cleaned_output for word in ["vibes", "Vibe", "Check"]
    ), "Human output missing vibe terminology"
```

---
### File: tests/test_emoji_removal_safety.py

```python
"""
Comprehensive tests for emoji removal safety.

Tests to ensure emoji removal doesn't break Python syntax or functionality.
"""

import ast
import sys
import tempfile
from pathlib import Path
from textwrap import dedent

import pytest

from vibelint.validators.single_file.emoji import EmojiUsageValidator


class TestEmojiRemovalSafety:
    """Test that emoji removal doesn't break Python syntax."""

    def setup_method(self):
        """Set up test validator."""
        self.validator = EmojiUsageValidator()

    def _compile_check(self, code: str) -> bool:
        """Check if code compiles without syntax errors."""
        try:
            ast.parse(code)
            return True
        except SyntaxError:
            return False

    def _exec_check(self, code: str) -> bool:
        """Check if code executes without runtime errors."""
        try:
            # Create a temporary file to execute the code
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                f.write(code)
                temp_path = f.name

            # Try to compile and execute
            with open(temp_path, 'r') as f:
                code_content = f.read()

            # Compile first
            compiled = compile(code_content, temp_path, 'exec')

            # Execute in isolated namespace
            namespace = {'__name__': '__main__'}
            exec(compiled, namespace)

            Path(temp_path).unlink()  # Clean up
            return True
        except Exception:
            try:
                Path(temp_path).unlink()  # Clean up on error
            except:
                pass
            return False

    def test_simple_string_emoji_removal(self):
        """Test emoji removal from simple strings."""
        code = '''
def greet():
    """Greet with emoji 🚀"""
    print("Hello! 👋")
    return "Done ✅"
'''

        # Original should compile
        assert self._compile_check(code)

        # Find emojis and apply fixes
        findings = list(self.validator.validate(Path("test.py"), code))
        assert len(findings) > 0  # Should find emojis

        # Apply all fixes
        fixed_code = code
        for finding in findings:
            fixed_code = self.validator.apply_fix(fixed_code, finding)

        # Fixed code should still compile and execute
        assert self._compile_check(fixed_code)
        assert self._exec_check(fixed_code)

        # Should have no emojis left
        remaining_findings = list(self.validator.validate(Path("test.py"), fixed_code))
        assert len(remaining_findings) == 0

    def test_complex_string_operations_with_emojis(self):
        """Test emoji removal from complex string operations."""
        code = '''
def process_messages():
    messages = [
        "Status: ✅ Complete",
        "Error: ❌ Failed",
        "Info: ℹ️ Processing",
        f"Result: {'🎉' if True else '😞'}"
    ]

    combined = " | ".join(messages)
    formatted = f"Summary: {combined} 📊"

    return {
        "status": "🟢 Active",
        "data": {"emoji_key": "🔑", "value": 42}
    }

result = process_messages()
'''

        # Original should work
        assert self._compile_check(code)
        assert self._exec_check(code)

        # Apply emoji fixes
        findings = list(self.validator.validate(Path("test.py"), code))
        fixed_code = code
        for finding in findings:
            fixed_code = self.validator.apply_fix(fixed_code, finding)

        # Fixed code should still work
        assert self._compile_check(fixed_code)
        assert self._exec_check(fixed_code)

    def test_emoji_in_function_names_and_variables(self):
        """Test handling of emojis in places where they'd break syntax."""
        # This is malformed code that should be detected but not break the fixer
        code = '''
def process_data🚀():  # Emoji in function name (should be detected)
    variable_name = "normal string"
    emoji_var🎯 = 42  # Emoji in variable name
    return variable_name + str(emoji_var🎯)
'''

        # This code is actually invalid Python, but our fixer should handle it gracefully
        findings = list(self.validator.validate(Path("test.py"), code))

        # Apply fixes
        fixed_code = code
        for finding in findings:
            fixed_code = self.validator.apply_fix(fixed_code, finding)

        # The fixed code should at least not crash the parser worse
        # (Though it may still be invalid due to remaining syntax issues)
        try:
            ast.parse(fixed_code)
        except SyntaxError:
            # This is expected for malformed code, but we shouldn't crash
            pass

    def test_emoji_removal_preserves_indentation(self):
        """Test that emoji removal preserves Python indentation."""
        code = '''
class DataProcessor:
    """Process data with emojis 🔄"""

    def __init__(self):
        self.status = "🟡 Ready"

    def process(self, data):
        """Process the data 📝"""
        if data:
            print("Processing... ⚙️")
            for item in data:
                if item:
                    print(f"  Item: {item} ✅")
                else:
                    print(f"  Skipped ⏭️")
        return "Done 🎉"

processor = DataProcessor()
result = processor.process(["a", "b", None, "c"])
'''

        # Original should work
        assert self._compile_check(code)
        assert self._exec_check(code)

        # Apply emoji fixes
        findings = list(self.validator.validate(Path("test.py"), code))
        fixed_code = code
        for finding in findings:
            fixed_code = self.validator.apply_fix(fixed_code, finding)

        # Fixed code should preserve indentation and work
        assert self._compile_check(fixed_code)
        assert self._exec_check(fixed_code)

        # Check that indentation levels are preserved
        original_lines = code.split('\n')
        fixed_lines = fixed_code.split('\n')

        assert len(original_lines) == len(fixed_lines)

        for orig, fixed in zip(original_lines, fixed_lines):
            # Indentation (leading whitespace) should be the same
            orig_indent = len(orig) - len(orig.lstrip())
            fixed_indent = len(fixed) - len(fixed.lstrip())
            assert orig_indent == fixed_indent

    def test_emoji_in_docstrings_and_comments(self):
        """Test emoji removal from docstrings and comments."""
        code = '''
def calculate_score(data):
    """
    Calculate score for data 📊

    Args:
        data: Input data 📥

    Returns:
        Score value 🔢
    """
    # Process the data 🔄
    score = 0
    for item in data:  # Iterate through items 🔍
        score += len(str(item))  # Add length 📏

    return score  # Return final score ✅

# Main execution 🚀
if __name__ == "__main__":
    result = calculate_score([1, 2, 3])  # Test data 🧪
    print(f"Final score: {result}")  # Output result 📤
'''

        # Original should work
        assert self._compile_check(code)
        assert self._exec_check(code)

        # Apply emoji fixes
        findings = list(self.validator.validate(Path("test.py"), code))
        fixed_code = code
        for finding in findings:
            fixed_code = self.validator.apply_fix(fixed_code, finding)

        # Fixed code should work
        assert self._compile_check(fixed_code)
        assert self._exec_check(fixed_code)

    def test_emoji_removal_edge_cases(self):
        """Test edge cases for emoji removal."""

        # Empty strings with emojis
        code1 = 'empty = "🚀"'
        findings = list(self.validator.validate(Path("test.py"), code1))
        fixed = code1
        for finding in findings:
            fixed = self.validator.apply_fix(fixed, finding)
        assert self._compile_check(fixed)

        # Multi-line strings with emojis
        code2 = '''
text = """
Multi-line string with emoji 🚀
And another emoji ✅
"""
'''
        findings = list(self.validator.validate(Path("test.py"), code2))
        fixed = code2
        for finding in findings:
            fixed = self.validator.apply_fix(fixed, finding)
        assert self._compile_check(fixed)

        # Raw strings with emojis
        code3 = r'raw_string = r"Raw string with emoji 🚀"'
        findings = list(self.validator.validate(Path("test.py"), code3))
        fixed = code3
        for finding in findings:
            fixed = self.validator.apply_fix(fixed, finding)
        assert self._compile_check(fixed)

    def test_no_double_space_cleanup(self):
        """Test that double spaces are properly cleaned up after emoji removal."""
        code = '''
message = "Before 🚀 After"
another = "Start ✅ Middle 🎉 End"
'''

        findings = list(self.validator.validate(Path("test.py"), code))
        fixed_code = code
        for finding in findings:
            fixed_code = self.validator.apply_fix(fixed_code, finding)

        # Should not have multiple consecutive spaces
        assert "  " not in fixed_code.replace('\n', ' ')
        assert self._compile_check(fixed_code)

    def test_syntax_preservation_comprehensive(self):
        """Comprehensive test of syntax preservation."""

        # Complex realistic code with emojis
        code = dedent('''
            import json
            from typing import List, Dict, Any

            class APIClient:
                """API client with emoji status indicators 🌐"""

                def __init__(self, base_url: str):
                    self.base_url = base_url
                    self.session_id = None
                    print("Initializing client... 🔧")

                async def authenticate(self, credentials: Dict[str, str]) -> bool:
                    """Authenticate with the API 🔐"""
                    try:
                        # Simulate authentication
                        if credentials.get("token"):
                            self.session_id = "session_123"
                            print("Authentication successful! ✅")
                            return True
                        else:
                            print("Authentication failed! ❌")
                            return False
                    except Exception as e:
                        print(f"Error during auth: {e} 🚨")
                        return False

                def fetch_data(self, endpoint: str) -> List[Dict[str, Any]]:
                    """Fetch data from endpoint 📡"""
                    if not self.session_id:
                        raise ValueError("Not authenticated 🔒")

                    # Mock data
                    return [
                        {"id": 1, "status": "active 🟢", "name": "Item 1"},
                        {"id": 2, "status": "pending 🟡", "name": "Item 2"},
                        {"id": 3, "status": "error 🔴", "name": "Item 3"}
                    ]

                def process_batch(self, items: List[Dict[str, Any]]) -> Dict[str, int]:
                    """Process a batch of items 📦"""
                    counts = {"success": 0, "error": 0}

                    for item in items:
                        try:
                            # Process item
                            processed = self._process_single_item(item)
                            if processed:
                                counts["success"] += 1
                                print(f"Processed {item['name']} ✅")
                            else:
                                counts["error"] += 1
                                print(f"Failed {item['name']} ❌")
                        except Exception:
                            counts["error"] += 1
                            print(f"Exception for {item['name']} 💥")

                    print(f"Batch complete! Success: {counts['success']}, Errors: {counts['error']} 📊")
                    return counts

                def _process_single_item(self, item: Dict[str, Any]) -> bool:
                    """Process a single item 🔧"""
                    return "error" not in item.get("status", "")

            # Usage example
            if __name__ == "__main__":
                client = APIClient("https://api.example.com")
                print("Starting API client demo 🚀")
        ''')

        # Original should compile and be valid
        assert self._compile_check(code)

        # Apply all emoji fixes
        findings = list(self.validator.validate(Path("test.py"), code))
        assert len(findings) > 10  # Should find many emojis

        fixed_code = code
        for finding in findings:
            fixed_code = self.validator.apply_fix(fixed_code, finding)

        # Fixed code should still be perfectly valid Python
        assert self._compile_check(fixed_code)

        # Should have no emojis remaining
        remaining_findings = list(self.validator.validate(Path("test.py"), fixed_code))
        assert len(remaining_findings) == 0

        # Verify the code structure is intact
        original_ast = ast.parse(code)
        fixed_ast = ast.parse(fixed_code)

        # Should have the same number of top-level nodes
        assert len(original_ast.body) == len(fixed_ast.body)


if __name__ == "__main__":
    # Run tests if called directly
    pytest.main([__file__, "-v"])
```

---
### File: tests/test_fix_functionality.py

```python
"""Tests for the --fix functionality to ensure it works correctly."""

from pathlib import Path
from unittest.mock import AsyncMock, MagicMock

import pytest

from vibelint.config import Config
from vibelint.fix import FixEngine, apply_fixes, can_fix_finding
from vibelint.plugin_system import Finding, Severity


class TestFixFunctionality:
    """Test the fix functionality."""

    def test_can_fix_finding_with_fixable_rules(self):
        """Test that can_fix_finding correctly identifies fixable rules."""
        # Test fixable rules
        finding1 = Finding(
            rule_id="DOCSTRING-MISSING",
            message="Missing docstring",
            file_path=Path("test.py"),
            line=1,
            severity=Severity.WARN,
        )
        assert can_fix_finding(finding1) is True

        finding2 = Finding(
            rule_id="DOCSTRING-PATH-REFERENCE",
            message="Missing path reference",
            file_path=Path("test.py"),
            line=1,
            severity=Severity.WARN,
        )
        assert can_fix_finding(finding2) is True

        finding3 = Finding(
            rule_id="EXPORTS-MISSING-ALL",
            message="Missing __all__",
            file_path=Path("test.py"),
            line=1,
            severity=Severity.WARN,
        )
        assert can_fix_finding(finding3) is True

    def test_can_fix_finding_with_non_fixable_rules(self):
        """Test that can_fix_finding correctly identifies non-fixable rules."""
        finding = Finding(
            rule_id="SOME-OTHER-RULE",
            message="Some other issue",
            file_path=Path("test.py"),
            line=1,
            severity=Severity.WARN,
        )
        assert can_fix_finding(finding) is False

    def test_fix_engine_can_fix_finding(self):
        """Test FixEngine.can_fix_finding method."""
        config = MagicMock()
        engine = FixEngine(config)

        finding = Finding(
            rule_id="DOCSTRING-MISSING",
            message="Missing docstring",
            file_path=Path("test.py"),
            line=1,
            severity=Severity.WARN,
        )
        assert engine.can_fix_finding(finding) is True

        finding2 = Finding(
            rule_id="NON-FIXABLE-RULE",
            message="Some other issue",
            file_path=Path("test.py"),
            line=1,
            severity=Severity.WARN,
        )
        assert engine.can_fix_finding(finding2) is False

    def test_deterministic_fixes(self):
        """Test that fixes are applied deterministically without LLM file rewriting."""
        config = MagicMock()
        engine = FixEngine(config)

        # Test can_fix_finding works correctly
        docstring_finding = Finding(
            rule_id="DOCSTRING-MISSING",
            message="Missing docstring",
            file_path=Path("test.py"),
            line=5,
            severity=Severity.WARN,
        )

        exports_finding = Finding(
            rule_id="EXPORTS-MISSING-ALL",
            message="Missing __all__",
            file_path=Path("test.py"),
            line=1,
            severity=Severity.WARN,
        )

        unfixable_finding = Finding(
            rule_id="SOME-OTHER-RULE",
            message="Some other issue",
            file_path=Path("test.py"),
            line=1,
            severity=Severity.WARN,
        )

        # Test which findings can be fixed
        assert engine.can_fix_finding(docstring_finding)
        assert engine.can_fix_finding(exports_finding)
        assert not engine.can_fix_finding(unfixable_finding)

    @pytest.mark.asyncio
    async def test_apply_fixes_integration(self):
        """Test apply_fixes function integration."""
        config = MagicMock()

        # Mock findings by file
        findings = [
            Finding(
                rule_id="DOCSTRING-MISSING",
                message="Missing docstring",
                file_path=Path("test.py"),
                line=5,
                severity=Severity.WARN,
            )
        ]

        file_findings = {Path("test.py"): findings}

        # Mock the FixEngine to avoid actual LLM calls
        with pytest.MonkeyPatch().context() as m:
            mock_fix_file = AsyncMock(return_value=False)
            m.setattr("vibelint.fix.FixEngine.fix_file", mock_fix_file)

            results = await apply_fixes(config, file_findings)

            # apply_fixes returns count of fixed files, not dict
            assert isinstance(results, int)
            assert results == 0  # No files were actually fixed (mocked to return False)


if __name__ == "__main__":
    pytest.main([__file__])
```

---
### File: tests/test_plugin_system.py

```python
"""
Tests for the vibelint plugin system.
"""

import json
from pathlib import Path
from typing import Iterator

from vibelint.plugin_system import (BaseFormatter, BaseValidator, Finding,
                                    Severity, plugin_manager)
from vibelint.reporting import JsonFormatter, NaturalLanguageFormatter
from vibelint.rules import RuleEngine


class TestValidator(BaseValidator):
    """Test validator for plugin system tests."""

    rule_id = "TEST001"
    name = "Test Validator"
    description = "A test validator"
    default_severity = Severity.WARN

    def __init__(self, severity=None, config=None):
        super().__init__(severity, config)

    def validate(self, file_path: Path, content: str, config=None) -> Iterator[Finding]:
        if "test_issue" in content:
            yield self.create_finding(
                message="Found test issue",
                file_path=file_path,
                line=1,
                suggestion="Remove test_issue",
            )


class TestFormatter(BaseFormatter):
    """Test formatter for plugin system tests."""

    name = "test"
    description = "Test formatter"

    def format_results(self, findings, summary):
        return f"Test format: {len(findings)} findings"


def test_finding_creation():
    """Test Finding dataclass functionality."""
    finding = Finding(
        rule_id="VBL001",
        message="Test message",
        file_path=Path("test.py"),
        line=10,
        severity=Severity.WARN,
    )

    assert finding.rule_id == "VBL001"
    assert finding.message == "Test message"
    assert finding.line == 10
    assert finding.severity == Severity.WARN

    # Test to_dict conversion
    data = finding.to_dict()
    assert data["rule"] == "VBL001"
    assert data["level"] == "WARN"
    assert data["path"] == "test.py"
    assert data["line"] == 10


def test_base_validator():
    """Test BaseValidator functionality."""
    validator = TestValidator()

    assert validator.rule_id == "TEST001"
    assert validator.severity == Severity.WARN

    # Test validation
    findings = list(validator.validate(Path("test.py"), "test_issue here"))
    assert len(findings) == 1
    assert findings[0].rule_id == "TEST001"
    assert findings[0].message == "Found test issue"

    # Test no findings
    findings = list(validator.validate(Path("test.py"), "clean code"))
    assert len(findings) == 0


def test_severity_override():
    """Test severity override in validators."""
    validator = TestValidator(severity=Severity.BLOCK)
    assert validator.severity == Severity.BLOCK

    findings = list(validator.validate(Path("test.py"), "test_issue here"))
    assert findings[0].severity == Severity.BLOCK


def test_plugin_manager_loads_formatters():
    """Test that plugin manager can load and retrieve formatters."""
    manager = plugin_manager

    # The plugin manager loads formatters from entry points
    manager.load_plugins()

    # Test getting all formatters
    formatters = manager.get_all_formatters()
    assert isinstance(formatters, dict)
    assert len(formatters) > 0

    # Test getting specific formatter
    formatter_class = manager.get_formatter("human")
    assert formatter_class is not None


def test_rule_engine():
    """Test RuleEngine functionality."""
    config = {"rules": {"TEST001": "BLOCK", "TEST002": "OFF"}}

    engine = RuleEngine(config)

    # Test rule enabling/disabling
    assert engine.is_rule_enabled("TEST001")
    assert not engine.is_rule_enabled("TEST002")
    assert engine.is_rule_enabled("TEST003")  # Default enabled

    # Test severity override
    assert engine.get_rule_severity("TEST001") == Severity.BLOCK
    assert engine.get_rule_severity("TEST003", Severity.INFO) == Severity.INFO


def test_human_formatter():
    """Test NaturalLanguageFormatter output."""
    formatter = NaturalLanguageFormatter()

    findings = [
        Finding(
            rule_id="VBL001",
            message="Test error",
            file_path=Path("test.py"),
            line=10,
            severity=Severity.BLOCK,
        ),
        Finding(
            rule_id="VBL002",
            message="Test warning",
            file_path=Path("other.py"),
            line=5,
            severity=Severity.WARN,
        ),
    ]

    summary = {"BLOCK": 1, "WARN": 1, "INFO": 0}
    output = formatter.format_results(findings, summary)

    assert "BLOCK:" in output
    assert "WARN:" in output
    assert "VBL001" in output
    assert "VBL002" in output
    assert "test.py:10" in output


def test_json_formatter():
    """Test JsonFormatter output."""
    formatter = JsonFormatter()

    findings = [
        Finding(
            rule_id="VBL001",
            message="Test issue",
            file_path=Path("test.py"),
            line=10,
            severity=Severity.WARN,
        )
    ]

    summary = {"WARN": 1}
    output = formatter.format_results(findings, summary)

    # Parse JSON to verify structure
    data = json.loads(output)
    assert "summary" in data
    assert "findings" in data
    assert data["summary"]["WARN"] == 1
    assert len(data["findings"]) == 1
    assert data["findings"][0]["rule"] == "VBL001"
    assert data["findings"][0]["level"] == "WARN"


def test_severity_comparison():
    """Test Severity enum comparison."""
    assert Severity.OFF < Severity.INFO
    assert Severity.INFO < Severity.WARN
    assert Severity.WARN < Severity.BLOCK

    # Test sorting
    severities = [Severity.BLOCK, Severity.OFF, Severity.WARN, Severity.INFO]
    sorted_severities = sorted(severities)
    expected = [Severity.OFF, Severity.INFO, Severity.WARN, Severity.BLOCK]
    assert sorted_severities == expected
```

---
### File: tests/test_print_suppression.py

```python
"""
Direct test for print statement suppression functionality.
"""

from pathlib import Path

from vibelint.validators.print_statements import PrintStatementValidator


def test_print_suppression_comments():
    """Test that suppression comments work correctly."""
    # Create validator with explicit empty config to avoid defaults
    validator = PrintStatementValidator()
    # Override the config to ensure no exclusions
    validator.config = {"print_validation": {"exclude_globs": []}}

    # Test basic suppression with vibelint: stdout
    code1 = 'print("CLI output")  # vibelint: stdout'
    findings1 = list(validator.validate(Path("script.py"), code1))
    assert len(findings1) == 0, "vibelint: stdout comment should suppress warning"

    # Test noqa: print suppression
    code2 = 'print("Debug")  # noqa: print'
    findings2 = list(validator.validate(Path("script.py"), code2))
    assert len(findings2) == 0, "noqa: print should suppress warning"

    # Test general noqa
    code3 = 'print("Debug")  # noqa'
    findings3 = list(validator.validate(Path("script.py"), code3))
    assert len(findings3) == 0, "noqa should suppress warning"

    # Test unsuppressed print
    code4 = 'print("Debug output")'
    findings4 = list(validator.validate(Path("script.py"), code4))
    print(f"Unsuppressed test: found {len(findings4)} findings")
    assert len(findings4) == 1, "Print without suppression should be flagged"

    # Test auto-detection of CLI patterns (URL)
    code5 = 'print("Server at http://localhost:8000")'
    findings5 = list(validator.validate(Path("script.py"), code5))
    assert len(findings5) == 0, "URL in print should be auto-detected as CLI output"

    # Test each case independently since multiline has issues
    codes = [
        ('print("Debug 1")', True, "Regular print should flag"),
        ('print("Status update")  # vibelint: stdout', False, "vibelint: stdout should suppress"),
        ('print("Debug 2")  # NOQA: PRINT', False, "NOQA: PRINT should suppress"),
        ('print("Starting server on port 8080")', False, "port number should auto-detect as CLI"),
    ]

    for code, should_flag, description in codes:
        findings = list(validator.validate(Path("script.py"), code))
        if should_flag:
            assert len(findings) == 1, f"{description} - expected 1 finding, got {len(findings)}"
        else:
            assert len(findings) == 0, f"{description} - expected 0 findings, got {len(findings)}"


if __name__ == "__main__":
    test_print_suppression_comments()
    print("All tests passed!")
```

---
### File: tests/test_validators.py

```python
"""
Tests for validators.
"""

from pathlib import Path

from vibelint.validators.emoji import EmojiUsageValidator
# Import validators from their individual modules
from vibelint.validators.print_statements import PrintStatementValidator
from vibelint.validators.typing_quality import TypingQualityValidator


def test_print_statement_validator():
    """Test PrintStatementValidator functionality."""
    validator = PrintStatementValidator()

    code = """
print("hello")
print("debug info")
"""

    findings = list(validator.validate(Path("test.py"), code))
    assert len(findings) == 2

    # Check that print statements are detected
    for finding in findings:
        assert "Print statement found" in finding.message
        assert "logging" in finding.message


def test_emoji_usage_validator():
    """Test EmojiUsageValidator."""
    validator = EmojiUsageValidator()

    code = '''# This has emojis 🚀 that cause issues
print("Hello world! ⭐")
def process_data():
    """Process data with fancy output 🎉"""
    return True
'''

    findings = list(validator.validate(Path("test.py"), code))
    assert len(findings) == 2  # Two lines with emojis

    # Check that suggestions mention MCP compatibility
    for finding in findings:
        assert (
            finding.suggestion is None
            or "MCP" in finding.suggestion
            or "encoding" in finding.suggestion
        )


def test_typing_quality_validator():
    """Test TypingQualityValidator functionality."""
    validator = TypingQualityValidator()

    code = '''
def untyped_function(a, b, c):
    """Function without type annotations."""
    return a + b + c
'''

    findings = list(validator.validate(Path("test.py"), code))

    # Should find missing type annotations for function and parameters
    assert len(findings) >= 3

    # Check that type annotation issues are detected
    messages = [f.message for f in findings]
    assert any("missing type annotations" in msg for msg in messages)


def test_llm_formatter():
    """Test that LLM formatter is available."""
    from vibelint.formatters import BUILTIN_FORMATTERS

    assert "llm" in BUILTIN_FORMATTERS
```

---

