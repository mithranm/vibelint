# Vibelint Analysis Reports

This directory contains detailed analysis reports generated by vibelint.

## Example Report

See `example-analysis.json` for a sample of what vibelint detects:

- **Code Smells**: Long methods, magic numbers, uncommunicative names
- **Architecture Issues**: SRP violations, coupling problems
- **Silent Failures**: Exception handlers that swallow errors
- **Dead Code**: Unused functions, unreferenced definitions
- **Type Issues**: Missing annotations, poor typing practices
- **Namespace Collisions**: Variable name conflicts across modules

## Usage

Generate your own reports:

```bash
# Quick check on modified files
PYTHONPATH=src vibelint check path/to/files.py

# Comprehensive analysis with JSON output
PYTHONPATH=src vibelint check src/ --format json > .vibelint-reports/$(date +%Y-%m-%d-%H%M%S)-analysis.json

# Filter results by rule type
grep -A5 -B1 '"rule": "ARCHITECTURE-LLM"' .vibelint-reports/latest.json

# Get rule frequency breakdown
grep '"rule":' .vibelint-reports/latest.json | sort | uniq -c | sort -nr
```

## LLM Workflow Tracing

See `example-llm-trace.json` for complete end-to-end logging of LLM analysis workflows:

- **Complete LLM Call Traces**: Every prompt, response, token usage, and latency
- **Routing Decisions**: Why each LLM was selected (fast vs orchestrator)
- **Workflow Timeline**: Start/end times, duration, and event sequences
- **Error Tracking**: Failures and exceptions in the analysis pipeline
- **Cost Analysis**: Token usage and estimated costs across all LLM calls

This comprehensive tracing enables:
- **Debugging**: Trace exactly what went wrong in failed analyses
- **Auditing**: Full accountability of AI decision-making process
- **Optimization**: Identify bottlenecks and improve routing efficiency
- **Cost Control**: Monitor and optimize LLM usage costs

## Single LLM Configuration

See `example-single-llm-config.toml` for configuring vibelint with just one LLM:

- **Fast LLM only**: Basic AI features (code smells, docstrings, simple validation)
- **Orchestrator LLM only**: All AI features but less efficient routing
- **Modular design**: Unconfigured features gracefully degrade
- **Fail-fast behavior**: Configured but unavailable models abort immediately

This allows users with limited resources to still benefit from vibelint's AI capabilities.

All reports except examples are gitignored to keep the repository clean.