# Example: Single LLM configuration for vibelint
# Users with only one LLM can still access available AI features

[tool.vibelint.llm]
# Option 1: Fast LLM only (good for basic AI features)
fast_api_url = "http://localhost:8001"
fast_model = "openai/gpt-oss-20b"
fast_temperature = 0.1
fast_max_tokens = 2048

# Orchestrator LLM not configured - architecture analysis features will be unavailable
# but code smell detection, docstring generation, and simple validation will work

# Option 2: Orchestrator LLM only (if you have powerful hardware)
# orchestrator_api_url = "http://localhost:11434"
# orchestrator_model = "C:\\dev\\openai_gpt-oss-120b-MXFP4.gguf"
# orchestrator_temperature = 0.2
# orchestrator_max_tokens = 8192

# This would provide all AI features but route everything to the large model
# Less efficient but gives full capability with one model

# Routing configuration
context_threshold = 3000              # Not relevant with single LLM
enable_fallback = false               # Always false - fail fast on unavailability

# Available features with fast LLM only:
# ✅ docstring_generation
# ✅ code_smell_detection
# ✅ simple_validation
# ❌ architecture_analysis (requires orchestrator)
# ❌ coverage_assessment (requires orchestrator)
# ❌ semantic_analysis (requires orchestrator)